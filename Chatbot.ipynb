{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPFvPhcUI74rPDpLqGnWk6t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecuadrafoy/toolbox/blob/master/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4KSJW3ksL_2",
        "colab_type": "text"
      },
      "source": [
        "# Building a Chatbot Using Attention-Based Neural Networks\n",
        "\n",
        "We will expand on our sequence-to-sequence models from the previous chapter, adding something called attention to our models.\n",
        "\n",
        "This improvement to the sequence-to-sequence models means that our model learns where in the input sentence to look to obtain the information it needs, rather than using the whole input sentence decision. \n",
        "\n",
        "## The theory of attention\n",
        "\n",
        "* Previously the encoder would take the hidden state of the whole sentence and depend on the decoder to transform it into the out put. However decoding over the entirety of the hidden state is not necessarily the most efficient way of using this task. This is because the hidden state represents the entirety of the input sentence.\n",
        "  * We do not need to consider the entirety of the input sentence, just the parts that are relevant to the prediction we are trying to make\n",
        "  * By using attention within our sequence-to-sequence neural network. We can teach our model to only look at the relevant parts of the input in order to make its prediction, resulting in a much more efficient and accurate model.\n",
        "  * There are two main types of attention mechanisms that we can implement: local and global attention.\n",
        "\n",
        "### Local Attention\n",
        "In local attention, our model only looks at a few hidden states from the encoder.\n",
        "  * For example, if we are performing a sentence translation task and we are calculating the second word in our translation, the model may wish to only look at the hidden states from the encoder related to the second word in the input sentence.\n",
        "\n",
        "![](https://learning.oreilly.com/library/view/hands-on-natural-language/9781789802740/image/B12365_08_2.jpg)\n",
        "\n",
        "  * We first start by calculating the aligned position, pt, from our final hidden state, hn. This tells us which hidden states we need to be looking at to make our prediction. \n",
        "  * We then calculate our local weights and apply them to our hidden states in order to determine our context vector\n",
        "    * These weights may tell us to pay more attention to the most relevant hidden state (h2) but less attention to the preceding hidden state (h1)\n",
        "  * We then take our context vector and pass it forward to our decoder in order to make its prediction\n",
        "    * Here instead of passing the final hidden state hn, we only consider the relevant hidden states that our model deems necessary to make its prediction.\n",
        "\n",
        "### Global Attention\n",
        "\n",
        "The global attention model works in a very similar way. However, instead of only looking at a few of the hidden states, we want to look at all of our model's hidden states\n",
        "![](https://learning.oreilly.com/library/view/hands-on-natural-language/9781789802740/image/B12365_08_3.jpg)\n",
        "\n",
        "* Our model is now looking at all the hidden states and calculating the global weights across all of them\n",
        "* Allowing our model to look at any given part of the input sentence that it considers relevant, instead of being limited to a local area determined by the local attention methodology\n",
        "* The global attention framework is like learning a mask that only allows through hidden states that are relevant to our prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDHoZfPQ0evs",
        "colab_type": "text"
      },
      "source": [
        "## Training the Chatbot\n",
        "* Our chatbot will take a line of human-entered input and respond to it with a generated sentence.\n",
        "* The perfect dataset for a task such as this would be actual chat logs from conversations between two human users.\n",
        "* Movie scripts consist of conversations between two or more characters. While this data is not naturally in the format we would like it to be in, we can easily transform it into the format that we need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrMeSuYE2wZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "from __future__ import absolute_import\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JzZxRi0sCBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "57c3fb84-19b6-46a4-8293-750a793871ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85cjcBymBP55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = \"movie_corpus\"\n",
        "corpus_name = \"movie_corpus\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryScarib2nSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/NLP_PyTorch/Glove\"\n",
        "datafile = os.path.join(path, \"formatted_movie_lines.txt\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g7ooe9i29L3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "872d30c0-ea40-43c5-d595-e053a2955d51"
      },
      "source": [
        "with open(datafile, 'rb') as file:\n",
        "    lines = file.readlines()\n",
        "    \n",
        "for line in lines[:3]:\n",
        "    print(str(line) + '\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\"\n",
            "\n",
            "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\"\n",
            "\n",
            "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj131Zh33KXa",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Vocabulary\n",
        "* In the past, our corpus has comprised of several dictionaries consisting of the unique words in our corpus and lookups between word and indices. However, we can do this in a far more elegant way by creating a vocabulary class that consists of all of the elements required:\n",
        "  * We start by creating our Vocabulary class.\n",
        "  * We initialize this class with empty dictionaries—word2index and word2count.\n",
        "  * We also initialize the index2word dictionary with placeholders for our padding tokens, as well as our Start-of-Sentence (SOS) and End-of-Sentence (EOS) tokens.\n",
        "  * We keep a running count of the number of words in our vocabulary, too (which is 3 to start with as our corpus already contains the three tokens mentioned)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV9fX9e53sJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "PAD_token = 0 \n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "    def addWord(self, w): \n",
        "      # addWord takes a word as input. If this is a new word that is not already in our vocabulary, we add this word to our indices\n",
        "      # set the count of this word to 1, and increment the total number of words in our vocabulary by 1. \n",
        "      #If the word in question is already in our vocabulary, we simply increment the count of this word by 1\n",
        "        if w not in self.word2index:\n",
        "            self.word2index[w] = self.num_words\n",
        "            self.word2count[w] = 1\n",
        "            self.index2word[self.num_words] = w\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[w] += 1        \n",
        "        \n",
        "    def addSentence(self, sent):\n",
        "      # We also use the addSentence function to apply the addWord function to all the words within a given sentence\n",
        "        for word in sent.split(' '):\n",
        "            self.addWord(word)\n",
        "# One thing we can do to speed up the training of our model is reduce the size of our vocabulary\n",
        "# An easy way to do this is to remove any low-frequency words from our vocabulary\n",
        "# Any words occurring just once or twice in our dataset are unlikely to have huge predictive power\n",
        "\n",
        "    def trim(self, min_cnt):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        words_to_keep = []\n",
        "\n",
        "        for k, v in self.word2count.items(): \n",
        "          # function first loops through the word count dictionary and if the occurrence of the word is greater than the minimum required count, it is appended to a new list:\n",
        "            if v >= min_cnt:\n",
        "                words_to_keep.append(k)\n",
        "\n",
        "        print('Words to Keep: {} / {} = {:.2%}'.format(\n",
        "            len(words_to_keep), len(self.word2index), len(words_to_keep) / len(self.word2index)\n",
        "        ))\n",
        "# Finally, our indices are rebuilt from the new words_to_keep list\n",
        "# We set all the indices to their initial empty values and then repopulate them by looping through our kept words with the addWord function:\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "        for w in words_to_keep:\n",
        "            self.addWord(w)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brXPs1dP89r5",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38bOafQo89aK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "  # converting it from Unicode into ASCII format\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def cleanString(s):\n",
        "  # process our input strings so that they are all in lowercase and do not contain any trailing whitespace or punctuation, except the most basic characters\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# This function reads our data file into lines and then applies the cleanString function to every line\n",
        "# It also creates an instance of the Vocabulary class\n",
        "def readVocs(datafile, corpus_name):\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    pairs = [[cleanString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Vocabulary(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "# filter our input pairs by their maximum length\n",
        "# done to reduce the potential dimensionality of our model.\n",
        "\n",
        "\n",
        "def filterPair(p, max_length):\n",
        "  # filterPair, returns a Boolean value based on whether the current line has an input and output length that is less than the maximum length.\n",
        "    return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n",
        "\n",
        "def filterPairs(pairs, max_length):\n",
        "  #  Our second function, filterPairs, simply applies this condition to all the pairs within our dataset, only keeping the ones that meet this condition\n",
        "    return [pair for pair in pairs if filterPair(pair, max_length)]\n",
        "\n",
        "# final function that applies all the previous functions we have put together and run it to create our vocabulary and data pairs\n",
        "def loadData(corpus, corpus_name, datafile, max_length):\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(str(len(pairs)) + \" Sentence pairs\")\n",
        "    pairs = filterPairs(pairs,max_length)\n",
        "    print(str(len(pairs))+ \" Sentence pairs after trimming\")\n",
        "    for p in pairs:\n",
        "        voc.addSentence(p[0])\n",
        "        voc.addSentence(p[1])\n",
        "    print(str(voc.num_words) + \" Distinct words in vocabulary\")\n",
        "    return voc, pairs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agOS2kHRBAd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "324b6665-5303-49e9-b5f1-9b665ba153f5"
      },
      "source": [
        "max_length = 10 \n",
        "voc, pairs = loadData(corpus, corpus_name, datafile, max_length)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "221282 Sentence pairs\n",
            "64271 Sentence pairs after trimming\n",
            "18008 Distinct words in vocabulary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evYe0QsaBh4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8a3a22a9-86d3-4aed-b974-646ff2b47a6e"
      },
      "source": [
        "print(\"Example Pairs:\")\n",
        "for pair in pairs[-20:]:\n",
        "    print(pair)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example Pairs:\n",
            "['yes i have mine .', 'and i have mine .']\n",
            "['yes . . .yes you have yours .', 'why don t we talk inside ?']\n",
            "['yes i know .', 'it wouldn t be fair to her .']\n",
            "['it wouldn t be fair to her .', 'yes i know .']\n",
            "['are you ready for me ?', 'mmmmmmmmm !']\n",
            "['mmmmmmmmm !', 'ready for fuchsmachen ? ? ?']\n",
            "['ready for fuchsmachen ? ? ?', 'mmmmmmmmmmmmmmm !']\n",
            "['his what ? ?', 'his schwanzstucker .']\n",
            "['his schwanzstucker .', 'whew ! a nineteen inch drill .']\n",
            "['how long is it so far ?', 'four']\n",
            "['four', 'three minutes to go !']\n",
            "['three minutes to go !', 'yes .']\n",
            "['another fifteen seconds to go .', 'do something ! stall them !']\n",
            "['yes sir name please ?', 'food !']\n",
            "['food !', 'do you have a reservation ?']\n",
            "['do you have a reservation ?', 'food ! !']\n",
            "['grrrhmmnnnjkjmmmnn !', 'franz ! help ! lunatic !']\n",
            "['what o clock is it mr noggs ?', 'eleven o clock my lorj']\n",
            "['stuart ?', 'yes .']\n",
            "['yes .', 'how quickly can you move your artillery forward ?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9gmn9PTB7WH",
        "colab_type": "text"
      },
      "source": [
        "## Removing Rare Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4c0EVUfB9jQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "40c43723-1ecc-42d1-d11f-be28a270f3ba"
      },
      "source": [
        "def removeRareWords(voc, all_pairs, minimum):\n",
        "# Create a function to remove these rare words and call the trim method from our vocabulary as our first step\n",
        "    voc.trim(minimum) # first calculate the percentage of words that we will keep within our model\n",
        "    \n",
        "    pairs_to_keep = []\n",
        "    \n",
        "    for p in all_pairs:\n",
        "      # we loop through all the words in the input and output sentences. \n",
        "      # If for a given pair either the input or output sentence has a word that isn't in our new trimmed corpus, we drop this pair from our dataset\n",
        "        keep = True\n",
        "        \n",
        "        for word in p[0].split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep = False\n",
        "                break\n",
        "        for word in p[1].split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep = False\n",
        "                break\n",
        "\n",
        "        if keep:\n",
        "            pairs_to_keep.append(p)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.2%} of total\".format(len(all_pairs)\\\n",
        "        , len(pairs_to_keep), len(pairs_to_keep)/ len(all_pairs)))\n",
        "    return pairs_to_keep\n",
        "\n",
        "\n",
        "minimum_count = 3\n",
        "pairs = removeRareWords(voc, pairs, minimum_count)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words to Keep: 7823 / 18005 = 43.45%\n",
            "Trimmed from 64271 pairs to 53165, 82.72% of total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOVSC3tvEEKo",
        "colab_type": "text"
      },
      "source": [
        "## Transforming Data to Tensors\n",
        "* our model will not take raw text as input, but rather, tensor representations of sentences\n",
        "*  We will also not process our sentences one by one, but instead in smaller batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bciVp2rhECCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexFromSentence(voc, sent):\n",
        "  # grabs the index of each word in the sentence from the vocabulary and appends an EOS token to the end\n",
        "    return [voc.word2index[w] for w in sent.split(' ')] + [EOS_token]\n",
        "\n",
        "def zeroPad(l, fillvalue=PAD_token):\n",
        "  # pads any tensors with zeroes so that all of the sentences within the tensor are effectively the same length\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "# to generate our input tensor, we apply both of these functions\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l] # First, we get the indices of our input sentence\n",
        "    padList = zeroPad(indexes_batch) # then apply padding\n",
        "    padTensor = torch.LongTensor(padList) # transform the output into LongTensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # obtain the lengths of each of our input sentences out output this as a tensor\n",
        "    return padTensor, lengths\n",
        "\n",
        "def getMask(l, value=PAD_token): # create a Boolean mask to ignore padded tokens\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq: # returns 1 if the output consists of a word and 0 if it consists of a padding token\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "def outputVar(l, voc):\n",
        "  # apply this to our outputVar function\n",
        "  # This is identical to the inputVar function, except that along with the indexed output tensor and the tensor of lengths, we also return the Boolean mask of our output tensor\n",
        "    indexes_batch = [indexFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPad(indexes_batch)\n",
        "  # This Boolean mask just returns True when there is a word within the output tensor and False when there is a padding token\n",
        "    mask = torch.BoolTensor(getMask(padList))\n",
        "    padTensor = torch.LongTensor(padList)\n",
        "    return padTensor, mask, max_target_len\n",
        "\n",
        "def batch2Train(voc, batch):\n",
        "  #in order to create our input and output batches concurrently, we loop through the pairs in our batch and create input and output tensors \n",
        "  # for both pairs using the functions we created previously\n",
        "    batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    \n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    \n",
        "    for p in batch:\n",
        "        input_batch.append(p[0])\n",
        "        output_batch.append(p[1])\n",
        "        \n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    \n",
        "    return inp, lengths, output, mask, max_target_len"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SP45GMfKSuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "ddc76c28-22b3-4010-85d2-90b4c4107339"
      },
      "source": [
        "test_batch_size = 5\n",
        "batches = batch2Train(voc, [random.choice(pairs) for _ in range(test_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"Input:\")\n",
        "print(input_variable)\n",
        "\n",
        "print(\"Target:\")\n",
        "print(target_variable)\n",
        "\n",
        "print(\"Mask:\")\n",
        "print(mask)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            "tensor([[  25,   25,  331, 3197,  598],\n",
            "        [ 988,  505,  117, 5580,    7],\n",
            "        [ 117,   60,  101,    4,    2],\n",
            "        [  84,    4, 2413,    2,    0],\n",
            "        [ 219,   25,    6,    0,    0],\n",
            "        [  25,  505,    2,    0,    0],\n",
            "        [ 200,  746,    0,    0,    0],\n",
            "        [3236, 4607,    0,    0,    0],\n",
            "        [   4,    4,    0,    0,    0],\n",
            "        [   2,    2,    0,    0,    0]])\n",
            "Target:\n",
            "tensor([[   7,  197,   34, 1014,   25],\n",
            "        [  14,  117,    4,  124, 1962],\n",
            "        [  64,   60,    2,  125,   86],\n",
            "        [ 266, 1313,    0,   76,  144],\n",
            "        [   4,    4,    0,  650,    7],\n",
            "        [   2,  385,    0,   64,    4],\n",
            "        [   0,  169,    0,  306,    2],\n",
            "        [   0,   83,    0, 1014,    0],\n",
            "        [   0,    4,    0,   66,    0],\n",
            "        [   0,    2,    0,    2,    0]])\n",
            "Mask:\n",
            "tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [False,  True, False,  True,  True],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False],\n",
            "        [False,  True, False,  True, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PREogJiEKd2g",
        "colab_type": "text"
      },
      "source": [
        "## Constructing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWpUln5TKdjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "# We now define our GRU, taking into account the size of our input, the number of layers, and whether we should implement dropout\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True) #Applying bidirectionality\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "# We do this by first embedding our input sentences and then using the pack_padded_sequence function on our embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # This function \"packs\" our padded sequence so that all of our inputs are of the same length\n",
        "        # We then pass out the packed sequences through our GRU to perform a forward pass\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs) # unpack our padding and sum the GRU outputs\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39RfkAaVOuxt",
        "colab_type": "text"
      },
      "source": [
        "## Constructing the Attention Module\n",
        "* we will apply to our encoder so that we can learn from the relevant parts of the encoder's output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXKOHC8oOyBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output): \n",
        "      # This function simply calculates the dot product of our encoder output with the output of our hidden state by our encoder\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "      # First, calculate the attention weights/energies based on the dot_score method, then transpose the results, and return the softmax transformed probability scores\n",
        "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "        attn_energies = attn_energies.t()\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEOmqGhWP9y2",
        "colab_type": "text"
      },
      "source": [
        "##Constructing the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7sh2d9dQCWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "# We will create an embedding layer and a corresponding dropout layer\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "# We use GRUs again for our decoder; however, this time, we do not need to make our GRU layer bidirectional as we will be decoding the output from our encoder sequentially\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "# create two linear layers—one regular layer for calculating our output\n",
        "        self.concat = nn.Linear(2 * hidden_size, hidden_size)\n",
        "# one layer that can be used for concatenation\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "      # the forward pass will be used one step (word) at a time\n",
        "\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "      # We start by getting the embedding of the current input word and making a forward pass through the GRU layer to get our output and hidden states:\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "      # we use the attention module to get the attention weights from the GRU output\n",
        "      # These weights are then multiplied by the encoder outputs to effectively give us a weighted sum of our attention weights and our encoder output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "      # We then concatenate our weighted context vector with the output of our GRU and apply a tanh function to get out final concatenated output\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "      # we simply use this final concatenated output to predict the next word and apply a softmax function\n",
        "      # The forward pass finally returns this output, along with the final hidden state\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output, hidden"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jki84aJ8Tj0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a loss function that applies a Boolean mask over our outputs and only calculates the loss of the non-padded tokens\n",
        "def NLLMaskLoss(inp, target, mask):\n",
        "  # we calculate cross-entropy loss across the whole output tensors.\n",
        "  # However, to get the total loss, we only average over the elements of the tensor that are selected by the Boolean mask\n",
        "    TotalN = mask.sum()\n",
        "    CELoss = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = CELoss.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, TotalN.item()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTajYIH_UUln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the majority of our training, we need two main functions—one function, train(), \n",
        "# which performs training on a single batch of our training data and another function, trainIters()\n",
        "# which iterates through our whole dataset and calls train() on each of the individual batches. \n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=max_length):\n",
        "#  defining train() in order to train on a single batch of data\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "# perform a forward pass of the inputs and sequence lengths though the encoder to get the output and hidden states:\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "# we create our initial decoder input, starting with SOS tokens for each sentence. We then set the initial hidden state of our decoder to be equal to that of the encoder\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    use_TF = True if random.random() < teacher_forcing_ratio else False\n",
        "# implement teacher forcing \n",
        "    if use_TF:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            mask_loss, nTotal = NLLMaskLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "      # if we do need to implement teacher forcing, run the following code. We pass each of our sequence batches through the decoder to obtain our output\n",
        "      # We then set the next input as the true output (target). Finally, we calculate and accumulate the loss using our loss function and print this to the console\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            _, topi = decoder_output.topk(1)\n",
        "          # If we do not implement teacher forcing on a given batch, the procedure is almost identical\n",
        "          # However, instead of using the true output as the next input into the sequence, we use the one generated by the model\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            mask_loss, nTotal = NLLMaskLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "#  as with all of our models, the final steps are to perform backpropagation, \n",
        "# implement gradient clipping, and step through both of our encoder and decoder optimizers to update the weights using gradient descent\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaSDoSDhYiwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer,\\\n",
        "               decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, \\\n",
        "               save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "# repeatedly calls our training function on different batches of input data.\n",
        "# plitting our data into batches using the batch2Train\n",
        "    training_batches = [batch2Train(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "# create a few variables that will allow us to count iterations and keep track of the total loss over each epoch\n",
        "    print('Starting ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "# define our training loop. For each iteration, we get a training batch from our list of batches\n",
        "    print(\"Beginning Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract relevant fields from our batch and run a single training iteration using these parameters\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "# On every iteration, we also make sure we print our progress so far, keeping track of how many iterations we have completed and what our loss was for each epoch\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent done: {:.1f}%; Mean loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "# we also need to save our model state after every few epochs\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LhoO2ffa3V9",
        "colab_type": "text"
      },
      "source": [
        "## Defining a Greedy encoder\n",
        "* Defining a class that will allow us to decode the encoded input and produce text\n",
        "* This simply means that at each step of the decoder, our model takes the word with the highest predicted probability as the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laxewv8zbBtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "      # initializing the GreedyEncoder() class with our pretrained encoder and decoder\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "      # pass the input through our encoder to get our encoder's output and hidden state\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "      # create the decoder input with SOS tokens and initialize the tensors to append decoded words to (initialized as a single zero value)\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "      # add a max function to obtain the highest-scoring predicted word and its score, which we then append to the all_tokens and all_scores variables\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "      # After the whole sequence has been iterated over, we return the complete predicted sentence:\n",
        "\n",
        "        return all_tokens, all_scores"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3vztyC_bERg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=max_length):\n",
        "  #  takes our input function and returns the predicted output words\n",
        "    indices = [indexFromSentence(voc, sentence)]# We start by transforming our input sentence into indices using our vocabulary\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indices])\n",
        "    input_batch = torch.LongTensor(indices).transpose(0, 1) # obtain a tensor of the lengths of each of these sentences and transpose it\n",
        "  # we assign our lengths and input tensors to the relevant devices.\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "  # Next, run the inputs through the searcher (GreedySearchDecoder) to obtain the word indices of the predicted output.\n",
        "  # Finally, we transform these word indices back into word tokens before returning them as the function output\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def runChatBot(encoder, decoder, searcher, voc):\n",
        "  # we create a runchatbot function, which acts as the interface with our chatbot\n",
        "  # This function takes human-typed input and prints the chatbot's response. \n",
        "  # We create this function as a while loop that continues until we terminate the function or type quit as our input\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            input_sentence = input('> ') #Take input and normalize it\n",
        "            if input_sentence == 'quit': break\n",
        "            input_sentence = cleanString(input_sentence)\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # we take these output words and format them, ignoring the EOS and padding tokens, before printing the chatbot's response\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')] \n",
        "            print('Response:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Unknown Word\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN-UfcW2di3I",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obOskRD5dm4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2f4e6ceb-650e-4159-b273-4a8738f22a8e"
      },
      "source": [
        "model_name = 'chatbot_model'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.15\n",
        "batch_size = 64\n",
        "\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "\n",
        "if loadFilename:\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# We first load our embeddings from the vocabulary\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "# If we have already trained a model, we can load the trained embeddings layer\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# creating model instances using the defined hyperparameters\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = DecoderRNN(embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "# if we have already trained a model, we simply load the trained model states into our models\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4OT6801eHrL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51f3ea3a-157c-4d77-bc9f-23f6fccee779"
      },
      "source": [
        "save_dir = './'\n",
        "\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "\n",
        "epochs = 4000\n",
        "\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "encoder.train() # switch models to train mode\n",
        "decoder.train()\n",
        "\n",
        "print('Building optimizers ...')\n",
        "#  create optimizers for both the encoder and decoder\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "# The final step before running the training is to make sure CUDA is configured to be called if you wish to use GPU training\n",
        "# To do this, we simply loop through the optimizer states for both the encoder and decoder and enable CUDA across all of the states\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, epochs, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Starting ...\n",
            "Beginning Training...\n",
            "Iteration: 1; Percent done: 0.0%; Mean loss: 8.9635\n",
            "Iteration: 2; Percent done: 0.1%; Mean loss: 8.8302\n",
            "Iteration: 3; Percent done: 0.1%; Mean loss: 8.7005\n",
            "Iteration: 4; Percent done: 0.1%; Mean loss: 8.4026\n",
            "Iteration: 5; Percent done: 0.1%; Mean loss: 8.0496\n",
            "Iteration: 6; Percent done: 0.1%; Mean loss: 7.4402\n",
            "Iteration: 7; Percent done: 0.2%; Mean loss: 7.0134\n",
            "Iteration: 8; Percent done: 0.2%; Mean loss: 6.8891\n",
            "Iteration: 9; Percent done: 0.2%; Mean loss: 6.8356\n",
            "Iteration: 10; Percent done: 0.2%; Mean loss: 6.5928\n",
            "Iteration: 11; Percent done: 0.3%; Mean loss: 6.4639\n",
            "Iteration: 12; Percent done: 0.3%; Mean loss: 5.9568\n",
            "Iteration: 13; Percent done: 0.3%; Mean loss: 5.7199\n",
            "Iteration: 14; Percent done: 0.4%; Mean loss: 5.6858\n",
            "Iteration: 15; Percent done: 0.4%; Mean loss: 5.3308\n",
            "Iteration: 16; Percent done: 0.4%; Mean loss: 5.3953\n",
            "Iteration: 17; Percent done: 0.4%; Mean loss: 5.4125\n",
            "Iteration: 18; Percent done: 0.4%; Mean loss: 5.2233\n",
            "Iteration: 19; Percent done: 0.5%; Mean loss: 5.1063\n",
            "Iteration: 20; Percent done: 0.5%; Mean loss: 4.9270\n",
            "Iteration: 21; Percent done: 0.5%; Mean loss: 5.1243\n",
            "Iteration: 22; Percent done: 0.5%; Mean loss: 4.9327\n",
            "Iteration: 23; Percent done: 0.6%; Mean loss: 4.9898\n",
            "Iteration: 24; Percent done: 0.6%; Mean loss: 4.8634\n",
            "Iteration: 25; Percent done: 0.6%; Mean loss: 4.7487\n",
            "Iteration: 26; Percent done: 0.7%; Mean loss: 4.9887\n",
            "Iteration: 27; Percent done: 0.7%; Mean loss: 4.9665\n",
            "Iteration: 28; Percent done: 0.7%; Mean loss: 4.8642\n",
            "Iteration: 29; Percent done: 0.7%; Mean loss: 4.6579\n",
            "Iteration: 30; Percent done: 0.8%; Mean loss: 4.7741\n",
            "Iteration: 31; Percent done: 0.8%; Mean loss: 4.7375\n",
            "Iteration: 32; Percent done: 0.8%; Mean loss: 4.7505\n",
            "Iteration: 33; Percent done: 0.8%; Mean loss: 4.7718\n",
            "Iteration: 34; Percent done: 0.9%; Mean loss: 4.7388\n",
            "Iteration: 35; Percent done: 0.9%; Mean loss: 4.7069\n",
            "Iteration: 36; Percent done: 0.9%; Mean loss: 4.5496\n",
            "Iteration: 37; Percent done: 0.9%; Mean loss: 4.7255\n",
            "Iteration: 38; Percent done: 0.9%; Mean loss: 4.8568\n",
            "Iteration: 39; Percent done: 1.0%; Mean loss: 5.0234\n",
            "Iteration: 40; Percent done: 1.0%; Mean loss: 4.8661\n",
            "Iteration: 41; Percent done: 1.0%; Mean loss: 4.9828\n",
            "Iteration: 42; Percent done: 1.1%; Mean loss: 4.6959\n",
            "Iteration: 43; Percent done: 1.1%; Mean loss: 4.5455\n",
            "Iteration: 44; Percent done: 1.1%; Mean loss: 4.8059\n",
            "Iteration: 45; Percent done: 1.1%; Mean loss: 4.5328\n",
            "Iteration: 46; Percent done: 1.1%; Mean loss: 4.5534\n",
            "Iteration: 47; Percent done: 1.2%; Mean loss: 4.7578\n",
            "Iteration: 48; Percent done: 1.2%; Mean loss: 4.4934\n",
            "Iteration: 49; Percent done: 1.2%; Mean loss: 4.7134\n",
            "Iteration: 50; Percent done: 1.2%; Mean loss: 4.6894\n",
            "Iteration: 51; Percent done: 1.3%; Mean loss: 4.5430\n",
            "Iteration: 52; Percent done: 1.3%; Mean loss: 4.6667\n",
            "Iteration: 53; Percent done: 1.3%; Mean loss: 4.3847\n",
            "Iteration: 54; Percent done: 1.4%; Mean loss: 4.5849\n",
            "Iteration: 55; Percent done: 1.4%; Mean loss: 4.5995\n",
            "Iteration: 56; Percent done: 1.4%; Mean loss: 4.6004\n",
            "Iteration: 57; Percent done: 1.4%; Mean loss: 4.7481\n",
            "Iteration: 58; Percent done: 1.5%; Mean loss: 4.5294\n",
            "Iteration: 59; Percent done: 1.5%; Mean loss: 4.8103\n",
            "Iteration: 60; Percent done: 1.5%; Mean loss: 4.4118\n",
            "Iteration: 61; Percent done: 1.5%; Mean loss: 4.6275\n",
            "Iteration: 62; Percent done: 1.6%; Mean loss: 4.6345\n",
            "Iteration: 63; Percent done: 1.6%; Mean loss: 4.4736\n",
            "Iteration: 64; Percent done: 1.6%; Mean loss: 4.4123\n",
            "Iteration: 65; Percent done: 1.6%; Mean loss: 4.2940\n",
            "Iteration: 66; Percent done: 1.7%; Mean loss: 4.7628\n",
            "Iteration: 67; Percent done: 1.7%; Mean loss: 4.4851\n",
            "Iteration: 68; Percent done: 1.7%; Mean loss: 4.6226\n",
            "Iteration: 69; Percent done: 1.7%; Mean loss: 4.6210\n",
            "Iteration: 70; Percent done: 1.8%; Mean loss: 4.7772\n",
            "Iteration: 71; Percent done: 1.8%; Mean loss: 4.6318\n",
            "Iteration: 72; Percent done: 1.8%; Mean loss: 4.4275\n",
            "Iteration: 73; Percent done: 1.8%; Mean loss: 4.4489\n",
            "Iteration: 74; Percent done: 1.8%; Mean loss: 4.4718\n",
            "Iteration: 75; Percent done: 1.9%; Mean loss: 4.4975\n",
            "Iteration: 76; Percent done: 1.9%; Mean loss: 4.2900\n",
            "Iteration: 77; Percent done: 1.9%; Mean loss: 4.5101\n",
            "Iteration: 78; Percent done: 1.9%; Mean loss: 4.4629\n",
            "Iteration: 79; Percent done: 2.0%; Mean loss: 4.2821\n",
            "Iteration: 80; Percent done: 2.0%; Mean loss: 4.7640\n",
            "Iteration: 81; Percent done: 2.0%; Mean loss: 4.1740\n",
            "Iteration: 82; Percent done: 2.1%; Mean loss: 4.3770\n",
            "Iteration: 83; Percent done: 2.1%; Mean loss: 4.4913\n",
            "Iteration: 84; Percent done: 2.1%; Mean loss: 4.5243\n",
            "Iteration: 85; Percent done: 2.1%; Mean loss: 4.5951\n",
            "Iteration: 86; Percent done: 2.1%; Mean loss: 4.3014\n",
            "Iteration: 87; Percent done: 2.2%; Mean loss: 4.5974\n",
            "Iteration: 88; Percent done: 2.2%; Mean loss: 4.4660\n",
            "Iteration: 89; Percent done: 2.2%; Mean loss: 4.5176\n",
            "Iteration: 90; Percent done: 2.2%; Mean loss: 4.5670\n",
            "Iteration: 91; Percent done: 2.3%; Mean loss: 4.4204\n",
            "Iteration: 92; Percent done: 2.3%; Mean loss: 4.2638\n",
            "Iteration: 93; Percent done: 2.3%; Mean loss: 4.4980\n",
            "Iteration: 94; Percent done: 2.4%; Mean loss: 4.6859\n",
            "Iteration: 95; Percent done: 2.4%; Mean loss: 4.5238\n",
            "Iteration: 96; Percent done: 2.4%; Mean loss: 4.7391\n",
            "Iteration: 97; Percent done: 2.4%; Mean loss: 4.4559\n",
            "Iteration: 98; Percent done: 2.5%; Mean loss: 4.6292\n",
            "Iteration: 99; Percent done: 2.5%; Mean loss: 4.4515\n",
            "Iteration: 100; Percent done: 2.5%; Mean loss: 4.4666\n",
            "Iteration: 101; Percent done: 2.5%; Mean loss: 4.4349\n",
            "Iteration: 102; Percent done: 2.5%; Mean loss: 4.4180\n",
            "Iteration: 103; Percent done: 2.6%; Mean loss: 4.3142\n",
            "Iteration: 104; Percent done: 2.6%; Mean loss: 4.4620\n",
            "Iteration: 105; Percent done: 2.6%; Mean loss: 4.5047\n",
            "Iteration: 106; Percent done: 2.6%; Mean loss: 4.3766\n",
            "Iteration: 107; Percent done: 2.7%; Mean loss: 4.2724\n",
            "Iteration: 108; Percent done: 2.7%; Mean loss: 4.4716\n",
            "Iteration: 109; Percent done: 2.7%; Mean loss: 4.5765\n",
            "Iteration: 110; Percent done: 2.8%; Mean loss: 4.5154\n",
            "Iteration: 111; Percent done: 2.8%; Mean loss: 4.5775\n",
            "Iteration: 112; Percent done: 2.8%; Mean loss: 4.2898\n",
            "Iteration: 113; Percent done: 2.8%; Mean loss: 4.4542\n",
            "Iteration: 114; Percent done: 2.9%; Mean loss: 4.2805\n",
            "Iteration: 115; Percent done: 2.9%; Mean loss: 4.3835\n",
            "Iteration: 116; Percent done: 2.9%; Mean loss: 4.2411\n",
            "Iteration: 117; Percent done: 2.9%; Mean loss: 4.4633\n",
            "Iteration: 118; Percent done: 2.9%; Mean loss: 4.5845\n",
            "Iteration: 119; Percent done: 3.0%; Mean loss: 4.3956\n",
            "Iteration: 120; Percent done: 3.0%; Mean loss: 4.2777\n",
            "Iteration: 121; Percent done: 3.0%; Mean loss: 4.6680\n",
            "Iteration: 122; Percent done: 3.0%; Mean loss: 4.4131\n",
            "Iteration: 123; Percent done: 3.1%; Mean loss: 4.2981\n",
            "Iteration: 124; Percent done: 3.1%; Mean loss: 4.5684\n",
            "Iteration: 125; Percent done: 3.1%; Mean loss: 4.4756\n",
            "Iteration: 126; Percent done: 3.1%; Mean loss: 4.2332\n",
            "Iteration: 127; Percent done: 3.2%; Mean loss: 4.4327\n",
            "Iteration: 128; Percent done: 3.2%; Mean loss: 4.4028\n",
            "Iteration: 129; Percent done: 3.2%; Mean loss: 4.4008\n",
            "Iteration: 130; Percent done: 3.2%; Mean loss: 4.5160\n",
            "Iteration: 131; Percent done: 3.3%; Mean loss: 4.1646\n",
            "Iteration: 132; Percent done: 3.3%; Mean loss: 4.2799\n",
            "Iteration: 133; Percent done: 3.3%; Mean loss: 4.5981\n",
            "Iteration: 134; Percent done: 3.4%; Mean loss: 4.2790\n",
            "Iteration: 135; Percent done: 3.4%; Mean loss: 4.0514\n",
            "Iteration: 136; Percent done: 3.4%; Mean loss: 4.2896\n",
            "Iteration: 137; Percent done: 3.4%; Mean loss: 4.4698\n",
            "Iteration: 138; Percent done: 3.5%; Mean loss: 4.3278\n",
            "Iteration: 139; Percent done: 3.5%; Mean loss: 4.2697\n",
            "Iteration: 140; Percent done: 3.5%; Mean loss: 4.2394\n",
            "Iteration: 141; Percent done: 3.5%; Mean loss: 4.5137\n",
            "Iteration: 142; Percent done: 3.5%; Mean loss: 4.0725\n",
            "Iteration: 143; Percent done: 3.6%; Mean loss: 4.5574\n",
            "Iteration: 144; Percent done: 3.6%; Mean loss: 4.3845\n",
            "Iteration: 145; Percent done: 3.6%; Mean loss: 4.3031\n",
            "Iteration: 146; Percent done: 3.6%; Mean loss: 4.3395\n",
            "Iteration: 147; Percent done: 3.7%; Mean loss: 4.3635\n",
            "Iteration: 148; Percent done: 3.7%; Mean loss: 4.4110\n",
            "Iteration: 149; Percent done: 3.7%; Mean loss: 4.2358\n",
            "Iteration: 150; Percent done: 3.8%; Mean loss: 4.4602\n",
            "Iteration: 151; Percent done: 3.8%; Mean loss: 4.2826\n",
            "Iteration: 152; Percent done: 3.8%; Mean loss: 4.4251\n",
            "Iteration: 153; Percent done: 3.8%; Mean loss: 4.1914\n",
            "Iteration: 154; Percent done: 3.9%; Mean loss: 4.3652\n",
            "Iteration: 155; Percent done: 3.9%; Mean loss: 4.4110\n",
            "Iteration: 156; Percent done: 3.9%; Mean loss: 4.2915\n",
            "Iteration: 157; Percent done: 3.9%; Mean loss: 4.5037\n",
            "Iteration: 158; Percent done: 4.0%; Mean loss: 4.2610\n",
            "Iteration: 159; Percent done: 4.0%; Mean loss: 4.1590\n",
            "Iteration: 160; Percent done: 4.0%; Mean loss: 4.2985\n",
            "Iteration: 161; Percent done: 4.0%; Mean loss: 4.2273\n",
            "Iteration: 162; Percent done: 4.0%; Mean loss: 4.0279\n",
            "Iteration: 163; Percent done: 4.1%; Mean loss: 4.1540\n",
            "Iteration: 164; Percent done: 4.1%; Mean loss: 4.1130\n",
            "Iteration: 165; Percent done: 4.1%; Mean loss: 4.0965\n",
            "Iteration: 166; Percent done: 4.2%; Mean loss: 4.4214\n",
            "Iteration: 167; Percent done: 4.2%; Mean loss: 4.4780\n",
            "Iteration: 168; Percent done: 4.2%; Mean loss: 4.3619\n",
            "Iteration: 169; Percent done: 4.2%; Mean loss: 4.3691\n",
            "Iteration: 170; Percent done: 4.2%; Mean loss: 4.1391\n",
            "Iteration: 171; Percent done: 4.3%; Mean loss: 4.1584\n",
            "Iteration: 172; Percent done: 4.3%; Mean loss: 4.2183\n",
            "Iteration: 173; Percent done: 4.3%; Mean loss: 4.3219\n",
            "Iteration: 174; Percent done: 4.3%; Mean loss: 4.3024\n",
            "Iteration: 175; Percent done: 4.4%; Mean loss: 4.0830\n",
            "Iteration: 176; Percent done: 4.4%; Mean loss: 4.2224\n",
            "Iteration: 177; Percent done: 4.4%; Mean loss: 4.0868\n",
            "Iteration: 178; Percent done: 4.5%; Mean loss: 4.1531\n",
            "Iteration: 179; Percent done: 4.5%; Mean loss: 4.3216\n",
            "Iteration: 180; Percent done: 4.5%; Mean loss: 4.1432\n",
            "Iteration: 181; Percent done: 4.5%; Mean loss: 4.1657\n",
            "Iteration: 182; Percent done: 4.5%; Mean loss: 4.1294\n",
            "Iteration: 183; Percent done: 4.6%; Mean loss: 4.2383\n",
            "Iteration: 184; Percent done: 4.6%; Mean loss: 4.4551\n",
            "Iteration: 185; Percent done: 4.6%; Mean loss: 4.1426\n",
            "Iteration: 186; Percent done: 4.7%; Mean loss: 4.2318\n",
            "Iteration: 187; Percent done: 4.7%; Mean loss: 4.2247\n",
            "Iteration: 188; Percent done: 4.7%; Mean loss: 4.2807\n",
            "Iteration: 189; Percent done: 4.7%; Mean loss: 4.1628\n",
            "Iteration: 190; Percent done: 4.8%; Mean loss: 4.2164\n",
            "Iteration: 191; Percent done: 4.8%; Mean loss: 4.3559\n",
            "Iteration: 192; Percent done: 4.8%; Mean loss: 4.0885\n",
            "Iteration: 193; Percent done: 4.8%; Mean loss: 3.9778\n",
            "Iteration: 194; Percent done: 4.9%; Mean loss: 4.1652\n",
            "Iteration: 195; Percent done: 4.9%; Mean loss: 4.2542\n",
            "Iteration: 196; Percent done: 4.9%; Mean loss: 4.2344\n",
            "Iteration: 197; Percent done: 4.9%; Mean loss: 4.1426\n",
            "Iteration: 198; Percent done: 5.0%; Mean loss: 4.1100\n",
            "Iteration: 199; Percent done: 5.0%; Mean loss: 4.0134\n",
            "Iteration: 200; Percent done: 5.0%; Mean loss: 4.1792\n",
            "Iteration: 201; Percent done: 5.0%; Mean loss: 4.1486\n",
            "Iteration: 202; Percent done: 5.1%; Mean loss: 4.0761\n",
            "Iteration: 203; Percent done: 5.1%; Mean loss: 4.0987\n",
            "Iteration: 204; Percent done: 5.1%; Mean loss: 4.1647\n",
            "Iteration: 205; Percent done: 5.1%; Mean loss: 3.9393\n",
            "Iteration: 206; Percent done: 5.1%; Mean loss: 4.1741\n",
            "Iteration: 207; Percent done: 5.2%; Mean loss: 3.9555\n",
            "Iteration: 208; Percent done: 5.2%; Mean loss: 4.0974\n",
            "Iteration: 209; Percent done: 5.2%; Mean loss: 4.2640\n",
            "Iteration: 210; Percent done: 5.2%; Mean loss: 3.8084\n",
            "Iteration: 211; Percent done: 5.3%; Mean loss: 4.2044\n",
            "Iteration: 212; Percent done: 5.3%; Mean loss: 3.9519\n",
            "Iteration: 213; Percent done: 5.3%; Mean loss: 4.2866\n",
            "Iteration: 214; Percent done: 5.3%; Mean loss: 3.7967\n",
            "Iteration: 215; Percent done: 5.4%; Mean loss: 4.1512\n",
            "Iteration: 216; Percent done: 5.4%; Mean loss: 3.7809\n",
            "Iteration: 217; Percent done: 5.4%; Mean loss: 4.1765\n",
            "Iteration: 218; Percent done: 5.5%; Mean loss: 4.1967\n",
            "Iteration: 219; Percent done: 5.5%; Mean loss: 4.0621\n",
            "Iteration: 220; Percent done: 5.5%; Mean loss: 4.1543\n",
            "Iteration: 221; Percent done: 5.5%; Mean loss: 4.0042\n",
            "Iteration: 222; Percent done: 5.5%; Mean loss: 3.9609\n",
            "Iteration: 223; Percent done: 5.6%; Mean loss: 4.3696\n",
            "Iteration: 224; Percent done: 5.6%; Mean loss: 3.9250\n",
            "Iteration: 225; Percent done: 5.6%; Mean loss: 4.3127\n",
            "Iteration: 226; Percent done: 5.7%; Mean loss: 4.1127\n",
            "Iteration: 227; Percent done: 5.7%; Mean loss: 3.8619\n",
            "Iteration: 228; Percent done: 5.7%; Mean loss: 4.1107\n",
            "Iteration: 229; Percent done: 5.7%; Mean loss: 4.0026\n",
            "Iteration: 230; Percent done: 5.8%; Mean loss: 4.0004\n",
            "Iteration: 231; Percent done: 5.8%; Mean loss: 3.8509\n",
            "Iteration: 232; Percent done: 5.8%; Mean loss: 3.9655\n",
            "Iteration: 233; Percent done: 5.8%; Mean loss: 3.7590\n",
            "Iteration: 234; Percent done: 5.9%; Mean loss: 4.2227\n",
            "Iteration: 235; Percent done: 5.9%; Mean loss: 3.9175\n",
            "Iteration: 236; Percent done: 5.9%; Mean loss: 4.1192\n",
            "Iteration: 237; Percent done: 5.9%; Mean loss: 3.9978\n",
            "Iteration: 238; Percent done: 5.9%; Mean loss: 3.8983\n",
            "Iteration: 239; Percent done: 6.0%; Mean loss: 4.1621\n",
            "Iteration: 240; Percent done: 6.0%; Mean loss: 3.9949\n",
            "Iteration: 241; Percent done: 6.0%; Mean loss: 4.0270\n",
            "Iteration: 242; Percent done: 6.0%; Mean loss: 4.0740\n",
            "Iteration: 243; Percent done: 6.1%; Mean loss: 4.0407\n",
            "Iteration: 244; Percent done: 6.1%; Mean loss: 3.9568\n",
            "Iteration: 245; Percent done: 6.1%; Mean loss: 3.8682\n",
            "Iteration: 246; Percent done: 6.2%; Mean loss: 4.1850\n",
            "Iteration: 247; Percent done: 6.2%; Mean loss: 3.9103\n",
            "Iteration: 248; Percent done: 6.2%; Mean loss: 3.9442\n",
            "Iteration: 249; Percent done: 6.2%; Mean loss: 4.1647\n",
            "Iteration: 250; Percent done: 6.2%; Mean loss: 4.0457\n",
            "Iteration: 251; Percent done: 6.3%; Mean loss: 3.9403\n",
            "Iteration: 252; Percent done: 6.3%; Mean loss: 3.9248\n",
            "Iteration: 253; Percent done: 6.3%; Mean loss: 4.4056\n",
            "Iteration: 254; Percent done: 6.3%; Mean loss: 4.0675\n",
            "Iteration: 255; Percent done: 6.4%; Mean loss: 3.9079\n",
            "Iteration: 256; Percent done: 6.4%; Mean loss: 4.1210\n",
            "Iteration: 257; Percent done: 6.4%; Mean loss: 3.6912\n",
            "Iteration: 258; Percent done: 6.5%; Mean loss: 3.7792\n",
            "Iteration: 259; Percent done: 6.5%; Mean loss: 3.9121\n",
            "Iteration: 260; Percent done: 6.5%; Mean loss: 3.9453\n",
            "Iteration: 261; Percent done: 6.5%; Mean loss: 3.8727\n",
            "Iteration: 262; Percent done: 6.6%; Mean loss: 3.9969\n",
            "Iteration: 263; Percent done: 6.6%; Mean loss: 4.0497\n",
            "Iteration: 264; Percent done: 6.6%; Mean loss: 3.7578\n",
            "Iteration: 265; Percent done: 6.6%; Mean loss: 3.7957\n",
            "Iteration: 266; Percent done: 6.7%; Mean loss: 3.8847\n",
            "Iteration: 267; Percent done: 6.7%; Mean loss: 3.7993\n",
            "Iteration: 268; Percent done: 6.7%; Mean loss: 3.8993\n",
            "Iteration: 269; Percent done: 6.7%; Mean loss: 3.6874\n",
            "Iteration: 270; Percent done: 6.8%; Mean loss: 3.9417\n",
            "Iteration: 271; Percent done: 6.8%; Mean loss: 4.0416\n",
            "Iteration: 272; Percent done: 6.8%; Mean loss: 4.1611\n",
            "Iteration: 273; Percent done: 6.8%; Mean loss: 4.0571\n",
            "Iteration: 274; Percent done: 6.9%; Mean loss: 4.0494\n",
            "Iteration: 275; Percent done: 6.9%; Mean loss: 4.0718\n",
            "Iteration: 276; Percent done: 6.9%; Mean loss: 4.0316\n",
            "Iteration: 277; Percent done: 6.9%; Mean loss: 4.0265\n",
            "Iteration: 278; Percent done: 7.0%; Mean loss: 3.7536\n",
            "Iteration: 279; Percent done: 7.0%; Mean loss: 3.7830\n",
            "Iteration: 280; Percent done: 7.0%; Mean loss: 3.8433\n",
            "Iteration: 281; Percent done: 7.0%; Mean loss: 3.9690\n",
            "Iteration: 282; Percent done: 7.0%; Mean loss: 3.7941\n",
            "Iteration: 283; Percent done: 7.1%; Mean loss: 3.6934\n",
            "Iteration: 284; Percent done: 7.1%; Mean loss: 3.8569\n",
            "Iteration: 285; Percent done: 7.1%; Mean loss: 4.0943\n",
            "Iteration: 286; Percent done: 7.1%; Mean loss: 4.0314\n",
            "Iteration: 287; Percent done: 7.2%; Mean loss: 3.9966\n",
            "Iteration: 288; Percent done: 7.2%; Mean loss: 3.9290\n",
            "Iteration: 289; Percent done: 7.2%; Mean loss: 3.9025\n",
            "Iteration: 290; Percent done: 7.2%; Mean loss: 3.8329\n",
            "Iteration: 291; Percent done: 7.3%; Mean loss: 4.1162\n",
            "Iteration: 292; Percent done: 7.3%; Mean loss: 4.0624\n",
            "Iteration: 293; Percent done: 7.3%; Mean loss: 3.8366\n",
            "Iteration: 294; Percent done: 7.3%; Mean loss: 4.0644\n",
            "Iteration: 295; Percent done: 7.4%; Mean loss: 3.9361\n",
            "Iteration: 296; Percent done: 7.4%; Mean loss: 3.9677\n",
            "Iteration: 297; Percent done: 7.4%; Mean loss: 4.3156\n",
            "Iteration: 298; Percent done: 7.4%; Mean loss: 3.7522\n",
            "Iteration: 299; Percent done: 7.5%; Mean loss: 3.6963\n",
            "Iteration: 300; Percent done: 7.5%; Mean loss: 4.0987\n",
            "Iteration: 301; Percent done: 7.5%; Mean loss: 3.9689\n",
            "Iteration: 302; Percent done: 7.5%; Mean loss: 4.0024\n",
            "Iteration: 303; Percent done: 7.6%; Mean loss: 4.1287\n",
            "Iteration: 304; Percent done: 7.6%; Mean loss: 4.0648\n",
            "Iteration: 305; Percent done: 7.6%; Mean loss: 3.9737\n",
            "Iteration: 306; Percent done: 7.6%; Mean loss: 3.8491\n",
            "Iteration: 307; Percent done: 7.7%; Mean loss: 3.7770\n",
            "Iteration: 308; Percent done: 7.7%; Mean loss: 3.9516\n",
            "Iteration: 309; Percent done: 7.7%; Mean loss: 3.8845\n",
            "Iteration: 310; Percent done: 7.8%; Mean loss: 3.8682\n",
            "Iteration: 311; Percent done: 7.8%; Mean loss: 3.8888\n",
            "Iteration: 312; Percent done: 7.8%; Mean loss: 3.6485\n",
            "Iteration: 313; Percent done: 7.8%; Mean loss: 3.9049\n",
            "Iteration: 314; Percent done: 7.8%; Mean loss: 3.8618\n",
            "Iteration: 315; Percent done: 7.9%; Mean loss: 3.9931\n",
            "Iteration: 316; Percent done: 7.9%; Mean loss: 4.0828\n",
            "Iteration: 317; Percent done: 7.9%; Mean loss: 3.8827\n",
            "Iteration: 318; Percent done: 8.0%; Mean loss: 4.0088\n",
            "Iteration: 319; Percent done: 8.0%; Mean loss: 4.1159\n",
            "Iteration: 320; Percent done: 8.0%; Mean loss: 3.8946\n",
            "Iteration: 321; Percent done: 8.0%; Mean loss: 3.8495\n",
            "Iteration: 322; Percent done: 8.1%; Mean loss: 3.8340\n",
            "Iteration: 323; Percent done: 8.1%; Mean loss: 3.8267\n",
            "Iteration: 324; Percent done: 8.1%; Mean loss: 3.9784\n",
            "Iteration: 325; Percent done: 8.1%; Mean loss: 3.8098\n",
            "Iteration: 326; Percent done: 8.2%; Mean loss: 4.2195\n",
            "Iteration: 327; Percent done: 8.2%; Mean loss: 3.8916\n",
            "Iteration: 328; Percent done: 8.2%; Mean loss: 4.1989\n",
            "Iteration: 329; Percent done: 8.2%; Mean loss: 3.7737\n",
            "Iteration: 330; Percent done: 8.2%; Mean loss: 3.8325\n",
            "Iteration: 331; Percent done: 8.3%; Mean loss: 4.0930\n",
            "Iteration: 332; Percent done: 8.3%; Mean loss: 3.9176\n",
            "Iteration: 333; Percent done: 8.3%; Mean loss: 4.0518\n",
            "Iteration: 334; Percent done: 8.3%; Mean loss: 3.8576\n",
            "Iteration: 335; Percent done: 8.4%; Mean loss: 4.1225\n",
            "Iteration: 336; Percent done: 8.4%; Mean loss: 3.8191\n",
            "Iteration: 337; Percent done: 8.4%; Mean loss: 3.8167\n",
            "Iteration: 338; Percent done: 8.5%; Mean loss: 3.6165\n",
            "Iteration: 339; Percent done: 8.5%; Mean loss: 3.9096\n",
            "Iteration: 340; Percent done: 8.5%; Mean loss: 3.8601\n",
            "Iteration: 341; Percent done: 8.5%; Mean loss: 3.8239\n",
            "Iteration: 342; Percent done: 8.6%; Mean loss: 3.5615\n",
            "Iteration: 343; Percent done: 8.6%; Mean loss: 3.9749\n",
            "Iteration: 344; Percent done: 8.6%; Mean loss: 4.1823\n",
            "Iteration: 345; Percent done: 8.6%; Mean loss: 3.8596\n",
            "Iteration: 346; Percent done: 8.6%; Mean loss: 3.9074\n",
            "Iteration: 347; Percent done: 8.7%; Mean loss: 4.0115\n",
            "Iteration: 348; Percent done: 8.7%; Mean loss: 3.8230\n",
            "Iteration: 349; Percent done: 8.7%; Mean loss: 3.5858\n",
            "Iteration: 350; Percent done: 8.8%; Mean loss: 3.7142\n",
            "Iteration: 351; Percent done: 8.8%; Mean loss: 4.2444\n",
            "Iteration: 352; Percent done: 8.8%; Mean loss: 3.9421\n",
            "Iteration: 353; Percent done: 8.8%; Mean loss: 3.7822\n",
            "Iteration: 354; Percent done: 8.8%; Mean loss: 3.9911\n",
            "Iteration: 355; Percent done: 8.9%; Mean loss: 3.9789\n",
            "Iteration: 356; Percent done: 8.9%; Mean loss: 3.7315\n",
            "Iteration: 357; Percent done: 8.9%; Mean loss: 3.7552\n",
            "Iteration: 358; Percent done: 8.9%; Mean loss: 3.7916\n",
            "Iteration: 359; Percent done: 9.0%; Mean loss: 3.8863\n",
            "Iteration: 360; Percent done: 9.0%; Mean loss: 3.8357\n",
            "Iteration: 361; Percent done: 9.0%; Mean loss: 3.9449\n",
            "Iteration: 362; Percent done: 9.0%; Mean loss: 3.8606\n",
            "Iteration: 363; Percent done: 9.1%; Mean loss: 3.9430\n",
            "Iteration: 364; Percent done: 9.1%; Mean loss: 3.5744\n",
            "Iteration: 365; Percent done: 9.1%; Mean loss: 3.7570\n",
            "Iteration: 366; Percent done: 9.2%; Mean loss: 3.8292\n",
            "Iteration: 367; Percent done: 9.2%; Mean loss: 3.7397\n",
            "Iteration: 368; Percent done: 9.2%; Mean loss: 3.8437\n",
            "Iteration: 369; Percent done: 9.2%; Mean loss: 4.0034\n",
            "Iteration: 370; Percent done: 9.2%; Mean loss: 3.7073\n",
            "Iteration: 371; Percent done: 9.3%; Mean loss: 4.0700\n",
            "Iteration: 372; Percent done: 9.3%; Mean loss: 3.9090\n",
            "Iteration: 373; Percent done: 9.3%; Mean loss: 3.7309\n",
            "Iteration: 374; Percent done: 9.3%; Mean loss: 3.9797\n",
            "Iteration: 375; Percent done: 9.4%; Mean loss: 3.7087\n",
            "Iteration: 376; Percent done: 9.4%; Mean loss: 3.6713\n",
            "Iteration: 377; Percent done: 9.4%; Mean loss: 4.0662\n",
            "Iteration: 378; Percent done: 9.4%; Mean loss: 3.8276\n",
            "Iteration: 379; Percent done: 9.5%; Mean loss: 3.6860\n",
            "Iteration: 380; Percent done: 9.5%; Mean loss: 3.8466\n",
            "Iteration: 381; Percent done: 9.5%; Mean loss: 3.7947\n",
            "Iteration: 382; Percent done: 9.6%; Mean loss: 3.8708\n",
            "Iteration: 383; Percent done: 9.6%; Mean loss: 3.8599\n",
            "Iteration: 384; Percent done: 9.6%; Mean loss: 4.0698\n",
            "Iteration: 385; Percent done: 9.6%; Mean loss: 4.0039\n",
            "Iteration: 386; Percent done: 9.7%; Mean loss: 3.8029\n",
            "Iteration: 387; Percent done: 9.7%; Mean loss: 3.9845\n",
            "Iteration: 388; Percent done: 9.7%; Mean loss: 3.9445\n",
            "Iteration: 389; Percent done: 9.7%; Mean loss: 3.8337\n",
            "Iteration: 390; Percent done: 9.8%; Mean loss: 4.0387\n",
            "Iteration: 391; Percent done: 9.8%; Mean loss: 3.9393\n",
            "Iteration: 392; Percent done: 9.8%; Mean loss: 3.7335\n",
            "Iteration: 393; Percent done: 9.8%; Mean loss: 4.0778\n",
            "Iteration: 394; Percent done: 9.8%; Mean loss: 3.9283\n",
            "Iteration: 395; Percent done: 9.9%; Mean loss: 3.7828\n",
            "Iteration: 396; Percent done: 9.9%; Mean loss: 3.8682\n",
            "Iteration: 397; Percent done: 9.9%; Mean loss: 3.8482\n",
            "Iteration: 398; Percent done: 10.0%; Mean loss: 3.6323\n",
            "Iteration: 399; Percent done: 10.0%; Mean loss: 3.8851\n",
            "Iteration: 400; Percent done: 10.0%; Mean loss: 3.7781\n",
            "Iteration: 401; Percent done: 10.0%; Mean loss: 3.8587\n",
            "Iteration: 402; Percent done: 10.1%; Mean loss: 3.8997\n",
            "Iteration: 403; Percent done: 10.1%; Mean loss: 3.7099\n",
            "Iteration: 404; Percent done: 10.1%; Mean loss: 3.7287\n",
            "Iteration: 405; Percent done: 10.1%; Mean loss: 3.9469\n",
            "Iteration: 406; Percent done: 10.2%; Mean loss: 3.7129\n",
            "Iteration: 407; Percent done: 10.2%; Mean loss: 3.7618\n",
            "Iteration: 408; Percent done: 10.2%; Mean loss: 3.8835\n",
            "Iteration: 409; Percent done: 10.2%; Mean loss: 3.7531\n",
            "Iteration: 410; Percent done: 10.2%; Mean loss: 3.6797\n",
            "Iteration: 411; Percent done: 10.3%; Mean loss: 3.6494\n",
            "Iteration: 412; Percent done: 10.3%; Mean loss: 3.7564\n",
            "Iteration: 413; Percent done: 10.3%; Mean loss: 4.0012\n",
            "Iteration: 414; Percent done: 10.3%; Mean loss: 3.9215\n",
            "Iteration: 415; Percent done: 10.4%; Mean loss: 3.7386\n",
            "Iteration: 416; Percent done: 10.4%; Mean loss: 3.7793\n",
            "Iteration: 417; Percent done: 10.4%; Mean loss: 3.7377\n",
            "Iteration: 418; Percent done: 10.4%; Mean loss: 3.6803\n",
            "Iteration: 419; Percent done: 10.5%; Mean loss: 3.6836\n",
            "Iteration: 420; Percent done: 10.5%; Mean loss: 4.0781\n",
            "Iteration: 421; Percent done: 10.5%; Mean loss: 4.0102\n",
            "Iteration: 422; Percent done: 10.5%; Mean loss: 3.7327\n",
            "Iteration: 423; Percent done: 10.6%; Mean loss: 3.7899\n",
            "Iteration: 424; Percent done: 10.6%; Mean loss: 3.9851\n",
            "Iteration: 425; Percent done: 10.6%; Mean loss: 3.7811\n",
            "Iteration: 426; Percent done: 10.7%; Mean loss: 3.8216\n",
            "Iteration: 427; Percent done: 10.7%; Mean loss: 3.8834\n",
            "Iteration: 428; Percent done: 10.7%; Mean loss: 3.6250\n",
            "Iteration: 429; Percent done: 10.7%; Mean loss: 3.7015\n",
            "Iteration: 430; Percent done: 10.8%; Mean loss: 3.6992\n",
            "Iteration: 431; Percent done: 10.8%; Mean loss: 3.7181\n",
            "Iteration: 432; Percent done: 10.8%; Mean loss: 3.5080\n",
            "Iteration: 433; Percent done: 10.8%; Mean loss: 3.9502\n",
            "Iteration: 434; Percent done: 10.8%; Mean loss: 3.6396\n",
            "Iteration: 435; Percent done: 10.9%; Mean loss: 3.9888\n",
            "Iteration: 436; Percent done: 10.9%; Mean loss: 3.9260\n",
            "Iteration: 437; Percent done: 10.9%; Mean loss: 3.9979\n",
            "Iteration: 438; Percent done: 10.9%; Mean loss: 3.9177\n",
            "Iteration: 439; Percent done: 11.0%; Mean loss: 3.8215\n",
            "Iteration: 440; Percent done: 11.0%; Mean loss: 3.6970\n",
            "Iteration: 441; Percent done: 11.0%; Mean loss: 3.7712\n",
            "Iteration: 442; Percent done: 11.1%; Mean loss: 3.6214\n",
            "Iteration: 443; Percent done: 11.1%; Mean loss: 3.7098\n",
            "Iteration: 444; Percent done: 11.1%; Mean loss: 3.8082\n",
            "Iteration: 445; Percent done: 11.1%; Mean loss: 3.8271\n",
            "Iteration: 446; Percent done: 11.2%; Mean loss: 3.9558\n",
            "Iteration: 447; Percent done: 11.2%; Mean loss: 3.7438\n",
            "Iteration: 448; Percent done: 11.2%; Mean loss: 3.9517\n",
            "Iteration: 449; Percent done: 11.2%; Mean loss: 3.7561\n",
            "Iteration: 450; Percent done: 11.2%; Mean loss: 3.6540\n",
            "Iteration: 451; Percent done: 11.3%; Mean loss: 3.8050\n",
            "Iteration: 452; Percent done: 11.3%; Mean loss: 3.6833\n",
            "Iteration: 453; Percent done: 11.3%; Mean loss: 3.7962\n",
            "Iteration: 454; Percent done: 11.3%; Mean loss: 3.8574\n",
            "Iteration: 455; Percent done: 11.4%; Mean loss: 3.6743\n",
            "Iteration: 456; Percent done: 11.4%; Mean loss: 3.8072\n",
            "Iteration: 457; Percent done: 11.4%; Mean loss: 3.6354\n",
            "Iteration: 458; Percent done: 11.5%; Mean loss: 3.7571\n",
            "Iteration: 459; Percent done: 11.5%; Mean loss: 3.7641\n",
            "Iteration: 460; Percent done: 11.5%; Mean loss: 3.7496\n",
            "Iteration: 461; Percent done: 11.5%; Mean loss: 3.6669\n",
            "Iteration: 462; Percent done: 11.6%; Mean loss: 3.7248\n",
            "Iteration: 463; Percent done: 11.6%; Mean loss: 3.5977\n",
            "Iteration: 464; Percent done: 11.6%; Mean loss: 3.5847\n",
            "Iteration: 465; Percent done: 11.6%; Mean loss: 3.9725\n",
            "Iteration: 466; Percent done: 11.7%; Mean loss: 3.7237\n",
            "Iteration: 467; Percent done: 11.7%; Mean loss: 3.8134\n",
            "Iteration: 468; Percent done: 11.7%; Mean loss: 3.8139\n",
            "Iteration: 469; Percent done: 11.7%; Mean loss: 3.9815\n",
            "Iteration: 470; Percent done: 11.8%; Mean loss: 3.8624\n",
            "Iteration: 471; Percent done: 11.8%; Mean loss: 3.7238\n",
            "Iteration: 472; Percent done: 11.8%; Mean loss: 3.7957\n",
            "Iteration: 473; Percent done: 11.8%; Mean loss: 3.4377\n",
            "Iteration: 474; Percent done: 11.8%; Mean loss: 4.0980\n",
            "Iteration: 475; Percent done: 11.9%; Mean loss: 3.7581\n",
            "Iteration: 476; Percent done: 11.9%; Mean loss: 3.6721\n",
            "Iteration: 477; Percent done: 11.9%; Mean loss: 3.5194\n",
            "Iteration: 478; Percent done: 11.9%; Mean loss: 3.7831\n",
            "Iteration: 479; Percent done: 12.0%; Mean loss: 3.6170\n",
            "Iteration: 480; Percent done: 12.0%; Mean loss: 4.0236\n",
            "Iteration: 481; Percent done: 12.0%; Mean loss: 3.7342\n",
            "Iteration: 482; Percent done: 12.0%; Mean loss: 3.7730\n",
            "Iteration: 483; Percent done: 12.1%; Mean loss: 3.9432\n",
            "Iteration: 484; Percent done: 12.1%; Mean loss: 3.8178\n",
            "Iteration: 485; Percent done: 12.1%; Mean loss: 3.7560\n",
            "Iteration: 486; Percent done: 12.2%; Mean loss: 3.9743\n",
            "Iteration: 487; Percent done: 12.2%; Mean loss: 3.7760\n",
            "Iteration: 488; Percent done: 12.2%; Mean loss: 3.5147\n",
            "Iteration: 489; Percent done: 12.2%; Mean loss: 3.8160\n",
            "Iteration: 490; Percent done: 12.2%; Mean loss: 3.7035\n",
            "Iteration: 491; Percent done: 12.3%; Mean loss: 3.8846\n",
            "Iteration: 492; Percent done: 12.3%; Mean loss: 3.6537\n",
            "Iteration: 493; Percent done: 12.3%; Mean loss: 3.7305\n",
            "Iteration: 494; Percent done: 12.3%; Mean loss: 3.9698\n",
            "Iteration: 495; Percent done: 12.4%; Mean loss: 3.7346\n",
            "Iteration: 496; Percent done: 12.4%; Mean loss: 3.7308\n",
            "Iteration: 497; Percent done: 12.4%; Mean loss: 3.8765\n",
            "Iteration: 498; Percent done: 12.4%; Mean loss: 3.6415\n",
            "Iteration: 499; Percent done: 12.5%; Mean loss: 3.7295\n",
            "Iteration: 500; Percent done: 12.5%; Mean loss: 3.6293\n",
            "Iteration: 501; Percent done: 12.5%; Mean loss: 3.8431\n",
            "Iteration: 502; Percent done: 12.6%; Mean loss: 3.9613\n",
            "Iteration: 503; Percent done: 12.6%; Mean loss: 3.6267\n",
            "Iteration: 504; Percent done: 12.6%; Mean loss: 3.8570\n",
            "Iteration: 505; Percent done: 12.6%; Mean loss: 3.8931\n",
            "Iteration: 506; Percent done: 12.7%; Mean loss: 3.8155\n",
            "Iteration: 507; Percent done: 12.7%; Mean loss: 3.7504\n",
            "Iteration: 508; Percent done: 12.7%; Mean loss: 3.7606\n",
            "Iteration: 509; Percent done: 12.7%; Mean loss: 3.8540\n",
            "Iteration: 510; Percent done: 12.8%; Mean loss: 3.5549\n",
            "Iteration: 511; Percent done: 12.8%; Mean loss: 3.8962\n",
            "Iteration: 512; Percent done: 12.8%; Mean loss: 3.8018\n",
            "Iteration: 513; Percent done: 12.8%; Mean loss: 3.7835\n",
            "Iteration: 514; Percent done: 12.8%; Mean loss: 3.8883\n",
            "Iteration: 515; Percent done: 12.9%; Mean loss: 3.8533\n",
            "Iteration: 516; Percent done: 12.9%; Mean loss: 3.7867\n",
            "Iteration: 517; Percent done: 12.9%; Mean loss: 3.7546\n",
            "Iteration: 518; Percent done: 13.0%; Mean loss: 3.8020\n",
            "Iteration: 519; Percent done: 13.0%; Mean loss: 3.5117\n",
            "Iteration: 520; Percent done: 13.0%; Mean loss: 3.6188\n",
            "Iteration: 521; Percent done: 13.0%; Mean loss: 3.9259\n",
            "Iteration: 522; Percent done: 13.1%; Mean loss: 3.7843\n",
            "Iteration: 523; Percent done: 13.1%; Mean loss: 3.8821\n",
            "Iteration: 524; Percent done: 13.1%; Mean loss: 3.6513\n",
            "Iteration: 525; Percent done: 13.1%; Mean loss: 3.6227\n",
            "Iteration: 526; Percent done: 13.2%; Mean loss: 3.4818\n",
            "Iteration: 527; Percent done: 13.2%; Mean loss: 3.8497\n",
            "Iteration: 528; Percent done: 13.2%; Mean loss: 3.8228\n",
            "Iteration: 529; Percent done: 13.2%; Mean loss: 3.8690\n",
            "Iteration: 530; Percent done: 13.2%; Mean loss: 3.6136\n",
            "Iteration: 531; Percent done: 13.3%; Mean loss: 3.5918\n",
            "Iteration: 532; Percent done: 13.3%; Mean loss: 3.6776\n",
            "Iteration: 533; Percent done: 13.3%; Mean loss: 3.7276\n",
            "Iteration: 534; Percent done: 13.4%; Mean loss: 3.6182\n",
            "Iteration: 535; Percent done: 13.4%; Mean loss: 3.6644\n",
            "Iteration: 536; Percent done: 13.4%; Mean loss: 3.7139\n",
            "Iteration: 537; Percent done: 13.4%; Mean loss: 3.8102\n",
            "Iteration: 538; Percent done: 13.5%; Mean loss: 3.7901\n",
            "Iteration: 539; Percent done: 13.5%; Mean loss: 3.7136\n",
            "Iteration: 540; Percent done: 13.5%; Mean loss: 3.7503\n",
            "Iteration: 541; Percent done: 13.5%; Mean loss: 3.5813\n",
            "Iteration: 542; Percent done: 13.6%; Mean loss: 3.9199\n",
            "Iteration: 543; Percent done: 13.6%; Mean loss: 3.6583\n",
            "Iteration: 544; Percent done: 13.6%; Mean loss: 3.8073\n",
            "Iteration: 545; Percent done: 13.6%; Mean loss: 3.6731\n",
            "Iteration: 546; Percent done: 13.7%; Mean loss: 3.5294\n",
            "Iteration: 547; Percent done: 13.7%; Mean loss: 3.7026\n",
            "Iteration: 548; Percent done: 13.7%; Mean loss: 3.5289\n",
            "Iteration: 549; Percent done: 13.7%; Mean loss: 3.8409\n",
            "Iteration: 550; Percent done: 13.8%; Mean loss: 3.4972\n",
            "Iteration: 551; Percent done: 13.8%; Mean loss: 3.2915\n",
            "Iteration: 552; Percent done: 13.8%; Mean loss: 3.4963\n",
            "Iteration: 553; Percent done: 13.8%; Mean loss: 3.6637\n",
            "Iteration: 554; Percent done: 13.9%; Mean loss: 3.9267\n",
            "Iteration: 555; Percent done: 13.9%; Mean loss: 3.8007\n",
            "Iteration: 556; Percent done: 13.9%; Mean loss: 3.7467\n",
            "Iteration: 557; Percent done: 13.9%; Mean loss: 3.8767\n",
            "Iteration: 558; Percent done: 14.0%; Mean loss: 3.5837\n",
            "Iteration: 559; Percent done: 14.0%; Mean loss: 3.5006\n",
            "Iteration: 560; Percent done: 14.0%; Mean loss: 3.5583\n",
            "Iteration: 561; Percent done: 14.0%; Mean loss: 3.8819\n",
            "Iteration: 562; Percent done: 14.1%; Mean loss: 3.5773\n",
            "Iteration: 563; Percent done: 14.1%; Mean loss: 3.5422\n",
            "Iteration: 564; Percent done: 14.1%; Mean loss: 3.6146\n",
            "Iteration: 565; Percent done: 14.1%; Mean loss: 3.6517\n",
            "Iteration: 566; Percent done: 14.1%; Mean loss: 3.6153\n",
            "Iteration: 567; Percent done: 14.2%; Mean loss: 3.6259\n",
            "Iteration: 568; Percent done: 14.2%; Mean loss: 3.7594\n",
            "Iteration: 569; Percent done: 14.2%; Mean loss: 3.7277\n",
            "Iteration: 570; Percent done: 14.2%; Mean loss: 3.9409\n",
            "Iteration: 571; Percent done: 14.3%; Mean loss: 3.5840\n",
            "Iteration: 572; Percent done: 14.3%; Mean loss: 3.6071\n",
            "Iteration: 573; Percent done: 14.3%; Mean loss: 3.9642\n",
            "Iteration: 574; Percent done: 14.3%; Mean loss: 3.5761\n",
            "Iteration: 575; Percent done: 14.4%; Mean loss: 3.7031\n",
            "Iteration: 576; Percent done: 14.4%; Mean loss: 3.6788\n",
            "Iteration: 577; Percent done: 14.4%; Mean loss: 3.6309\n",
            "Iteration: 578; Percent done: 14.4%; Mean loss: 3.9318\n",
            "Iteration: 579; Percent done: 14.5%; Mean loss: 3.5658\n",
            "Iteration: 580; Percent done: 14.5%; Mean loss: 3.7340\n",
            "Iteration: 581; Percent done: 14.5%; Mean loss: 3.4867\n",
            "Iteration: 582; Percent done: 14.5%; Mean loss: 3.8793\n",
            "Iteration: 583; Percent done: 14.6%; Mean loss: 3.7792\n",
            "Iteration: 584; Percent done: 14.6%; Mean loss: 3.4851\n",
            "Iteration: 585; Percent done: 14.6%; Mean loss: 3.5670\n",
            "Iteration: 586; Percent done: 14.6%; Mean loss: 3.7557\n",
            "Iteration: 587; Percent done: 14.7%; Mean loss: 3.6788\n",
            "Iteration: 588; Percent done: 14.7%; Mean loss: 3.7675\n",
            "Iteration: 589; Percent done: 14.7%; Mean loss: 3.9516\n",
            "Iteration: 590; Percent done: 14.8%; Mean loss: 3.6719\n",
            "Iteration: 591; Percent done: 14.8%; Mean loss: 3.9553\n",
            "Iteration: 592; Percent done: 14.8%; Mean loss: 3.9903\n",
            "Iteration: 593; Percent done: 14.8%; Mean loss: 3.6973\n",
            "Iteration: 594; Percent done: 14.8%; Mean loss: 3.5028\n",
            "Iteration: 595; Percent done: 14.9%; Mean loss: 3.6091\n",
            "Iteration: 596; Percent done: 14.9%; Mean loss: 3.7329\n",
            "Iteration: 597; Percent done: 14.9%; Mean loss: 3.6118\n",
            "Iteration: 598; Percent done: 14.9%; Mean loss: 3.7426\n",
            "Iteration: 599; Percent done: 15.0%; Mean loss: 3.5749\n",
            "Iteration: 600; Percent done: 15.0%; Mean loss: 3.4763\n",
            "Iteration: 601; Percent done: 15.0%; Mean loss: 3.5101\n",
            "Iteration: 602; Percent done: 15.0%; Mean loss: 3.6089\n",
            "Iteration: 603; Percent done: 15.1%; Mean loss: 3.5901\n",
            "Iteration: 604; Percent done: 15.1%; Mean loss: 3.5477\n",
            "Iteration: 605; Percent done: 15.1%; Mean loss: 3.5602\n",
            "Iteration: 606; Percent done: 15.2%; Mean loss: 3.7493\n",
            "Iteration: 607; Percent done: 15.2%; Mean loss: 3.7318\n",
            "Iteration: 608; Percent done: 15.2%; Mean loss: 3.8664\n",
            "Iteration: 609; Percent done: 15.2%; Mean loss: 3.5601\n",
            "Iteration: 610; Percent done: 15.2%; Mean loss: 3.6233\n",
            "Iteration: 611; Percent done: 15.3%; Mean loss: 3.6198\n",
            "Iteration: 612; Percent done: 15.3%; Mean loss: 3.7005\n",
            "Iteration: 613; Percent done: 15.3%; Mean loss: 3.5070\n",
            "Iteration: 614; Percent done: 15.3%; Mean loss: 3.4297\n",
            "Iteration: 615; Percent done: 15.4%; Mean loss: 3.7094\n",
            "Iteration: 616; Percent done: 15.4%; Mean loss: 3.5886\n",
            "Iteration: 617; Percent done: 15.4%; Mean loss: 3.8245\n",
            "Iteration: 618; Percent done: 15.4%; Mean loss: 3.6272\n",
            "Iteration: 619; Percent done: 15.5%; Mean loss: 3.7651\n",
            "Iteration: 620; Percent done: 15.5%; Mean loss: 3.6352\n",
            "Iteration: 621; Percent done: 15.5%; Mean loss: 3.9188\n",
            "Iteration: 622; Percent done: 15.6%; Mean loss: 3.5607\n",
            "Iteration: 623; Percent done: 15.6%; Mean loss: 3.7844\n",
            "Iteration: 624; Percent done: 15.6%; Mean loss: 3.6008\n",
            "Iteration: 625; Percent done: 15.6%; Mean loss: 3.5259\n",
            "Iteration: 626; Percent done: 15.7%; Mean loss: 3.8417\n",
            "Iteration: 627; Percent done: 15.7%; Mean loss: 3.6879\n",
            "Iteration: 628; Percent done: 15.7%; Mean loss: 3.6432\n",
            "Iteration: 629; Percent done: 15.7%; Mean loss: 3.6471\n",
            "Iteration: 630; Percent done: 15.8%; Mean loss: 3.7552\n",
            "Iteration: 631; Percent done: 15.8%; Mean loss: 3.6535\n",
            "Iteration: 632; Percent done: 15.8%; Mean loss: 3.6882\n",
            "Iteration: 633; Percent done: 15.8%; Mean loss: 3.4397\n",
            "Iteration: 634; Percent done: 15.8%; Mean loss: 3.6312\n",
            "Iteration: 635; Percent done: 15.9%; Mean loss: 3.7875\n",
            "Iteration: 636; Percent done: 15.9%; Mean loss: 3.6404\n",
            "Iteration: 637; Percent done: 15.9%; Mean loss: 3.6500\n",
            "Iteration: 638; Percent done: 16.0%; Mean loss: 3.8112\n",
            "Iteration: 639; Percent done: 16.0%; Mean loss: 3.6957\n",
            "Iteration: 640; Percent done: 16.0%; Mean loss: 3.4311\n",
            "Iteration: 641; Percent done: 16.0%; Mean loss: 3.4881\n",
            "Iteration: 642; Percent done: 16.1%; Mean loss: 3.8399\n",
            "Iteration: 643; Percent done: 16.1%; Mean loss: 3.5935\n",
            "Iteration: 644; Percent done: 16.1%; Mean loss: 3.5601\n",
            "Iteration: 645; Percent done: 16.1%; Mean loss: 3.4487\n",
            "Iteration: 646; Percent done: 16.2%; Mean loss: 3.7237\n",
            "Iteration: 647; Percent done: 16.2%; Mean loss: 3.6528\n",
            "Iteration: 648; Percent done: 16.2%; Mean loss: 3.4811\n",
            "Iteration: 649; Percent done: 16.2%; Mean loss: 3.6093\n",
            "Iteration: 650; Percent done: 16.2%; Mean loss: 3.5676\n",
            "Iteration: 651; Percent done: 16.3%; Mean loss: 3.6126\n",
            "Iteration: 652; Percent done: 16.3%; Mean loss: 3.5467\n",
            "Iteration: 653; Percent done: 16.3%; Mean loss: 4.0096\n",
            "Iteration: 654; Percent done: 16.4%; Mean loss: 3.5793\n",
            "Iteration: 655; Percent done: 16.4%; Mean loss: 3.3278\n",
            "Iteration: 656; Percent done: 16.4%; Mean loss: 3.5576\n",
            "Iteration: 657; Percent done: 16.4%; Mean loss: 3.6053\n",
            "Iteration: 658; Percent done: 16.4%; Mean loss: 3.3479\n",
            "Iteration: 659; Percent done: 16.5%; Mean loss: 3.5047\n",
            "Iteration: 660; Percent done: 16.5%; Mean loss: 3.7151\n",
            "Iteration: 661; Percent done: 16.5%; Mean loss: 3.8157\n",
            "Iteration: 662; Percent done: 16.6%; Mean loss: 3.8834\n",
            "Iteration: 663; Percent done: 16.6%; Mean loss: 3.4064\n",
            "Iteration: 664; Percent done: 16.6%; Mean loss: 3.7865\n",
            "Iteration: 665; Percent done: 16.6%; Mean loss: 3.8042\n",
            "Iteration: 666; Percent done: 16.7%; Mean loss: 3.7401\n",
            "Iteration: 667; Percent done: 16.7%; Mean loss: 3.7998\n",
            "Iteration: 668; Percent done: 16.7%; Mean loss: 3.6976\n",
            "Iteration: 669; Percent done: 16.7%; Mean loss: 3.6450\n",
            "Iteration: 670; Percent done: 16.8%; Mean loss: 3.5377\n",
            "Iteration: 671; Percent done: 16.8%; Mean loss: 3.7675\n",
            "Iteration: 672; Percent done: 16.8%; Mean loss: 3.7012\n",
            "Iteration: 673; Percent done: 16.8%; Mean loss: 3.3365\n",
            "Iteration: 674; Percent done: 16.9%; Mean loss: 3.6087\n",
            "Iteration: 675; Percent done: 16.9%; Mean loss: 3.7808\n",
            "Iteration: 676; Percent done: 16.9%; Mean loss: 3.5446\n",
            "Iteration: 677; Percent done: 16.9%; Mean loss: 3.5879\n",
            "Iteration: 678; Percent done: 17.0%; Mean loss: 3.6158\n",
            "Iteration: 679; Percent done: 17.0%; Mean loss: 3.6507\n",
            "Iteration: 680; Percent done: 17.0%; Mean loss: 3.7968\n",
            "Iteration: 681; Percent done: 17.0%; Mean loss: 3.5819\n",
            "Iteration: 682; Percent done: 17.1%; Mean loss: 3.5245\n",
            "Iteration: 683; Percent done: 17.1%; Mean loss: 3.7280\n",
            "Iteration: 684; Percent done: 17.1%; Mean loss: 3.2858\n",
            "Iteration: 685; Percent done: 17.1%; Mean loss: 3.6080\n",
            "Iteration: 686; Percent done: 17.2%; Mean loss: 3.5381\n",
            "Iteration: 687; Percent done: 17.2%; Mean loss: 3.6292\n",
            "Iteration: 688; Percent done: 17.2%; Mean loss: 3.4834\n",
            "Iteration: 689; Percent done: 17.2%; Mean loss: 3.6660\n",
            "Iteration: 690; Percent done: 17.2%; Mean loss: 3.7424\n",
            "Iteration: 691; Percent done: 17.3%; Mean loss: 3.3721\n",
            "Iteration: 692; Percent done: 17.3%; Mean loss: 3.6887\n",
            "Iteration: 693; Percent done: 17.3%; Mean loss: 3.7518\n",
            "Iteration: 694; Percent done: 17.3%; Mean loss: 3.7680\n",
            "Iteration: 695; Percent done: 17.4%; Mean loss: 3.5055\n",
            "Iteration: 696; Percent done: 17.4%; Mean loss: 3.6137\n",
            "Iteration: 697; Percent done: 17.4%; Mean loss: 3.5343\n",
            "Iteration: 698; Percent done: 17.4%; Mean loss: 3.7541\n",
            "Iteration: 699; Percent done: 17.5%; Mean loss: 3.6102\n",
            "Iteration: 700; Percent done: 17.5%; Mean loss: 3.6597\n",
            "Iteration: 701; Percent done: 17.5%; Mean loss: 3.6918\n",
            "Iteration: 702; Percent done: 17.5%; Mean loss: 3.6820\n",
            "Iteration: 703; Percent done: 17.6%; Mean loss: 3.3883\n",
            "Iteration: 704; Percent done: 17.6%; Mean loss: 3.6428\n",
            "Iteration: 705; Percent done: 17.6%; Mean loss: 3.7104\n",
            "Iteration: 706; Percent done: 17.6%; Mean loss: 3.5914\n",
            "Iteration: 707; Percent done: 17.7%; Mean loss: 3.8979\n",
            "Iteration: 708; Percent done: 17.7%; Mean loss: 3.6828\n",
            "Iteration: 709; Percent done: 17.7%; Mean loss: 3.4946\n",
            "Iteration: 710; Percent done: 17.8%; Mean loss: 3.5353\n",
            "Iteration: 711; Percent done: 17.8%; Mean loss: 3.5722\n",
            "Iteration: 712; Percent done: 17.8%; Mean loss: 3.4928\n",
            "Iteration: 713; Percent done: 17.8%; Mean loss: 3.4145\n",
            "Iteration: 714; Percent done: 17.8%; Mean loss: 3.4691\n",
            "Iteration: 715; Percent done: 17.9%; Mean loss: 3.6078\n",
            "Iteration: 716; Percent done: 17.9%; Mean loss: 3.7901\n",
            "Iteration: 717; Percent done: 17.9%; Mean loss: 3.5923\n",
            "Iteration: 718; Percent done: 17.9%; Mean loss: 3.3932\n",
            "Iteration: 719; Percent done: 18.0%; Mean loss: 3.4752\n",
            "Iteration: 720; Percent done: 18.0%; Mean loss: 3.9107\n",
            "Iteration: 721; Percent done: 18.0%; Mean loss: 3.6892\n",
            "Iteration: 722; Percent done: 18.1%; Mean loss: 3.3476\n",
            "Iteration: 723; Percent done: 18.1%; Mean loss: 3.3585\n",
            "Iteration: 724; Percent done: 18.1%; Mean loss: 3.5543\n",
            "Iteration: 725; Percent done: 18.1%; Mean loss: 3.2671\n",
            "Iteration: 726; Percent done: 18.1%; Mean loss: 3.5489\n",
            "Iteration: 727; Percent done: 18.2%; Mean loss: 3.8235\n",
            "Iteration: 728; Percent done: 18.2%; Mean loss: 3.6587\n",
            "Iteration: 729; Percent done: 18.2%; Mean loss: 3.8440\n",
            "Iteration: 730; Percent done: 18.2%; Mean loss: 3.4369\n",
            "Iteration: 731; Percent done: 18.3%; Mean loss: 3.5314\n",
            "Iteration: 732; Percent done: 18.3%; Mean loss: 3.5002\n",
            "Iteration: 733; Percent done: 18.3%; Mean loss: 3.4596\n",
            "Iteration: 734; Percent done: 18.4%; Mean loss: 3.7699\n",
            "Iteration: 735; Percent done: 18.4%; Mean loss: 3.8320\n",
            "Iteration: 736; Percent done: 18.4%; Mean loss: 3.7211\n",
            "Iteration: 737; Percent done: 18.4%; Mean loss: 3.5397\n",
            "Iteration: 738; Percent done: 18.4%; Mean loss: 3.6766\n",
            "Iteration: 739; Percent done: 18.5%; Mean loss: 3.7858\n",
            "Iteration: 740; Percent done: 18.5%; Mean loss: 3.6495\n",
            "Iteration: 741; Percent done: 18.5%; Mean loss: 3.6612\n",
            "Iteration: 742; Percent done: 18.6%; Mean loss: 3.7828\n",
            "Iteration: 743; Percent done: 18.6%; Mean loss: 3.5484\n",
            "Iteration: 744; Percent done: 18.6%; Mean loss: 3.7344\n",
            "Iteration: 745; Percent done: 18.6%; Mean loss: 3.5594\n",
            "Iteration: 746; Percent done: 18.6%; Mean loss: 3.5660\n",
            "Iteration: 747; Percent done: 18.7%; Mean loss: 3.4254\n",
            "Iteration: 748; Percent done: 18.7%; Mean loss: 3.7386\n",
            "Iteration: 749; Percent done: 18.7%; Mean loss: 3.6800\n",
            "Iteration: 750; Percent done: 18.8%; Mean loss: 3.6961\n",
            "Iteration: 751; Percent done: 18.8%; Mean loss: 3.6619\n",
            "Iteration: 752; Percent done: 18.8%; Mean loss: 3.4334\n",
            "Iteration: 753; Percent done: 18.8%; Mean loss: 3.6562\n",
            "Iteration: 754; Percent done: 18.9%; Mean loss: 3.6894\n",
            "Iteration: 755; Percent done: 18.9%; Mean loss: 3.8711\n",
            "Iteration: 756; Percent done: 18.9%; Mean loss: 3.7037\n",
            "Iteration: 757; Percent done: 18.9%; Mean loss: 3.6034\n",
            "Iteration: 758; Percent done: 18.9%; Mean loss: 3.4252\n",
            "Iteration: 759; Percent done: 19.0%; Mean loss: 3.5495\n",
            "Iteration: 760; Percent done: 19.0%; Mean loss: 3.5068\n",
            "Iteration: 761; Percent done: 19.0%; Mean loss: 3.6645\n",
            "Iteration: 762; Percent done: 19.1%; Mean loss: 3.7823\n",
            "Iteration: 763; Percent done: 19.1%; Mean loss: 3.7649\n",
            "Iteration: 764; Percent done: 19.1%; Mean loss: 3.8284\n",
            "Iteration: 765; Percent done: 19.1%; Mean loss: 3.3895\n",
            "Iteration: 766; Percent done: 19.1%; Mean loss: 3.6734\n",
            "Iteration: 767; Percent done: 19.2%; Mean loss: 3.5645\n",
            "Iteration: 768; Percent done: 19.2%; Mean loss: 3.4886\n",
            "Iteration: 769; Percent done: 19.2%; Mean loss: 3.5150\n",
            "Iteration: 770; Percent done: 19.2%; Mean loss: 3.4556\n",
            "Iteration: 771; Percent done: 19.3%; Mean loss: 3.4896\n",
            "Iteration: 772; Percent done: 19.3%; Mean loss: 3.7998\n",
            "Iteration: 773; Percent done: 19.3%; Mean loss: 3.5564\n",
            "Iteration: 774; Percent done: 19.4%; Mean loss: 3.5251\n",
            "Iteration: 775; Percent done: 19.4%; Mean loss: 3.5608\n",
            "Iteration: 776; Percent done: 19.4%; Mean loss: 3.6775\n",
            "Iteration: 777; Percent done: 19.4%; Mean loss: 3.4771\n",
            "Iteration: 778; Percent done: 19.4%; Mean loss: 3.7709\n",
            "Iteration: 779; Percent done: 19.5%; Mean loss: 3.5646\n",
            "Iteration: 780; Percent done: 19.5%; Mean loss: 3.4509\n",
            "Iteration: 781; Percent done: 19.5%; Mean loss: 3.6589\n",
            "Iteration: 782; Percent done: 19.6%; Mean loss: 3.5903\n",
            "Iteration: 783; Percent done: 19.6%; Mean loss: 3.5894\n",
            "Iteration: 784; Percent done: 19.6%; Mean loss: 3.7228\n",
            "Iteration: 785; Percent done: 19.6%; Mean loss: 3.8803\n",
            "Iteration: 786; Percent done: 19.7%; Mean loss: 3.7838\n",
            "Iteration: 787; Percent done: 19.7%; Mean loss: 3.4009\n",
            "Iteration: 788; Percent done: 19.7%; Mean loss: 3.5502\n",
            "Iteration: 789; Percent done: 19.7%; Mean loss: 3.6689\n",
            "Iteration: 790; Percent done: 19.8%; Mean loss: 3.5747\n",
            "Iteration: 791; Percent done: 19.8%; Mean loss: 3.6386\n",
            "Iteration: 792; Percent done: 19.8%; Mean loss: 3.4419\n",
            "Iteration: 793; Percent done: 19.8%; Mean loss: 3.4491\n",
            "Iteration: 794; Percent done: 19.9%; Mean loss: 3.6405\n",
            "Iteration: 795; Percent done: 19.9%; Mean loss: 3.6342\n",
            "Iteration: 796; Percent done: 19.9%; Mean loss: 3.7123\n",
            "Iteration: 797; Percent done: 19.9%; Mean loss: 3.4633\n",
            "Iteration: 798; Percent done: 20.0%; Mean loss: 3.4256\n",
            "Iteration: 799; Percent done: 20.0%; Mean loss: 3.6037\n",
            "Iteration: 800; Percent done: 20.0%; Mean loss: 3.3408\n",
            "Iteration: 801; Percent done: 20.0%; Mean loss: 3.4357\n",
            "Iteration: 802; Percent done: 20.1%; Mean loss: 3.5531\n",
            "Iteration: 803; Percent done: 20.1%; Mean loss: 3.6699\n",
            "Iteration: 804; Percent done: 20.1%; Mean loss: 3.3759\n",
            "Iteration: 805; Percent done: 20.1%; Mean loss: 3.4140\n",
            "Iteration: 806; Percent done: 20.2%; Mean loss: 3.6568\n",
            "Iteration: 807; Percent done: 20.2%; Mean loss: 3.3001\n",
            "Iteration: 808; Percent done: 20.2%; Mean loss: 3.4890\n",
            "Iteration: 809; Percent done: 20.2%; Mean loss: 3.6176\n",
            "Iteration: 810; Percent done: 20.2%; Mean loss: 3.3128\n",
            "Iteration: 811; Percent done: 20.3%; Mean loss: 3.5194\n",
            "Iteration: 812; Percent done: 20.3%; Mean loss: 3.4597\n",
            "Iteration: 813; Percent done: 20.3%; Mean loss: 3.4911\n",
            "Iteration: 814; Percent done: 20.3%; Mean loss: 3.3814\n",
            "Iteration: 815; Percent done: 20.4%; Mean loss: 3.6419\n",
            "Iteration: 816; Percent done: 20.4%; Mean loss: 3.6950\n",
            "Iteration: 817; Percent done: 20.4%; Mean loss: 3.7054\n",
            "Iteration: 818; Percent done: 20.4%; Mean loss: 3.4603\n",
            "Iteration: 819; Percent done: 20.5%; Mean loss: 3.3751\n",
            "Iteration: 820; Percent done: 20.5%; Mean loss: 3.4766\n",
            "Iteration: 821; Percent done: 20.5%; Mean loss: 3.3915\n",
            "Iteration: 822; Percent done: 20.5%; Mean loss: 3.6029\n",
            "Iteration: 823; Percent done: 20.6%; Mean loss: 3.5582\n",
            "Iteration: 824; Percent done: 20.6%; Mean loss: 3.6067\n",
            "Iteration: 825; Percent done: 20.6%; Mean loss: 3.4025\n",
            "Iteration: 826; Percent done: 20.6%; Mean loss: 3.6083\n",
            "Iteration: 827; Percent done: 20.7%; Mean loss: 3.5734\n",
            "Iteration: 828; Percent done: 20.7%; Mean loss: 3.5916\n",
            "Iteration: 829; Percent done: 20.7%; Mean loss: 3.6097\n",
            "Iteration: 830; Percent done: 20.8%; Mean loss: 3.4046\n",
            "Iteration: 831; Percent done: 20.8%; Mean loss: 3.3370\n",
            "Iteration: 832; Percent done: 20.8%; Mean loss: 3.5086\n",
            "Iteration: 833; Percent done: 20.8%; Mean loss: 3.6043\n",
            "Iteration: 834; Percent done: 20.8%; Mean loss: 3.5641\n",
            "Iteration: 835; Percent done: 20.9%; Mean loss: 3.7776\n",
            "Iteration: 836; Percent done: 20.9%; Mean loss: 3.5181\n",
            "Iteration: 837; Percent done: 20.9%; Mean loss: 3.4591\n",
            "Iteration: 838; Percent done: 20.9%; Mean loss: 3.5863\n",
            "Iteration: 839; Percent done: 21.0%; Mean loss: 3.7290\n",
            "Iteration: 840; Percent done: 21.0%; Mean loss: 3.5684\n",
            "Iteration: 841; Percent done: 21.0%; Mean loss: 3.6082\n",
            "Iteration: 842; Percent done: 21.1%; Mean loss: 3.7046\n",
            "Iteration: 843; Percent done: 21.1%; Mean loss: 3.3127\n",
            "Iteration: 844; Percent done: 21.1%; Mean loss: 3.8653\n",
            "Iteration: 845; Percent done: 21.1%; Mean loss: 3.5678\n",
            "Iteration: 846; Percent done: 21.1%; Mean loss: 3.6883\n",
            "Iteration: 847; Percent done: 21.2%; Mean loss: 3.3602\n",
            "Iteration: 848; Percent done: 21.2%; Mean loss: 3.6132\n",
            "Iteration: 849; Percent done: 21.2%; Mean loss: 3.5106\n",
            "Iteration: 850; Percent done: 21.2%; Mean loss: 3.5802\n",
            "Iteration: 851; Percent done: 21.3%; Mean loss: 3.6738\n",
            "Iteration: 852; Percent done: 21.3%; Mean loss: 3.8143\n",
            "Iteration: 853; Percent done: 21.3%; Mean loss: 3.7745\n",
            "Iteration: 854; Percent done: 21.3%; Mean loss: 3.5042\n",
            "Iteration: 855; Percent done: 21.4%; Mean loss: 3.5609\n",
            "Iteration: 856; Percent done: 21.4%; Mean loss: 3.2005\n",
            "Iteration: 857; Percent done: 21.4%; Mean loss: 3.7250\n",
            "Iteration: 858; Percent done: 21.4%; Mean loss: 3.4315\n",
            "Iteration: 859; Percent done: 21.5%; Mean loss: 3.3430\n",
            "Iteration: 860; Percent done: 21.5%; Mean loss: 3.5321\n",
            "Iteration: 861; Percent done: 21.5%; Mean loss: 3.4617\n",
            "Iteration: 862; Percent done: 21.6%; Mean loss: 3.4332\n",
            "Iteration: 863; Percent done: 21.6%; Mean loss: 3.6385\n",
            "Iteration: 864; Percent done: 21.6%; Mean loss: 3.4050\n",
            "Iteration: 865; Percent done: 21.6%; Mean loss: 3.7583\n",
            "Iteration: 866; Percent done: 21.6%; Mean loss: 3.3869\n",
            "Iteration: 867; Percent done: 21.7%; Mean loss: 3.6409\n",
            "Iteration: 868; Percent done: 21.7%; Mean loss: 3.6353\n",
            "Iteration: 869; Percent done: 21.7%; Mean loss: 3.6724\n",
            "Iteration: 870; Percent done: 21.8%; Mean loss: 3.5963\n",
            "Iteration: 871; Percent done: 21.8%; Mean loss: 3.5495\n",
            "Iteration: 872; Percent done: 21.8%; Mean loss: 3.4916\n",
            "Iteration: 873; Percent done: 21.8%; Mean loss: 3.3579\n",
            "Iteration: 874; Percent done: 21.9%; Mean loss: 3.4129\n",
            "Iteration: 875; Percent done: 21.9%; Mean loss: 3.4142\n",
            "Iteration: 876; Percent done: 21.9%; Mean loss: 3.5327\n",
            "Iteration: 877; Percent done: 21.9%; Mean loss: 3.4302\n",
            "Iteration: 878; Percent done: 21.9%; Mean loss: 3.6023\n",
            "Iteration: 879; Percent done: 22.0%; Mean loss: 3.6040\n",
            "Iteration: 880; Percent done: 22.0%; Mean loss: 3.3519\n",
            "Iteration: 881; Percent done: 22.0%; Mean loss: 3.4494\n",
            "Iteration: 882; Percent done: 22.1%; Mean loss: 3.6598\n",
            "Iteration: 883; Percent done: 22.1%; Mean loss: 3.2669\n",
            "Iteration: 884; Percent done: 22.1%; Mean loss: 3.3325\n",
            "Iteration: 885; Percent done: 22.1%; Mean loss: 3.3816\n",
            "Iteration: 886; Percent done: 22.1%; Mean loss: 3.2950\n",
            "Iteration: 887; Percent done: 22.2%; Mean loss: 3.6867\n",
            "Iteration: 888; Percent done: 22.2%; Mean loss: 3.6587\n",
            "Iteration: 889; Percent done: 22.2%; Mean loss: 3.2440\n",
            "Iteration: 890; Percent done: 22.2%; Mean loss: 3.1328\n",
            "Iteration: 891; Percent done: 22.3%; Mean loss: 3.6510\n",
            "Iteration: 892; Percent done: 22.3%; Mean loss: 3.5365\n",
            "Iteration: 893; Percent done: 22.3%; Mean loss: 3.8230\n",
            "Iteration: 894; Percent done: 22.4%; Mean loss: 3.2992\n",
            "Iteration: 895; Percent done: 22.4%; Mean loss: 3.5350\n",
            "Iteration: 896; Percent done: 22.4%; Mean loss: 3.5511\n",
            "Iteration: 897; Percent done: 22.4%; Mean loss: 3.3291\n",
            "Iteration: 898; Percent done: 22.4%; Mean loss: 3.7750\n",
            "Iteration: 899; Percent done: 22.5%; Mean loss: 3.3984\n",
            "Iteration: 900; Percent done: 22.5%; Mean loss: 3.3984\n",
            "Iteration: 901; Percent done: 22.5%; Mean loss: 3.3687\n",
            "Iteration: 902; Percent done: 22.6%; Mean loss: 3.8074\n",
            "Iteration: 903; Percent done: 22.6%; Mean loss: 3.4191\n",
            "Iteration: 904; Percent done: 22.6%; Mean loss: 3.5189\n",
            "Iteration: 905; Percent done: 22.6%; Mean loss: 3.6363\n",
            "Iteration: 906; Percent done: 22.7%; Mean loss: 3.3280\n",
            "Iteration: 907; Percent done: 22.7%; Mean loss: 3.4727\n",
            "Iteration: 908; Percent done: 22.7%; Mean loss: 3.4820\n",
            "Iteration: 909; Percent done: 22.7%; Mean loss: 3.5157\n",
            "Iteration: 910; Percent done: 22.8%; Mean loss: 3.6797\n",
            "Iteration: 911; Percent done: 22.8%; Mean loss: 3.7377\n",
            "Iteration: 912; Percent done: 22.8%; Mean loss: 3.3037\n",
            "Iteration: 913; Percent done: 22.8%; Mean loss: 3.4835\n",
            "Iteration: 914; Percent done: 22.9%; Mean loss: 3.6345\n",
            "Iteration: 915; Percent done: 22.9%; Mean loss: 3.4716\n",
            "Iteration: 916; Percent done: 22.9%; Mean loss: 3.5842\n",
            "Iteration: 917; Percent done: 22.9%; Mean loss: 3.5713\n",
            "Iteration: 918; Percent done: 22.9%; Mean loss: 3.4896\n",
            "Iteration: 919; Percent done: 23.0%; Mean loss: 3.4018\n",
            "Iteration: 920; Percent done: 23.0%; Mean loss: 3.3563\n",
            "Iteration: 921; Percent done: 23.0%; Mean loss: 3.3433\n",
            "Iteration: 922; Percent done: 23.1%; Mean loss: 3.6596\n",
            "Iteration: 923; Percent done: 23.1%; Mean loss: 3.5087\n",
            "Iteration: 924; Percent done: 23.1%; Mean loss: 3.4672\n",
            "Iteration: 925; Percent done: 23.1%; Mean loss: 3.2514\n",
            "Iteration: 926; Percent done: 23.2%; Mean loss: 3.6741\n",
            "Iteration: 927; Percent done: 23.2%; Mean loss: 3.6384\n",
            "Iteration: 928; Percent done: 23.2%; Mean loss: 3.4642\n",
            "Iteration: 929; Percent done: 23.2%; Mean loss: 3.6006\n",
            "Iteration: 930; Percent done: 23.2%; Mean loss: 3.5301\n",
            "Iteration: 931; Percent done: 23.3%; Mean loss: 3.5716\n",
            "Iteration: 932; Percent done: 23.3%; Mean loss: 3.7330\n",
            "Iteration: 933; Percent done: 23.3%; Mean loss: 3.7740\n",
            "Iteration: 934; Percent done: 23.4%; Mean loss: 3.6112\n",
            "Iteration: 935; Percent done: 23.4%; Mean loss: 3.5147\n",
            "Iteration: 936; Percent done: 23.4%; Mean loss: 3.5913\n",
            "Iteration: 937; Percent done: 23.4%; Mean loss: 3.7152\n",
            "Iteration: 938; Percent done: 23.4%; Mean loss: 3.4140\n",
            "Iteration: 939; Percent done: 23.5%; Mean loss: 3.3093\n",
            "Iteration: 940; Percent done: 23.5%; Mean loss: 3.4740\n",
            "Iteration: 941; Percent done: 23.5%; Mean loss: 3.4132\n",
            "Iteration: 942; Percent done: 23.5%; Mean loss: 3.4877\n",
            "Iteration: 943; Percent done: 23.6%; Mean loss: 3.4667\n",
            "Iteration: 944; Percent done: 23.6%; Mean loss: 3.5164\n",
            "Iteration: 945; Percent done: 23.6%; Mean loss: 3.5137\n",
            "Iteration: 946; Percent done: 23.6%; Mean loss: 3.3907\n",
            "Iteration: 947; Percent done: 23.7%; Mean loss: 3.3748\n",
            "Iteration: 948; Percent done: 23.7%; Mean loss: 3.6114\n",
            "Iteration: 949; Percent done: 23.7%; Mean loss: 3.3188\n",
            "Iteration: 950; Percent done: 23.8%; Mean loss: 3.4727\n",
            "Iteration: 951; Percent done: 23.8%; Mean loss: 3.4105\n",
            "Iteration: 952; Percent done: 23.8%; Mean loss: 3.1097\n",
            "Iteration: 953; Percent done: 23.8%; Mean loss: 3.5983\n",
            "Iteration: 954; Percent done: 23.8%; Mean loss: 3.4598\n",
            "Iteration: 955; Percent done: 23.9%; Mean loss: 3.5695\n",
            "Iteration: 956; Percent done: 23.9%; Mean loss: 3.4708\n",
            "Iteration: 957; Percent done: 23.9%; Mean loss: 3.4306\n",
            "Iteration: 958; Percent done: 23.9%; Mean loss: 3.3608\n",
            "Iteration: 959; Percent done: 24.0%; Mean loss: 3.4997\n",
            "Iteration: 960; Percent done: 24.0%; Mean loss: 3.6790\n",
            "Iteration: 961; Percent done: 24.0%; Mean loss: 3.5371\n",
            "Iteration: 962; Percent done: 24.1%; Mean loss: 3.4225\n",
            "Iteration: 963; Percent done: 24.1%; Mean loss: 3.6936\n",
            "Iteration: 964; Percent done: 24.1%; Mean loss: 3.6789\n",
            "Iteration: 965; Percent done: 24.1%; Mean loss: 3.4023\n",
            "Iteration: 966; Percent done: 24.1%; Mean loss: 3.3488\n",
            "Iteration: 967; Percent done: 24.2%; Mean loss: 3.2919\n",
            "Iteration: 968; Percent done: 24.2%; Mean loss: 3.3787\n",
            "Iteration: 969; Percent done: 24.2%; Mean loss: 3.1833\n",
            "Iteration: 970; Percent done: 24.2%; Mean loss: 3.3052\n",
            "Iteration: 971; Percent done: 24.3%; Mean loss: 3.5525\n",
            "Iteration: 972; Percent done: 24.3%; Mean loss: 3.3754\n",
            "Iteration: 973; Percent done: 24.3%; Mean loss: 3.5704\n",
            "Iteration: 974; Percent done: 24.3%; Mean loss: 3.4190\n",
            "Iteration: 975; Percent done: 24.4%; Mean loss: 3.5374\n",
            "Iteration: 976; Percent done: 24.4%; Mean loss: 3.6405\n",
            "Iteration: 977; Percent done: 24.4%; Mean loss: 3.5043\n",
            "Iteration: 978; Percent done: 24.4%; Mean loss: 3.6035\n",
            "Iteration: 979; Percent done: 24.5%; Mean loss: 3.3757\n",
            "Iteration: 980; Percent done: 24.5%; Mean loss: 3.4901\n",
            "Iteration: 981; Percent done: 24.5%; Mean loss: 3.5895\n",
            "Iteration: 982; Percent done: 24.6%; Mean loss: 3.3537\n",
            "Iteration: 983; Percent done: 24.6%; Mean loss: 3.6052\n",
            "Iteration: 984; Percent done: 24.6%; Mean loss: 3.5148\n",
            "Iteration: 985; Percent done: 24.6%; Mean loss: 3.4400\n",
            "Iteration: 986; Percent done: 24.6%; Mean loss: 3.2932\n",
            "Iteration: 987; Percent done: 24.7%; Mean loss: 3.5019\n",
            "Iteration: 988; Percent done: 24.7%; Mean loss: 3.7293\n",
            "Iteration: 989; Percent done: 24.7%; Mean loss: 3.4280\n",
            "Iteration: 990; Percent done: 24.8%; Mean loss: 3.5713\n",
            "Iteration: 991; Percent done: 24.8%; Mean loss: 3.4064\n",
            "Iteration: 992; Percent done: 24.8%; Mean loss: 3.5760\n",
            "Iteration: 993; Percent done: 24.8%; Mean loss: 3.3447\n",
            "Iteration: 994; Percent done: 24.9%; Mean loss: 3.8182\n",
            "Iteration: 995; Percent done: 24.9%; Mean loss: 3.5041\n",
            "Iteration: 996; Percent done: 24.9%; Mean loss: 3.4382\n",
            "Iteration: 997; Percent done: 24.9%; Mean loss: 3.5097\n",
            "Iteration: 998; Percent done: 24.9%; Mean loss: 3.8883\n",
            "Iteration: 999; Percent done: 25.0%; Mean loss: 3.4462\n",
            "Iteration: 1000; Percent done: 25.0%; Mean loss: 3.4154\n",
            "Iteration: 1001; Percent done: 25.0%; Mean loss: 3.3823\n",
            "Iteration: 1002; Percent done: 25.1%; Mean loss: 3.4759\n",
            "Iteration: 1003; Percent done: 25.1%; Mean loss: 3.3974\n",
            "Iteration: 1004; Percent done: 25.1%; Mean loss: 3.7838\n",
            "Iteration: 1005; Percent done: 25.1%; Mean loss: 3.4135\n",
            "Iteration: 1006; Percent done: 25.1%; Mean loss: 3.4053\n",
            "Iteration: 1007; Percent done: 25.2%; Mean loss: 3.3407\n",
            "Iteration: 1008; Percent done: 25.2%; Mean loss: 3.2905\n",
            "Iteration: 1009; Percent done: 25.2%; Mean loss: 3.4785\n",
            "Iteration: 1010; Percent done: 25.2%; Mean loss: 3.2949\n",
            "Iteration: 1011; Percent done: 25.3%; Mean loss: 3.2883\n",
            "Iteration: 1012; Percent done: 25.3%; Mean loss: 3.4274\n",
            "Iteration: 1013; Percent done: 25.3%; Mean loss: 3.1906\n",
            "Iteration: 1014; Percent done: 25.4%; Mean loss: 3.4123\n",
            "Iteration: 1015; Percent done: 25.4%; Mean loss: 3.7761\n",
            "Iteration: 1016; Percent done: 25.4%; Mean loss: 3.6014\n",
            "Iteration: 1017; Percent done: 25.4%; Mean loss: 3.5871\n",
            "Iteration: 1018; Percent done: 25.4%; Mean loss: 3.6932\n",
            "Iteration: 1019; Percent done: 25.5%; Mean loss: 3.5692\n",
            "Iteration: 1020; Percent done: 25.5%; Mean loss: 3.6876\n",
            "Iteration: 1021; Percent done: 25.5%; Mean loss: 3.4598\n",
            "Iteration: 1022; Percent done: 25.6%; Mean loss: 3.7215\n",
            "Iteration: 1023; Percent done: 25.6%; Mean loss: 3.5202\n",
            "Iteration: 1024; Percent done: 25.6%; Mean loss: 3.4908\n",
            "Iteration: 1025; Percent done: 25.6%; Mean loss: 3.5054\n",
            "Iteration: 1026; Percent done: 25.7%; Mean loss: 3.4695\n",
            "Iteration: 1027; Percent done: 25.7%; Mean loss: 3.3175\n",
            "Iteration: 1028; Percent done: 25.7%; Mean loss: 3.2617\n",
            "Iteration: 1029; Percent done: 25.7%; Mean loss: 3.5952\n",
            "Iteration: 1030; Percent done: 25.8%; Mean loss: 3.3477\n",
            "Iteration: 1031; Percent done: 25.8%; Mean loss: 3.3784\n",
            "Iteration: 1032; Percent done: 25.8%; Mean loss: 3.2557\n",
            "Iteration: 1033; Percent done: 25.8%; Mean loss: 3.6567\n",
            "Iteration: 1034; Percent done: 25.9%; Mean loss: 3.4941\n",
            "Iteration: 1035; Percent done: 25.9%; Mean loss: 3.7168\n",
            "Iteration: 1036; Percent done: 25.9%; Mean loss: 3.2720\n",
            "Iteration: 1037; Percent done: 25.9%; Mean loss: 3.4912\n",
            "Iteration: 1038; Percent done: 25.9%; Mean loss: 3.8585\n",
            "Iteration: 1039; Percent done: 26.0%; Mean loss: 3.3810\n",
            "Iteration: 1040; Percent done: 26.0%; Mean loss: 3.7415\n",
            "Iteration: 1041; Percent done: 26.0%; Mean loss: 3.6632\n",
            "Iteration: 1042; Percent done: 26.1%; Mean loss: 3.5332\n",
            "Iteration: 1043; Percent done: 26.1%; Mean loss: 3.6257\n",
            "Iteration: 1044; Percent done: 26.1%; Mean loss: 3.3306\n",
            "Iteration: 1045; Percent done: 26.1%; Mean loss: 3.5014\n",
            "Iteration: 1046; Percent done: 26.2%; Mean loss: 3.7873\n",
            "Iteration: 1047; Percent done: 26.2%; Mean loss: 3.4120\n",
            "Iteration: 1048; Percent done: 26.2%; Mean loss: 3.3265\n",
            "Iteration: 1049; Percent done: 26.2%; Mean loss: 3.2065\n",
            "Iteration: 1050; Percent done: 26.2%; Mean loss: 3.3989\n",
            "Iteration: 1051; Percent done: 26.3%; Mean loss: 3.5560\n",
            "Iteration: 1052; Percent done: 26.3%; Mean loss: 3.5487\n",
            "Iteration: 1053; Percent done: 26.3%; Mean loss: 3.5837\n",
            "Iteration: 1054; Percent done: 26.4%; Mean loss: 3.6984\n",
            "Iteration: 1055; Percent done: 26.4%; Mean loss: 3.5595\n",
            "Iteration: 1056; Percent done: 26.4%; Mean loss: 3.2847\n",
            "Iteration: 1057; Percent done: 26.4%; Mean loss: 3.4947\n",
            "Iteration: 1058; Percent done: 26.5%; Mean loss: 3.5365\n",
            "Iteration: 1059; Percent done: 26.5%; Mean loss: 3.1847\n",
            "Iteration: 1060; Percent done: 26.5%; Mean loss: 3.7364\n",
            "Iteration: 1061; Percent done: 26.5%; Mean loss: 3.7547\n",
            "Iteration: 1062; Percent done: 26.6%; Mean loss: 3.4696\n",
            "Iteration: 1063; Percent done: 26.6%; Mean loss: 3.2982\n",
            "Iteration: 1064; Percent done: 26.6%; Mean loss: 3.3150\n",
            "Iteration: 1065; Percent done: 26.6%; Mean loss: 3.6806\n",
            "Iteration: 1066; Percent done: 26.7%; Mean loss: 3.2791\n",
            "Iteration: 1067; Percent done: 26.7%; Mean loss: 3.4804\n",
            "Iteration: 1068; Percent done: 26.7%; Mean loss: 3.3835\n",
            "Iteration: 1069; Percent done: 26.7%; Mean loss: 3.2564\n",
            "Iteration: 1070; Percent done: 26.8%; Mean loss: 3.6906\n",
            "Iteration: 1071; Percent done: 26.8%; Mean loss: 3.7429\n",
            "Iteration: 1072; Percent done: 26.8%; Mean loss: 3.4836\n",
            "Iteration: 1073; Percent done: 26.8%; Mean loss: 3.5435\n",
            "Iteration: 1074; Percent done: 26.9%; Mean loss: 3.4891\n",
            "Iteration: 1075; Percent done: 26.9%; Mean loss: 3.3341\n",
            "Iteration: 1076; Percent done: 26.9%; Mean loss: 3.6452\n",
            "Iteration: 1077; Percent done: 26.9%; Mean loss: 3.6109\n",
            "Iteration: 1078; Percent done: 27.0%; Mean loss: 3.4136\n",
            "Iteration: 1079; Percent done: 27.0%; Mean loss: 3.4576\n",
            "Iteration: 1080; Percent done: 27.0%; Mean loss: 3.4201\n",
            "Iteration: 1081; Percent done: 27.0%; Mean loss: 3.5207\n",
            "Iteration: 1082; Percent done: 27.1%; Mean loss: 3.3081\n",
            "Iteration: 1083; Percent done: 27.1%; Mean loss: 3.3230\n",
            "Iteration: 1084; Percent done: 27.1%; Mean loss: 3.4774\n",
            "Iteration: 1085; Percent done: 27.1%; Mean loss: 3.2592\n",
            "Iteration: 1086; Percent done: 27.2%; Mean loss: 3.3043\n",
            "Iteration: 1087; Percent done: 27.2%; Mean loss: 3.4431\n",
            "Iteration: 1088; Percent done: 27.2%; Mean loss: 3.6990\n",
            "Iteration: 1089; Percent done: 27.2%; Mean loss: 3.4064\n",
            "Iteration: 1090; Percent done: 27.3%; Mean loss: 3.2621\n",
            "Iteration: 1091; Percent done: 27.3%; Mean loss: 3.6359\n",
            "Iteration: 1092; Percent done: 27.3%; Mean loss: 3.0398\n",
            "Iteration: 1093; Percent done: 27.3%; Mean loss: 3.4123\n",
            "Iteration: 1094; Percent done: 27.4%; Mean loss: 3.5007\n",
            "Iteration: 1095; Percent done: 27.4%; Mean loss: 3.4382\n",
            "Iteration: 1096; Percent done: 27.4%; Mean loss: 3.5297\n",
            "Iteration: 1097; Percent done: 27.4%; Mean loss: 3.4080\n",
            "Iteration: 1098; Percent done: 27.5%; Mean loss: 3.2926\n",
            "Iteration: 1099; Percent done: 27.5%; Mean loss: 3.4487\n",
            "Iteration: 1100; Percent done: 27.5%; Mean loss: 3.5508\n",
            "Iteration: 1101; Percent done: 27.5%; Mean loss: 3.4833\n",
            "Iteration: 1102; Percent done: 27.6%; Mean loss: 3.1367\n",
            "Iteration: 1103; Percent done: 27.6%; Mean loss: 3.5652\n",
            "Iteration: 1104; Percent done: 27.6%; Mean loss: 3.4878\n",
            "Iteration: 1105; Percent done: 27.6%; Mean loss: 3.5294\n",
            "Iteration: 1106; Percent done: 27.7%; Mean loss: 3.3839\n",
            "Iteration: 1107; Percent done: 27.7%; Mean loss: 3.3414\n",
            "Iteration: 1108; Percent done: 27.7%; Mean loss: 3.5453\n",
            "Iteration: 1109; Percent done: 27.7%; Mean loss: 3.6894\n",
            "Iteration: 1110; Percent done: 27.8%; Mean loss: 3.1835\n",
            "Iteration: 1111; Percent done: 27.8%; Mean loss: 3.5701\n",
            "Iteration: 1112; Percent done: 27.8%; Mean loss: 3.4657\n",
            "Iteration: 1113; Percent done: 27.8%; Mean loss: 3.3981\n",
            "Iteration: 1114; Percent done: 27.9%; Mean loss: 3.4893\n",
            "Iteration: 1115; Percent done: 27.9%; Mean loss: 3.4266\n",
            "Iteration: 1116; Percent done: 27.9%; Mean loss: 3.5861\n",
            "Iteration: 1117; Percent done: 27.9%; Mean loss: 3.5969\n",
            "Iteration: 1118; Percent done: 28.0%; Mean loss: 3.4470\n",
            "Iteration: 1119; Percent done: 28.0%; Mean loss: 3.2427\n",
            "Iteration: 1120; Percent done: 28.0%; Mean loss: 3.4813\n",
            "Iteration: 1121; Percent done: 28.0%; Mean loss: 3.4895\n",
            "Iteration: 1122; Percent done: 28.1%; Mean loss: 3.5037\n",
            "Iteration: 1123; Percent done: 28.1%; Mean loss: 3.6787\n",
            "Iteration: 1124; Percent done: 28.1%; Mean loss: 3.4440\n",
            "Iteration: 1125; Percent done: 28.1%; Mean loss: 3.4870\n",
            "Iteration: 1126; Percent done: 28.1%; Mean loss: 3.4132\n",
            "Iteration: 1127; Percent done: 28.2%; Mean loss: 3.1256\n",
            "Iteration: 1128; Percent done: 28.2%; Mean loss: 3.6779\n",
            "Iteration: 1129; Percent done: 28.2%; Mean loss: 3.4207\n",
            "Iteration: 1130; Percent done: 28.2%; Mean loss: 3.4027\n",
            "Iteration: 1131; Percent done: 28.3%; Mean loss: 3.4636\n",
            "Iteration: 1132; Percent done: 28.3%; Mean loss: 3.4665\n",
            "Iteration: 1133; Percent done: 28.3%; Mean loss: 3.1694\n",
            "Iteration: 1134; Percent done: 28.3%; Mean loss: 3.4730\n",
            "Iteration: 1135; Percent done: 28.4%; Mean loss: 3.1997\n",
            "Iteration: 1136; Percent done: 28.4%; Mean loss: 3.9043\n",
            "Iteration: 1137; Percent done: 28.4%; Mean loss: 3.1692\n",
            "Iteration: 1138; Percent done: 28.4%; Mean loss: 3.3075\n",
            "Iteration: 1139; Percent done: 28.5%; Mean loss: 3.3758\n",
            "Iteration: 1140; Percent done: 28.5%; Mean loss: 3.4547\n",
            "Iteration: 1141; Percent done: 28.5%; Mean loss: 3.2170\n",
            "Iteration: 1142; Percent done: 28.5%; Mean loss: 3.4768\n",
            "Iteration: 1143; Percent done: 28.6%; Mean loss: 3.2523\n",
            "Iteration: 1144; Percent done: 28.6%; Mean loss: 3.4297\n",
            "Iteration: 1145; Percent done: 28.6%; Mean loss: 3.3865\n",
            "Iteration: 1146; Percent done: 28.6%; Mean loss: 3.7349\n",
            "Iteration: 1147; Percent done: 28.7%; Mean loss: 3.6306\n",
            "Iteration: 1148; Percent done: 28.7%; Mean loss: 3.2566\n",
            "Iteration: 1149; Percent done: 28.7%; Mean loss: 3.2572\n",
            "Iteration: 1150; Percent done: 28.7%; Mean loss: 3.3638\n",
            "Iteration: 1151; Percent done: 28.8%; Mean loss: 3.2161\n",
            "Iteration: 1152; Percent done: 28.8%; Mean loss: 3.6101\n",
            "Iteration: 1153; Percent done: 28.8%; Mean loss: 3.2797\n",
            "Iteration: 1154; Percent done: 28.8%; Mean loss: 3.4784\n",
            "Iteration: 1155; Percent done: 28.9%; Mean loss: 3.6398\n",
            "Iteration: 1156; Percent done: 28.9%; Mean loss: 3.3191\n",
            "Iteration: 1157; Percent done: 28.9%; Mean loss: 3.4655\n",
            "Iteration: 1158; Percent done: 28.9%; Mean loss: 3.3686\n",
            "Iteration: 1159; Percent done: 29.0%; Mean loss: 3.6024\n",
            "Iteration: 1160; Percent done: 29.0%; Mean loss: 3.3362\n",
            "Iteration: 1161; Percent done: 29.0%; Mean loss: 3.4864\n",
            "Iteration: 1162; Percent done: 29.0%; Mean loss: 3.4569\n",
            "Iteration: 1163; Percent done: 29.1%; Mean loss: 3.4759\n",
            "Iteration: 1164; Percent done: 29.1%; Mean loss: 3.2598\n",
            "Iteration: 1165; Percent done: 29.1%; Mean loss: 3.5766\n",
            "Iteration: 1166; Percent done: 29.1%; Mean loss: 3.3254\n",
            "Iteration: 1167; Percent done: 29.2%; Mean loss: 3.3898\n",
            "Iteration: 1168; Percent done: 29.2%; Mean loss: 3.3355\n",
            "Iteration: 1169; Percent done: 29.2%; Mean loss: 3.1084\n",
            "Iteration: 1170; Percent done: 29.2%; Mean loss: 3.1203\n",
            "Iteration: 1171; Percent done: 29.3%; Mean loss: 3.4374\n",
            "Iteration: 1172; Percent done: 29.3%; Mean loss: 3.5057\n",
            "Iteration: 1173; Percent done: 29.3%; Mean loss: 3.1143\n",
            "Iteration: 1174; Percent done: 29.3%; Mean loss: 3.3684\n",
            "Iteration: 1175; Percent done: 29.4%; Mean loss: 3.3133\n",
            "Iteration: 1176; Percent done: 29.4%; Mean loss: 3.3486\n",
            "Iteration: 1177; Percent done: 29.4%; Mean loss: 3.7777\n",
            "Iteration: 1178; Percent done: 29.4%; Mean loss: 3.2087\n",
            "Iteration: 1179; Percent done: 29.5%; Mean loss: 3.4361\n",
            "Iteration: 1180; Percent done: 29.5%; Mean loss: 3.2893\n",
            "Iteration: 1181; Percent done: 29.5%; Mean loss: 3.2509\n",
            "Iteration: 1182; Percent done: 29.5%; Mean loss: 3.3090\n",
            "Iteration: 1183; Percent done: 29.6%; Mean loss: 3.1799\n",
            "Iteration: 1184; Percent done: 29.6%; Mean loss: 3.4744\n",
            "Iteration: 1185; Percent done: 29.6%; Mean loss: 3.4099\n",
            "Iteration: 1186; Percent done: 29.6%; Mean loss: 3.4236\n",
            "Iteration: 1187; Percent done: 29.7%; Mean loss: 3.3737\n",
            "Iteration: 1188; Percent done: 29.7%; Mean loss: 3.4335\n",
            "Iteration: 1189; Percent done: 29.7%; Mean loss: 3.3786\n",
            "Iteration: 1190; Percent done: 29.8%; Mean loss: 3.7489\n",
            "Iteration: 1191; Percent done: 29.8%; Mean loss: 3.2881\n",
            "Iteration: 1192; Percent done: 29.8%; Mean loss: 3.3738\n",
            "Iteration: 1193; Percent done: 29.8%; Mean loss: 3.4086\n",
            "Iteration: 1194; Percent done: 29.8%; Mean loss: 3.3066\n",
            "Iteration: 1195; Percent done: 29.9%; Mean loss: 3.4052\n",
            "Iteration: 1196; Percent done: 29.9%; Mean loss: 3.7156\n",
            "Iteration: 1197; Percent done: 29.9%; Mean loss: 3.6728\n",
            "Iteration: 1198; Percent done: 29.9%; Mean loss: 3.3635\n",
            "Iteration: 1199; Percent done: 30.0%; Mean loss: 3.5960\n",
            "Iteration: 1200; Percent done: 30.0%; Mean loss: 3.4595\n",
            "Iteration: 1201; Percent done: 30.0%; Mean loss: 3.2939\n",
            "Iteration: 1202; Percent done: 30.0%; Mean loss: 3.5070\n",
            "Iteration: 1203; Percent done: 30.1%; Mean loss: 3.6219\n",
            "Iteration: 1204; Percent done: 30.1%; Mean loss: 3.2207\n",
            "Iteration: 1205; Percent done: 30.1%; Mean loss: 3.4314\n",
            "Iteration: 1206; Percent done: 30.1%; Mean loss: 3.4535\n",
            "Iteration: 1207; Percent done: 30.2%; Mean loss: 3.5330\n",
            "Iteration: 1208; Percent done: 30.2%; Mean loss: 3.4549\n",
            "Iteration: 1209; Percent done: 30.2%; Mean loss: 3.4761\n",
            "Iteration: 1210; Percent done: 30.2%; Mean loss: 3.4295\n",
            "Iteration: 1211; Percent done: 30.3%; Mean loss: 3.6928\n",
            "Iteration: 1212; Percent done: 30.3%; Mean loss: 3.4102\n",
            "Iteration: 1213; Percent done: 30.3%; Mean loss: 3.7014\n",
            "Iteration: 1214; Percent done: 30.3%; Mean loss: 3.4319\n",
            "Iteration: 1215; Percent done: 30.4%; Mean loss: 3.3794\n",
            "Iteration: 1216; Percent done: 30.4%; Mean loss: 3.2338\n",
            "Iteration: 1217; Percent done: 30.4%; Mean loss: 3.5216\n",
            "Iteration: 1218; Percent done: 30.4%; Mean loss: 3.3179\n",
            "Iteration: 1219; Percent done: 30.5%; Mean loss: 3.4358\n",
            "Iteration: 1220; Percent done: 30.5%; Mean loss: 3.4735\n",
            "Iteration: 1221; Percent done: 30.5%; Mean loss: 3.0951\n",
            "Iteration: 1222; Percent done: 30.6%; Mean loss: 3.4626\n",
            "Iteration: 1223; Percent done: 30.6%; Mean loss: 3.4925\n",
            "Iteration: 1224; Percent done: 30.6%; Mean loss: 3.6484\n",
            "Iteration: 1225; Percent done: 30.6%; Mean loss: 3.5255\n",
            "Iteration: 1226; Percent done: 30.6%; Mean loss: 3.2993\n",
            "Iteration: 1227; Percent done: 30.7%; Mean loss: 3.3300\n",
            "Iteration: 1228; Percent done: 30.7%; Mean loss: 3.2890\n",
            "Iteration: 1229; Percent done: 30.7%; Mean loss: 3.2857\n",
            "Iteration: 1230; Percent done: 30.8%; Mean loss: 3.4761\n",
            "Iteration: 1231; Percent done: 30.8%; Mean loss: 3.3824\n",
            "Iteration: 1232; Percent done: 30.8%; Mean loss: 3.1577\n",
            "Iteration: 1233; Percent done: 30.8%; Mean loss: 3.3971\n",
            "Iteration: 1234; Percent done: 30.9%; Mean loss: 3.2688\n",
            "Iteration: 1235; Percent done: 30.9%; Mean loss: 3.4228\n",
            "Iteration: 1236; Percent done: 30.9%; Mean loss: 3.8249\n",
            "Iteration: 1237; Percent done: 30.9%; Mean loss: 3.3555\n",
            "Iteration: 1238; Percent done: 30.9%; Mean loss: 3.2431\n",
            "Iteration: 1239; Percent done: 31.0%; Mean loss: 3.5765\n",
            "Iteration: 1240; Percent done: 31.0%; Mean loss: 3.2948\n",
            "Iteration: 1241; Percent done: 31.0%; Mean loss: 3.5615\n",
            "Iteration: 1242; Percent done: 31.1%; Mean loss: 3.6755\n",
            "Iteration: 1243; Percent done: 31.1%; Mean loss: 3.4189\n",
            "Iteration: 1244; Percent done: 31.1%; Mean loss: 3.3815\n",
            "Iteration: 1245; Percent done: 31.1%; Mean loss: 3.1693\n",
            "Iteration: 1246; Percent done: 31.1%; Mean loss: 3.3664\n",
            "Iteration: 1247; Percent done: 31.2%; Mean loss: 3.3856\n",
            "Iteration: 1248; Percent done: 31.2%; Mean loss: 3.2840\n",
            "Iteration: 1249; Percent done: 31.2%; Mean loss: 3.3490\n",
            "Iteration: 1250; Percent done: 31.2%; Mean loss: 3.3756\n",
            "Iteration: 1251; Percent done: 31.3%; Mean loss: 3.1667\n",
            "Iteration: 1252; Percent done: 31.3%; Mean loss: 3.3295\n",
            "Iteration: 1253; Percent done: 31.3%; Mean loss: 3.6162\n",
            "Iteration: 1254; Percent done: 31.4%; Mean loss: 3.2845\n",
            "Iteration: 1255; Percent done: 31.4%; Mean loss: 3.4293\n",
            "Iteration: 1256; Percent done: 31.4%; Mean loss: 3.4533\n",
            "Iteration: 1257; Percent done: 31.4%; Mean loss: 3.3492\n",
            "Iteration: 1258; Percent done: 31.4%; Mean loss: 3.3587\n",
            "Iteration: 1259; Percent done: 31.5%; Mean loss: 3.2353\n",
            "Iteration: 1260; Percent done: 31.5%; Mean loss: 3.7104\n",
            "Iteration: 1261; Percent done: 31.5%; Mean loss: 3.2480\n",
            "Iteration: 1262; Percent done: 31.6%; Mean loss: 3.6707\n",
            "Iteration: 1263; Percent done: 31.6%; Mean loss: 3.5620\n",
            "Iteration: 1264; Percent done: 31.6%; Mean loss: 3.2475\n",
            "Iteration: 1265; Percent done: 31.6%; Mean loss: 3.2343\n",
            "Iteration: 1266; Percent done: 31.6%; Mean loss: 3.2557\n",
            "Iteration: 1267; Percent done: 31.7%; Mean loss: 3.3653\n",
            "Iteration: 1268; Percent done: 31.7%; Mean loss: 3.4822\n",
            "Iteration: 1269; Percent done: 31.7%; Mean loss: 3.0901\n",
            "Iteration: 1270; Percent done: 31.8%; Mean loss: 3.5597\n",
            "Iteration: 1271; Percent done: 31.8%; Mean loss: 3.2933\n",
            "Iteration: 1272; Percent done: 31.8%; Mean loss: 3.4724\n",
            "Iteration: 1273; Percent done: 31.8%; Mean loss: 3.2409\n",
            "Iteration: 1274; Percent done: 31.9%; Mean loss: 3.4110\n",
            "Iteration: 1275; Percent done: 31.9%; Mean loss: 3.2839\n",
            "Iteration: 1276; Percent done: 31.9%; Mean loss: 3.3841\n",
            "Iteration: 1277; Percent done: 31.9%; Mean loss: 3.0810\n",
            "Iteration: 1278; Percent done: 31.9%; Mean loss: 3.3333\n",
            "Iteration: 1279; Percent done: 32.0%; Mean loss: 3.3131\n",
            "Iteration: 1280; Percent done: 32.0%; Mean loss: 3.2690\n",
            "Iteration: 1281; Percent done: 32.0%; Mean loss: 3.2547\n",
            "Iteration: 1282; Percent done: 32.0%; Mean loss: 3.5244\n",
            "Iteration: 1283; Percent done: 32.1%; Mean loss: 3.4816\n",
            "Iteration: 1284; Percent done: 32.1%; Mean loss: 3.5042\n",
            "Iteration: 1285; Percent done: 32.1%; Mean loss: 3.3992\n",
            "Iteration: 1286; Percent done: 32.1%; Mean loss: 3.2037\n",
            "Iteration: 1287; Percent done: 32.2%; Mean loss: 3.4136\n",
            "Iteration: 1288; Percent done: 32.2%; Mean loss: 3.2850\n",
            "Iteration: 1289; Percent done: 32.2%; Mean loss: 3.5095\n",
            "Iteration: 1290; Percent done: 32.2%; Mean loss: 3.5413\n",
            "Iteration: 1291; Percent done: 32.3%; Mean loss: 3.6469\n",
            "Iteration: 1292; Percent done: 32.3%; Mean loss: 3.1970\n",
            "Iteration: 1293; Percent done: 32.3%; Mean loss: 3.3227\n",
            "Iteration: 1294; Percent done: 32.4%; Mean loss: 3.5864\n",
            "Iteration: 1295; Percent done: 32.4%; Mean loss: 3.2703\n",
            "Iteration: 1296; Percent done: 32.4%; Mean loss: 3.4356\n",
            "Iteration: 1297; Percent done: 32.4%; Mean loss: 3.3687\n",
            "Iteration: 1298; Percent done: 32.5%; Mean loss: 3.3357\n",
            "Iteration: 1299; Percent done: 32.5%; Mean loss: 3.2796\n",
            "Iteration: 1300; Percent done: 32.5%; Mean loss: 3.3905\n",
            "Iteration: 1301; Percent done: 32.5%; Mean loss: 3.7368\n",
            "Iteration: 1302; Percent done: 32.6%; Mean loss: 3.3537\n",
            "Iteration: 1303; Percent done: 32.6%; Mean loss: 3.5237\n",
            "Iteration: 1304; Percent done: 32.6%; Mean loss: 3.2297\n",
            "Iteration: 1305; Percent done: 32.6%; Mean loss: 3.5135\n",
            "Iteration: 1306; Percent done: 32.6%; Mean loss: 3.4449\n",
            "Iteration: 1307; Percent done: 32.7%; Mean loss: 3.2351\n",
            "Iteration: 1308; Percent done: 32.7%; Mean loss: 3.2481\n",
            "Iteration: 1309; Percent done: 32.7%; Mean loss: 3.4502\n",
            "Iteration: 1310; Percent done: 32.8%; Mean loss: 3.2982\n",
            "Iteration: 1311; Percent done: 32.8%; Mean loss: 3.1759\n",
            "Iteration: 1312; Percent done: 32.8%; Mean loss: 3.3360\n",
            "Iteration: 1313; Percent done: 32.8%; Mean loss: 3.2267\n",
            "Iteration: 1314; Percent done: 32.9%; Mean loss: 3.4380\n",
            "Iteration: 1315; Percent done: 32.9%; Mean loss: 3.6056\n",
            "Iteration: 1316; Percent done: 32.9%; Mean loss: 3.7881\n",
            "Iteration: 1317; Percent done: 32.9%; Mean loss: 3.3807\n",
            "Iteration: 1318; Percent done: 33.0%; Mean loss: 3.1754\n",
            "Iteration: 1319; Percent done: 33.0%; Mean loss: 3.1985\n",
            "Iteration: 1320; Percent done: 33.0%; Mean loss: 3.3954\n",
            "Iteration: 1321; Percent done: 33.0%; Mean loss: 3.6856\n",
            "Iteration: 1322; Percent done: 33.1%; Mean loss: 3.3570\n",
            "Iteration: 1323; Percent done: 33.1%; Mean loss: 3.2306\n",
            "Iteration: 1324; Percent done: 33.1%; Mean loss: 3.2925\n",
            "Iteration: 1325; Percent done: 33.1%; Mean loss: 3.3437\n",
            "Iteration: 1326; Percent done: 33.1%; Mean loss: 3.3861\n",
            "Iteration: 1327; Percent done: 33.2%; Mean loss: 3.3868\n",
            "Iteration: 1328; Percent done: 33.2%; Mean loss: 3.3741\n",
            "Iteration: 1329; Percent done: 33.2%; Mean loss: 3.3220\n",
            "Iteration: 1330; Percent done: 33.2%; Mean loss: 3.1729\n",
            "Iteration: 1331; Percent done: 33.3%; Mean loss: 3.5830\n",
            "Iteration: 1332; Percent done: 33.3%; Mean loss: 3.3172\n",
            "Iteration: 1333; Percent done: 33.3%; Mean loss: 3.4826\n",
            "Iteration: 1334; Percent done: 33.4%; Mean loss: 3.4762\n",
            "Iteration: 1335; Percent done: 33.4%; Mean loss: 3.3230\n",
            "Iteration: 1336; Percent done: 33.4%; Mean loss: 3.3249\n",
            "Iteration: 1337; Percent done: 33.4%; Mean loss: 3.3472\n",
            "Iteration: 1338; Percent done: 33.5%; Mean loss: 3.2404\n",
            "Iteration: 1339; Percent done: 33.5%; Mean loss: 3.2732\n",
            "Iteration: 1340; Percent done: 33.5%; Mean loss: 3.4899\n",
            "Iteration: 1341; Percent done: 33.5%; Mean loss: 3.5566\n",
            "Iteration: 1342; Percent done: 33.6%; Mean loss: 3.3361\n",
            "Iteration: 1343; Percent done: 33.6%; Mean loss: 3.5655\n",
            "Iteration: 1344; Percent done: 33.6%; Mean loss: 3.1492\n",
            "Iteration: 1345; Percent done: 33.6%; Mean loss: 3.3777\n",
            "Iteration: 1346; Percent done: 33.7%; Mean loss: 3.3917\n",
            "Iteration: 1347; Percent done: 33.7%; Mean loss: 3.3368\n",
            "Iteration: 1348; Percent done: 33.7%; Mean loss: 3.2162\n",
            "Iteration: 1349; Percent done: 33.7%; Mean loss: 3.2332\n",
            "Iteration: 1350; Percent done: 33.8%; Mean loss: 3.4206\n",
            "Iteration: 1351; Percent done: 33.8%; Mean loss: 3.4685\n",
            "Iteration: 1352; Percent done: 33.8%; Mean loss: 3.3992\n",
            "Iteration: 1353; Percent done: 33.8%; Mean loss: 3.5775\n",
            "Iteration: 1354; Percent done: 33.9%; Mean loss: 3.4460\n",
            "Iteration: 1355; Percent done: 33.9%; Mean loss: 3.4154\n",
            "Iteration: 1356; Percent done: 33.9%; Mean loss: 3.3306\n",
            "Iteration: 1357; Percent done: 33.9%; Mean loss: 3.1888\n",
            "Iteration: 1358; Percent done: 34.0%; Mean loss: 3.2382\n",
            "Iteration: 1359; Percent done: 34.0%; Mean loss: 3.4148\n",
            "Iteration: 1360; Percent done: 34.0%; Mean loss: 3.4346\n",
            "Iteration: 1361; Percent done: 34.0%; Mean loss: 3.5006\n",
            "Iteration: 1362; Percent done: 34.1%; Mean loss: 3.3866\n",
            "Iteration: 1363; Percent done: 34.1%; Mean loss: 3.3507\n",
            "Iteration: 1364; Percent done: 34.1%; Mean loss: 3.6145\n",
            "Iteration: 1365; Percent done: 34.1%; Mean loss: 3.4881\n",
            "Iteration: 1366; Percent done: 34.2%; Mean loss: 3.1205\n",
            "Iteration: 1367; Percent done: 34.2%; Mean loss: 3.4660\n",
            "Iteration: 1368; Percent done: 34.2%; Mean loss: 2.9883\n",
            "Iteration: 1369; Percent done: 34.2%; Mean loss: 3.2521\n",
            "Iteration: 1370; Percent done: 34.2%; Mean loss: 3.4240\n",
            "Iteration: 1371; Percent done: 34.3%; Mean loss: 3.3544\n",
            "Iteration: 1372; Percent done: 34.3%; Mean loss: 3.4412\n",
            "Iteration: 1373; Percent done: 34.3%; Mean loss: 3.2769\n",
            "Iteration: 1374; Percent done: 34.4%; Mean loss: 3.2195\n",
            "Iteration: 1375; Percent done: 34.4%; Mean loss: 3.7683\n",
            "Iteration: 1376; Percent done: 34.4%; Mean loss: 3.2568\n",
            "Iteration: 1377; Percent done: 34.4%; Mean loss: 3.3480\n",
            "Iteration: 1378; Percent done: 34.4%; Mean loss: 3.3575\n",
            "Iteration: 1379; Percent done: 34.5%; Mean loss: 3.3527\n",
            "Iteration: 1380; Percent done: 34.5%; Mean loss: 3.6340\n",
            "Iteration: 1381; Percent done: 34.5%; Mean loss: 3.2577\n",
            "Iteration: 1382; Percent done: 34.5%; Mean loss: 3.4237\n",
            "Iteration: 1383; Percent done: 34.6%; Mean loss: 3.0814\n",
            "Iteration: 1384; Percent done: 34.6%; Mean loss: 3.4471\n",
            "Iteration: 1385; Percent done: 34.6%; Mean loss: 3.1168\n",
            "Iteration: 1386; Percent done: 34.6%; Mean loss: 3.1373\n",
            "Iteration: 1387; Percent done: 34.7%; Mean loss: 3.3406\n",
            "Iteration: 1388; Percent done: 34.7%; Mean loss: 3.3430\n",
            "Iteration: 1389; Percent done: 34.7%; Mean loss: 3.3138\n",
            "Iteration: 1390; Percent done: 34.8%; Mean loss: 3.2489\n",
            "Iteration: 1391; Percent done: 34.8%; Mean loss: 3.2601\n",
            "Iteration: 1392; Percent done: 34.8%; Mean loss: 3.2432\n",
            "Iteration: 1393; Percent done: 34.8%; Mean loss: 3.5307\n",
            "Iteration: 1394; Percent done: 34.8%; Mean loss: 3.1369\n",
            "Iteration: 1395; Percent done: 34.9%; Mean loss: 3.3321\n",
            "Iteration: 1396; Percent done: 34.9%; Mean loss: 3.2342\n",
            "Iteration: 1397; Percent done: 34.9%; Mean loss: 3.4957\n",
            "Iteration: 1398; Percent done: 34.9%; Mean loss: 3.4538\n",
            "Iteration: 1399; Percent done: 35.0%; Mean loss: 3.2629\n",
            "Iteration: 1400; Percent done: 35.0%; Mean loss: 3.2029\n",
            "Iteration: 1401; Percent done: 35.0%; Mean loss: 3.5624\n",
            "Iteration: 1402; Percent done: 35.0%; Mean loss: 3.3716\n",
            "Iteration: 1403; Percent done: 35.1%; Mean loss: 3.3647\n",
            "Iteration: 1404; Percent done: 35.1%; Mean loss: 3.1935\n",
            "Iteration: 1405; Percent done: 35.1%; Mean loss: 3.0127\n",
            "Iteration: 1406; Percent done: 35.1%; Mean loss: 3.5266\n",
            "Iteration: 1407; Percent done: 35.2%; Mean loss: 3.4781\n",
            "Iteration: 1408; Percent done: 35.2%; Mean loss: 3.3578\n",
            "Iteration: 1409; Percent done: 35.2%; Mean loss: 3.4937\n",
            "Iteration: 1410; Percent done: 35.2%; Mean loss: 3.4520\n",
            "Iteration: 1411; Percent done: 35.3%; Mean loss: 3.4478\n",
            "Iteration: 1412; Percent done: 35.3%; Mean loss: 3.2939\n",
            "Iteration: 1413; Percent done: 35.3%; Mean loss: 3.1183\n",
            "Iteration: 1414; Percent done: 35.4%; Mean loss: 3.3204\n",
            "Iteration: 1415; Percent done: 35.4%; Mean loss: 3.2198\n",
            "Iteration: 1416; Percent done: 35.4%; Mean loss: 3.3398\n",
            "Iteration: 1417; Percent done: 35.4%; Mean loss: 3.3845\n",
            "Iteration: 1418; Percent done: 35.4%; Mean loss: 3.4809\n",
            "Iteration: 1419; Percent done: 35.5%; Mean loss: 3.4633\n",
            "Iteration: 1420; Percent done: 35.5%; Mean loss: 3.3574\n",
            "Iteration: 1421; Percent done: 35.5%; Mean loss: 3.6560\n",
            "Iteration: 1422; Percent done: 35.5%; Mean loss: 3.2600\n",
            "Iteration: 1423; Percent done: 35.6%; Mean loss: 3.2262\n",
            "Iteration: 1424; Percent done: 35.6%; Mean loss: 3.1859\n",
            "Iteration: 1425; Percent done: 35.6%; Mean loss: 3.0927\n",
            "Iteration: 1426; Percent done: 35.6%; Mean loss: 3.3101\n",
            "Iteration: 1427; Percent done: 35.7%; Mean loss: 3.1805\n",
            "Iteration: 1428; Percent done: 35.7%; Mean loss: 3.4617\n",
            "Iteration: 1429; Percent done: 35.7%; Mean loss: 3.5288\n",
            "Iteration: 1430; Percent done: 35.8%; Mean loss: 3.0725\n",
            "Iteration: 1431; Percent done: 35.8%; Mean loss: 3.5084\n",
            "Iteration: 1432; Percent done: 35.8%; Mean loss: 3.0655\n",
            "Iteration: 1433; Percent done: 35.8%; Mean loss: 3.2207\n",
            "Iteration: 1434; Percent done: 35.9%; Mean loss: 3.0848\n",
            "Iteration: 1435; Percent done: 35.9%; Mean loss: 3.2954\n",
            "Iteration: 1436; Percent done: 35.9%; Mean loss: 3.5305\n",
            "Iteration: 1437; Percent done: 35.9%; Mean loss: 3.2208\n",
            "Iteration: 1438; Percent done: 35.9%; Mean loss: 3.2862\n",
            "Iteration: 1439; Percent done: 36.0%; Mean loss: 3.1961\n",
            "Iteration: 1440; Percent done: 36.0%; Mean loss: 3.3590\n",
            "Iteration: 1441; Percent done: 36.0%; Mean loss: 3.5274\n",
            "Iteration: 1442; Percent done: 36.0%; Mean loss: 3.5137\n",
            "Iteration: 1443; Percent done: 36.1%; Mean loss: 3.0974\n",
            "Iteration: 1444; Percent done: 36.1%; Mean loss: 3.4964\n",
            "Iteration: 1445; Percent done: 36.1%; Mean loss: 3.2034\n",
            "Iteration: 1446; Percent done: 36.1%; Mean loss: 3.2060\n",
            "Iteration: 1447; Percent done: 36.2%; Mean loss: 3.1641\n",
            "Iteration: 1448; Percent done: 36.2%; Mean loss: 3.5108\n",
            "Iteration: 1449; Percent done: 36.2%; Mean loss: 3.3453\n",
            "Iteration: 1450; Percent done: 36.2%; Mean loss: 3.3434\n",
            "Iteration: 1451; Percent done: 36.3%; Mean loss: 3.0987\n",
            "Iteration: 1452; Percent done: 36.3%; Mean loss: 3.3477\n",
            "Iteration: 1453; Percent done: 36.3%; Mean loss: 3.2704\n",
            "Iteration: 1454; Percent done: 36.4%; Mean loss: 3.4915\n",
            "Iteration: 1455; Percent done: 36.4%; Mean loss: 3.3913\n",
            "Iteration: 1456; Percent done: 36.4%; Mean loss: 3.3731\n",
            "Iteration: 1457; Percent done: 36.4%; Mean loss: 3.2373\n",
            "Iteration: 1458; Percent done: 36.4%; Mean loss: 3.6206\n",
            "Iteration: 1459; Percent done: 36.5%; Mean loss: 3.3004\n",
            "Iteration: 1460; Percent done: 36.5%; Mean loss: 3.2458\n",
            "Iteration: 1461; Percent done: 36.5%; Mean loss: 3.3310\n",
            "Iteration: 1462; Percent done: 36.5%; Mean loss: 3.3657\n",
            "Iteration: 1463; Percent done: 36.6%; Mean loss: 3.1544\n",
            "Iteration: 1464; Percent done: 36.6%; Mean loss: 3.3767\n",
            "Iteration: 1465; Percent done: 36.6%; Mean loss: 3.1883\n",
            "Iteration: 1466; Percent done: 36.6%; Mean loss: 3.3085\n",
            "Iteration: 1467; Percent done: 36.7%; Mean loss: 3.4840\n",
            "Iteration: 1468; Percent done: 36.7%; Mean loss: 3.3589\n",
            "Iteration: 1469; Percent done: 36.7%; Mean loss: 3.3740\n",
            "Iteration: 1470; Percent done: 36.8%; Mean loss: 3.5673\n",
            "Iteration: 1471; Percent done: 36.8%; Mean loss: 3.1692\n",
            "Iteration: 1472; Percent done: 36.8%; Mean loss: 3.1101\n",
            "Iteration: 1473; Percent done: 36.8%; Mean loss: 3.2682\n",
            "Iteration: 1474; Percent done: 36.9%; Mean loss: 3.3790\n",
            "Iteration: 1475; Percent done: 36.9%; Mean loss: 3.4487\n",
            "Iteration: 1476; Percent done: 36.9%; Mean loss: 3.5357\n",
            "Iteration: 1477; Percent done: 36.9%; Mean loss: 3.3614\n",
            "Iteration: 1478; Percent done: 37.0%; Mean loss: 3.0585\n",
            "Iteration: 1479; Percent done: 37.0%; Mean loss: 3.1996\n",
            "Iteration: 1480; Percent done: 37.0%; Mean loss: 3.4492\n",
            "Iteration: 1481; Percent done: 37.0%; Mean loss: 3.3096\n",
            "Iteration: 1482; Percent done: 37.0%; Mean loss: 3.3722\n",
            "Iteration: 1483; Percent done: 37.1%; Mean loss: 3.4007\n",
            "Iteration: 1484; Percent done: 37.1%; Mean loss: 3.2461\n",
            "Iteration: 1485; Percent done: 37.1%; Mean loss: 3.2648\n",
            "Iteration: 1486; Percent done: 37.1%; Mean loss: 3.3787\n",
            "Iteration: 1487; Percent done: 37.2%; Mean loss: 3.1755\n",
            "Iteration: 1488; Percent done: 37.2%; Mean loss: 3.1993\n",
            "Iteration: 1489; Percent done: 37.2%; Mean loss: 3.3406\n",
            "Iteration: 1490; Percent done: 37.2%; Mean loss: 3.2390\n",
            "Iteration: 1491; Percent done: 37.3%; Mean loss: 3.1942\n",
            "Iteration: 1492; Percent done: 37.3%; Mean loss: 3.1361\n",
            "Iteration: 1493; Percent done: 37.3%; Mean loss: 3.2922\n",
            "Iteration: 1494; Percent done: 37.4%; Mean loss: 3.1624\n",
            "Iteration: 1495; Percent done: 37.4%; Mean loss: 3.0863\n",
            "Iteration: 1496; Percent done: 37.4%; Mean loss: 3.2818\n",
            "Iteration: 1497; Percent done: 37.4%; Mean loss: 3.2589\n",
            "Iteration: 1498; Percent done: 37.5%; Mean loss: 3.3119\n",
            "Iteration: 1499; Percent done: 37.5%; Mean loss: 3.0787\n",
            "Iteration: 1500; Percent done: 37.5%; Mean loss: 3.7038\n",
            "Iteration: 1501; Percent done: 37.5%; Mean loss: 3.3278\n",
            "Iteration: 1502; Percent done: 37.5%; Mean loss: 3.5512\n",
            "Iteration: 1503; Percent done: 37.6%; Mean loss: 3.2933\n",
            "Iteration: 1504; Percent done: 37.6%; Mean loss: 3.5439\n",
            "Iteration: 1505; Percent done: 37.6%; Mean loss: 3.3249\n",
            "Iteration: 1506; Percent done: 37.6%; Mean loss: 3.2263\n",
            "Iteration: 1507; Percent done: 37.7%; Mean loss: 3.1489\n",
            "Iteration: 1508; Percent done: 37.7%; Mean loss: 3.4113\n",
            "Iteration: 1509; Percent done: 37.7%; Mean loss: 3.5368\n",
            "Iteration: 1510; Percent done: 37.8%; Mean loss: 3.4212\n",
            "Iteration: 1511; Percent done: 37.8%; Mean loss: 3.1705\n",
            "Iteration: 1512; Percent done: 37.8%; Mean loss: 3.2519\n",
            "Iteration: 1513; Percent done: 37.8%; Mean loss: 3.2831\n",
            "Iteration: 1514; Percent done: 37.9%; Mean loss: 3.2653\n",
            "Iteration: 1515; Percent done: 37.9%; Mean loss: 3.5485\n",
            "Iteration: 1516; Percent done: 37.9%; Mean loss: 3.1454\n",
            "Iteration: 1517; Percent done: 37.9%; Mean loss: 3.3418\n",
            "Iteration: 1518; Percent done: 38.0%; Mean loss: 3.0685\n",
            "Iteration: 1519; Percent done: 38.0%; Mean loss: 3.6331\n",
            "Iteration: 1520; Percent done: 38.0%; Mean loss: 3.2709\n",
            "Iteration: 1521; Percent done: 38.0%; Mean loss: 3.2446\n",
            "Iteration: 1522; Percent done: 38.0%; Mean loss: 3.3934\n",
            "Iteration: 1523; Percent done: 38.1%; Mean loss: 3.3183\n",
            "Iteration: 1524; Percent done: 38.1%; Mean loss: 3.3785\n",
            "Iteration: 1525; Percent done: 38.1%; Mean loss: 3.0948\n",
            "Iteration: 1526; Percent done: 38.1%; Mean loss: 3.0799\n",
            "Iteration: 1527; Percent done: 38.2%; Mean loss: 3.3121\n",
            "Iteration: 1528; Percent done: 38.2%; Mean loss: 3.2373\n",
            "Iteration: 1529; Percent done: 38.2%; Mean loss: 3.2790\n",
            "Iteration: 1530; Percent done: 38.2%; Mean loss: 3.3770\n",
            "Iteration: 1531; Percent done: 38.3%; Mean loss: 3.2612\n",
            "Iteration: 1532; Percent done: 38.3%; Mean loss: 3.1744\n",
            "Iteration: 1533; Percent done: 38.3%; Mean loss: 3.2174\n",
            "Iteration: 1534; Percent done: 38.4%; Mean loss: 3.2516\n",
            "Iteration: 1535; Percent done: 38.4%; Mean loss: 3.1502\n",
            "Iteration: 1536; Percent done: 38.4%; Mean loss: 3.3588\n",
            "Iteration: 1537; Percent done: 38.4%; Mean loss: 3.3863\n",
            "Iteration: 1538; Percent done: 38.5%; Mean loss: 3.5031\n",
            "Iteration: 1539; Percent done: 38.5%; Mean loss: 3.2776\n",
            "Iteration: 1540; Percent done: 38.5%; Mean loss: 2.9787\n",
            "Iteration: 1541; Percent done: 38.5%; Mean loss: 3.3549\n",
            "Iteration: 1542; Percent done: 38.6%; Mean loss: 3.4639\n",
            "Iteration: 1543; Percent done: 38.6%; Mean loss: 3.3098\n",
            "Iteration: 1544; Percent done: 38.6%; Mean loss: 3.3553\n",
            "Iteration: 1545; Percent done: 38.6%; Mean loss: 3.1899\n",
            "Iteration: 1546; Percent done: 38.6%; Mean loss: 3.4975\n",
            "Iteration: 1547; Percent done: 38.7%; Mean loss: 3.1502\n",
            "Iteration: 1548; Percent done: 38.7%; Mean loss: 3.0833\n",
            "Iteration: 1549; Percent done: 38.7%; Mean loss: 3.3573\n",
            "Iteration: 1550; Percent done: 38.8%; Mean loss: 3.2681\n",
            "Iteration: 1551; Percent done: 38.8%; Mean loss: 3.6079\n",
            "Iteration: 1552; Percent done: 38.8%; Mean loss: 3.4386\n",
            "Iteration: 1553; Percent done: 38.8%; Mean loss: 3.4516\n",
            "Iteration: 1554; Percent done: 38.9%; Mean loss: 3.2202\n",
            "Iteration: 1555; Percent done: 38.9%; Mean loss: 3.4532\n",
            "Iteration: 1556; Percent done: 38.9%; Mean loss: 3.3479\n",
            "Iteration: 1557; Percent done: 38.9%; Mean loss: 3.0053\n",
            "Iteration: 1558; Percent done: 39.0%; Mean loss: 3.2555\n",
            "Iteration: 1559; Percent done: 39.0%; Mean loss: 3.5240\n",
            "Iteration: 1560; Percent done: 39.0%; Mean loss: 3.2319\n",
            "Iteration: 1561; Percent done: 39.0%; Mean loss: 3.3366\n",
            "Iteration: 1562; Percent done: 39.1%; Mean loss: 3.0895\n",
            "Iteration: 1563; Percent done: 39.1%; Mean loss: 3.1411\n",
            "Iteration: 1564; Percent done: 39.1%; Mean loss: 3.1668\n",
            "Iteration: 1565; Percent done: 39.1%; Mean loss: 3.4177\n",
            "Iteration: 1566; Percent done: 39.1%; Mean loss: 3.3331\n",
            "Iteration: 1567; Percent done: 39.2%; Mean loss: 3.3134\n",
            "Iteration: 1568; Percent done: 39.2%; Mean loss: 3.3994\n",
            "Iteration: 1569; Percent done: 39.2%; Mean loss: 3.3633\n",
            "Iteration: 1570; Percent done: 39.2%; Mean loss: 3.2963\n",
            "Iteration: 1571; Percent done: 39.3%; Mean loss: 3.4352\n",
            "Iteration: 1572; Percent done: 39.3%; Mean loss: 3.4749\n",
            "Iteration: 1573; Percent done: 39.3%; Mean loss: 3.2365\n",
            "Iteration: 1574; Percent done: 39.4%; Mean loss: 3.0766\n",
            "Iteration: 1575; Percent done: 39.4%; Mean loss: 3.3269\n",
            "Iteration: 1576; Percent done: 39.4%; Mean loss: 3.3243\n",
            "Iteration: 1577; Percent done: 39.4%; Mean loss: 3.2047\n",
            "Iteration: 1578; Percent done: 39.5%; Mean loss: 3.3823\n",
            "Iteration: 1579; Percent done: 39.5%; Mean loss: 3.2596\n",
            "Iteration: 1580; Percent done: 39.5%; Mean loss: 3.4218\n",
            "Iteration: 1581; Percent done: 39.5%; Mean loss: 3.0358\n",
            "Iteration: 1582; Percent done: 39.6%; Mean loss: 3.2255\n",
            "Iteration: 1583; Percent done: 39.6%; Mean loss: 3.4120\n",
            "Iteration: 1584; Percent done: 39.6%; Mean loss: 3.3976\n",
            "Iteration: 1585; Percent done: 39.6%; Mean loss: 3.5579\n",
            "Iteration: 1586; Percent done: 39.6%; Mean loss: 3.2847\n",
            "Iteration: 1587; Percent done: 39.7%; Mean loss: 3.5273\n",
            "Iteration: 1588; Percent done: 39.7%; Mean loss: 3.2864\n",
            "Iteration: 1589; Percent done: 39.7%; Mean loss: 3.3936\n",
            "Iteration: 1590; Percent done: 39.8%; Mean loss: 3.2409\n",
            "Iteration: 1591; Percent done: 39.8%; Mean loss: 3.2595\n",
            "Iteration: 1592; Percent done: 39.8%; Mean loss: 3.2879\n",
            "Iteration: 1593; Percent done: 39.8%; Mean loss: 3.3456\n",
            "Iteration: 1594; Percent done: 39.9%; Mean loss: 3.2076\n",
            "Iteration: 1595; Percent done: 39.9%; Mean loss: 3.0644\n",
            "Iteration: 1596; Percent done: 39.9%; Mean loss: 3.4371\n",
            "Iteration: 1597; Percent done: 39.9%; Mean loss: 3.4973\n",
            "Iteration: 1598; Percent done: 40.0%; Mean loss: 3.3911\n",
            "Iteration: 1599; Percent done: 40.0%; Mean loss: 3.4141\n",
            "Iteration: 1600; Percent done: 40.0%; Mean loss: 3.5416\n",
            "Iteration: 1601; Percent done: 40.0%; Mean loss: 3.3424\n",
            "Iteration: 1602; Percent done: 40.1%; Mean loss: 3.2907\n",
            "Iteration: 1603; Percent done: 40.1%; Mean loss: 3.5114\n",
            "Iteration: 1604; Percent done: 40.1%; Mean loss: 2.9490\n",
            "Iteration: 1605; Percent done: 40.1%; Mean loss: 3.1389\n",
            "Iteration: 1606; Percent done: 40.2%; Mean loss: 3.1194\n",
            "Iteration: 1607; Percent done: 40.2%; Mean loss: 3.3416\n",
            "Iteration: 1608; Percent done: 40.2%; Mean loss: 3.3452\n",
            "Iteration: 1609; Percent done: 40.2%; Mean loss: 3.2886\n",
            "Iteration: 1610; Percent done: 40.2%; Mean loss: 3.1239\n",
            "Iteration: 1611; Percent done: 40.3%; Mean loss: 3.1680\n",
            "Iteration: 1612; Percent done: 40.3%; Mean loss: 3.2056\n",
            "Iteration: 1613; Percent done: 40.3%; Mean loss: 3.4530\n",
            "Iteration: 1614; Percent done: 40.4%; Mean loss: 3.4492\n",
            "Iteration: 1615; Percent done: 40.4%; Mean loss: 3.1847\n",
            "Iteration: 1616; Percent done: 40.4%; Mean loss: 3.2640\n",
            "Iteration: 1617; Percent done: 40.4%; Mean loss: 3.3160\n",
            "Iteration: 1618; Percent done: 40.5%; Mean loss: 3.4363\n",
            "Iteration: 1619; Percent done: 40.5%; Mean loss: 3.1811\n",
            "Iteration: 1620; Percent done: 40.5%; Mean loss: 3.0746\n",
            "Iteration: 1621; Percent done: 40.5%; Mean loss: 3.2707\n",
            "Iteration: 1622; Percent done: 40.6%; Mean loss: 3.5166\n",
            "Iteration: 1623; Percent done: 40.6%; Mean loss: 3.5504\n",
            "Iteration: 1624; Percent done: 40.6%; Mean loss: 3.4652\n",
            "Iteration: 1625; Percent done: 40.6%; Mean loss: 3.4395\n",
            "Iteration: 1626; Percent done: 40.6%; Mean loss: 3.1699\n",
            "Iteration: 1627; Percent done: 40.7%; Mean loss: 3.2429\n",
            "Iteration: 1628; Percent done: 40.7%; Mean loss: 3.4452\n",
            "Iteration: 1629; Percent done: 40.7%; Mean loss: 3.0133\n",
            "Iteration: 1630; Percent done: 40.8%; Mean loss: 3.4688\n",
            "Iteration: 1631; Percent done: 40.8%; Mean loss: 3.2921\n",
            "Iteration: 1632; Percent done: 40.8%; Mean loss: 3.1506\n",
            "Iteration: 1633; Percent done: 40.8%; Mean loss: 3.1108\n",
            "Iteration: 1634; Percent done: 40.8%; Mean loss: 3.1877\n",
            "Iteration: 1635; Percent done: 40.9%; Mean loss: 3.2204\n",
            "Iteration: 1636; Percent done: 40.9%; Mean loss: 3.2985\n",
            "Iteration: 1637; Percent done: 40.9%; Mean loss: 3.3455\n",
            "Iteration: 1638; Percent done: 40.9%; Mean loss: 3.1381\n",
            "Iteration: 1639; Percent done: 41.0%; Mean loss: 3.2535\n",
            "Iteration: 1640; Percent done: 41.0%; Mean loss: 3.1751\n",
            "Iteration: 1641; Percent done: 41.0%; Mean loss: 3.3291\n",
            "Iteration: 1642; Percent done: 41.0%; Mean loss: 3.0386\n",
            "Iteration: 1643; Percent done: 41.1%; Mean loss: 3.0921\n",
            "Iteration: 1644; Percent done: 41.1%; Mean loss: 3.2931\n",
            "Iteration: 1645; Percent done: 41.1%; Mean loss: 3.1850\n",
            "Iteration: 1646; Percent done: 41.1%; Mean loss: 3.1118\n",
            "Iteration: 1647; Percent done: 41.2%; Mean loss: 3.5674\n",
            "Iteration: 1648; Percent done: 41.2%; Mean loss: 3.4366\n",
            "Iteration: 1649; Percent done: 41.2%; Mean loss: 3.6042\n",
            "Iteration: 1650; Percent done: 41.2%; Mean loss: 3.2075\n",
            "Iteration: 1651; Percent done: 41.3%; Mean loss: 3.3052\n",
            "Iteration: 1652; Percent done: 41.3%; Mean loss: 3.0551\n",
            "Iteration: 1653; Percent done: 41.3%; Mean loss: 3.3340\n",
            "Iteration: 1654; Percent done: 41.3%; Mean loss: 3.1014\n",
            "Iteration: 1655; Percent done: 41.4%; Mean loss: 3.0673\n",
            "Iteration: 1656; Percent done: 41.4%; Mean loss: 3.2089\n",
            "Iteration: 1657; Percent done: 41.4%; Mean loss: 3.5693\n",
            "Iteration: 1658; Percent done: 41.4%; Mean loss: 3.1961\n",
            "Iteration: 1659; Percent done: 41.5%; Mean loss: 3.2803\n",
            "Iteration: 1660; Percent done: 41.5%; Mean loss: 3.8229\n",
            "Iteration: 1661; Percent done: 41.5%; Mean loss: 3.3052\n",
            "Iteration: 1662; Percent done: 41.5%; Mean loss: 3.3193\n",
            "Iteration: 1663; Percent done: 41.6%; Mean loss: 3.2277\n",
            "Iteration: 1664; Percent done: 41.6%; Mean loss: 3.1346\n",
            "Iteration: 1665; Percent done: 41.6%; Mean loss: 3.2824\n",
            "Iteration: 1666; Percent done: 41.6%; Mean loss: 3.4316\n",
            "Iteration: 1667; Percent done: 41.7%; Mean loss: 3.2634\n",
            "Iteration: 1668; Percent done: 41.7%; Mean loss: 3.2481\n",
            "Iteration: 1669; Percent done: 41.7%; Mean loss: 3.2308\n",
            "Iteration: 1670; Percent done: 41.8%; Mean loss: 3.1501\n",
            "Iteration: 1671; Percent done: 41.8%; Mean loss: 3.3769\n",
            "Iteration: 1672; Percent done: 41.8%; Mean loss: 3.4093\n",
            "Iteration: 1673; Percent done: 41.8%; Mean loss: 3.0855\n",
            "Iteration: 1674; Percent done: 41.9%; Mean loss: 3.5234\n",
            "Iteration: 1675; Percent done: 41.9%; Mean loss: 3.2117\n",
            "Iteration: 1676; Percent done: 41.9%; Mean loss: 3.1376\n",
            "Iteration: 1677; Percent done: 41.9%; Mean loss: 3.3724\n",
            "Iteration: 1678; Percent done: 41.9%; Mean loss: 3.2416\n",
            "Iteration: 1679; Percent done: 42.0%; Mean loss: 3.4779\n",
            "Iteration: 1680; Percent done: 42.0%; Mean loss: 3.2379\n",
            "Iteration: 1681; Percent done: 42.0%; Mean loss: 3.2776\n",
            "Iteration: 1682; Percent done: 42.0%; Mean loss: 3.4038\n",
            "Iteration: 1683; Percent done: 42.1%; Mean loss: 3.1723\n",
            "Iteration: 1684; Percent done: 42.1%; Mean loss: 3.3221\n",
            "Iteration: 1685; Percent done: 42.1%; Mean loss: 3.4231\n",
            "Iteration: 1686; Percent done: 42.1%; Mean loss: 3.4218\n",
            "Iteration: 1687; Percent done: 42.2%; Mean loss: 3.4764\n",
            "Iteration: 1688; Percent done: 42.2%; Mean loss: 2.9569\n",
            "Iteration: 1689; Percent done: 42.2%; Mean loss: 3.3851\n",
            "Iteration: 1690; Percent done: 42.2%; Mean loss: 3.2572\n",
            "Iteration: 1691; Percent done: 42.3%; Mean loss: 2.9743\n",
            "Iteration: 1692; Percent done: 42.3%; Mean loss: 3.3122\n",
            "Iteration: 1693; Percent done: 42.3%; Mean loss: 3.4327\n",
            "Iteration: 1694; Percent done: 42.4%; Mean loss: 3.1878\n",
            "Iteration: 1695; Percent done: 42.4%; Mean loss: 3.3431\n",
            "Iteration: 1696; Percent done: 42.4%; Mean loss: 2.9887\n",
            "Iteration: 1697; Percent done: 42.4%; Mean loss: 3.1749\n",
            "Iteration: 1698; Percent done: 42.4%; Mean loss: 3.4090\n",
            "Iteration: 1699; Percent done: 42.5%; Mean loss: 3.4546\n",
            "Iteration: 1700; Percent done: 42.5%; Mean loss: 3.2607\n",
            "Iteration: 1701; Percent done: 42.5%; Mean loss: 3.4015\n",
            "Iteration: 1702; Percent done: 42.5%; Mean loss: 3.3963\n",
            "Iteration: 1703; Percent done: 42.6%; Mean loss: 3.2004\n",
            "Iteration: 1704; Percent done: 42.6%; Mean loss: 3.2374\n",
            "Iteration: 1705; Percent done: 42.6%; Mean loss: 3.0450\n",
            "Iteration: 1706; Percent done: 42.6%; Mean loss: 3.1343\n",
            "Iteration: 1707; Percent done: 42.7%; Mean loss: 3.5843\n",
            "Iteration: 1708; Percent done: 42.7%; Mean loss: 3.1647\n",
            "Iteration: 1709; Percent done: 42.7%; Mean loss: 3.5401\n",
            "Iteration: 1710; Percent done: 42.8%; Mean loss: 3.2994\n",
            "Iteration: 1711; Percent done: 42.8%; Mean loss: 3.3118\n",
            "Iteration: 1712; Percent done: 42.8%; Mean loss: 3.3239\n",
            "Iteration: 1713; Percent done: 42.8%; Mean loss: 3.1922\n",
            "Iteration: 1714; Percent done: 42.9%; Mean loss: 3.2316\n",
            "Iteration: 1715; Percent done: 42.9%; Mean loss: 3.3763\n",
            "Iteration: 1716; Percent done: 42.9%; Mean loss: 3.2362\n",
            "Iteration: 1717; Percent done: 42.9%; Mean loss: 3.3782\n",
            "Iteration: 1718; Percent done: 43.0%; Mean loss: 3.0915\n",
            "Iteration: 1719; Percent done: 43.0%; Mean loss: 3.0312\n",
            "Iteration: 1720; Percent done: 43.0%; Mean loss: 3.1135\n",
            "Iteration: 1721; Percent done: 43.0%; Mean loss: 3.1464\n",
            "Iteration: 1722; Percent done: 43.0%; Mean loss: 3.3046\n",
            "Iteration: 1723; Percent done: 43.1%; Mean loss: 3.4924\n",
            "Iteration: 1724; Percent done: 43.1%; Mean loss: 3.3365\n",
            "Iteration: 1725; Percent done: 43.1%; Mean loss: 3.2475\n",
            "Iteration: 1726; Percent done: 43.1%; Mean loss: 3.4462\n",
            "Iteration: 1727; Percent done: 43.2%; Mean loss: 3.2812\n",
            "Iteration: 1728; Percent done: 43.2%; Mean loss: 3.4135\n",
            "Iteration: 1729; Percent done: 43.2%; Mean loss: 3.2215\n",
            "Iteration: 1730; Percent done: 43.2%; Mean loss: 3.4554\n",
            "Iteration: 1731; Percent done: 43.3%; Mean loss: 3.2758\n",
            "Iteration: 1732; Percent done: 43.3%; Mean loss: 3.2341\n",
            "Iteration: 1733; Percent done: 43.3%; Mean loss: 3.0828\n",
            "Iteration: 1734; Percent done: 43.4%; Mean loss: 2.9981\n",
            "Iteration: 1735; Percent done: 43.4%; Mean loss: 3.4017\n",
            "Iteration: 1736; Percent done: 43.4%; Mean loss: 3.4799\n",
            "Iteration: 1737; Percent done: 43.4%; Mean loss: 3.2647\n",
            "Iteration: 1738; Percent done: 43.5%; Mean loss: 3.2669\n",
            "Iteration: 1739; Percent done: 43.5%; Mean loss: 3.3802\n",
            "Iteration: 1740; Percent done: 43.5%; Mean loss: 3.2440\n",
            "Iteration: 1741; Percent done: 43.5%; Mean loss: 3.3874\n",
            "Iteration: 1742; Percent done: 43.5%; Mean loss: 3.1471\n",
            "Iteration: 1743; Percent done: 43.6%; Mean loss: 3.2386\n",
            "Iteration: 1744; Percent done: 43.6%; Mean loss: 3.1221\n",
            "Iteration: 1745; Percent done: 43.6%; Mean loss: 3.2924\n",
            "Iteration: 1746; Percent done: 43.6%; Mean loss: 3.5802\n",
            "Iteration: 1747; Percent done: 43.7%; Mean loss: 3.2739\n",
            "Iteration: 1748; Percent done: 43.7%; Mean loss: 3.2168\n",
            "Iteration: 1749; Percent done: 43.7%; Mean loss: 3.1674\n",
            "Iteration: 1750; Percent done: 43.8%; Mean loss: 3.1757\n",
            "Iteration: 1751; Percent done: 43.8%; Mean loss: 3.3291\n",
            "Iteration: 1752; Percent done: 43.8%; Mean loss: 3.2373\n",
            "Iteration: 1753; Percent done: 43.8%; Mean loss: 2.9658\n",
            "Iteration: 1754; Percent done: 43.9%; Mean loss: 3.4412\n",
            "Iteration: 1755; Percent done: 43.9%; Mean loss: 3.2683\n",
            "Iteration: 1756; Percent done: 43.9%; Mean loss: 3.1559\n",
            "Iteration: 1757; Percent done: 43.9%; Mean loss: 3.2376\n",
            "Iteration: 1758; Percent done: 44.0%; Mean loss: 3.4629\n",
            "Iteration: 1759; Percent done: 44.0%; Mean loss: 3.2812\n",
            "Iteration: 1760; Percent done: 44.0%; Mean loss: 3.1258\n",
            "Iteration: 1761; Percent done: 44.0%; Mean loss: 3.1635\n",
            "Iteration: 1762; Percent done: 44.0%; Mean loss: 3.1729\n",
            "Iteration: 1763; Percent done: 44.1%; Mean loss: 3.3260\n",
            "Iteration: 1764; Percent done: 44.1%; Mean loss: 3.0487\n",
            "Iteration: 1765; Percent done: 44.1%; Mean loss: 3.5112\n",
            "Iteration: 1766; Percent done: 44.1%; Mean loss: 3.1123\n",
            "Iteration: 1767; Percent done: 44.2%; Mean loss: 3.3282\n",
            "Iteration: 1768; Percent done: 44.2%; Mean loss: 3.3662\n",
            "Iteration: 1769; Percent done: 44.2%; Mean loss: 3.3561\n",
            "Iteration: 1770; Percent done: 44.2%; Mean loss: 3.3601\n",
            "Iteration: 1771; Percent done: 44.3%; Mean loss: 3.1241\n",
            "Iteration: 1772; Percent done: 44.3%; Mean loss: 3.4180\n",
            "Iteration: 1773; Percent done: 44.3%; Mean loss: 3.3955\n",
            "Iteration: 1774; Percent done: 44.4%; Mean loss: 3.0524\n",
            "Iteration: 1775; Percent done: 44.4%; Mean loss: 3.4185\n",
            "Iteration: 1776; Percent done: 44.4%; Mean loss: 3.2747\n",
            "Iteration: 1777; Percent done: 44.4%; Mean loss: 3.4312\n",
            "Iteration: 1778; Percent done: 44.5%; Mean loss: 3.3352\n",
            "Iteration: 1779; Percent done: 44.5%; Mean loss: 3.4558\n",
            "Iteration: 1780; Percent done: 44.5%; Mean loss: 3.2071\n",
            "Iteration: 1781; Percent done: 44.5%; Mean loss: 3.0610\n",
            "Iteration: 1782; Percent done: 44.5%; Mean loss: 3.0989\n",
            "Iteration: 1783; Percent done: 44.6%; Mean loss: 3.0881\n",
            "Iteration: 1784; Percent done: 44.6%; Mean loss: 3.1447\n",
            "Iteration: 1785; Percent done: 44.6%; Mean loss: 3.0893\n",
            "Iteration: 1786; Percent done: 44.6%; Mean loss: 3.3893\n",
            "Iteration: 1787; Percent done: 44.7%; Mean loss: 3.4027\n",
            "Iteration: 1788; Percent done: 44.7%; Mean loss: 3.1322\n",
            "Iteration: 1789; Percent done: 44.7%; Mean loss: 3.3783\n",
            "Iteration: 1790; Percent done: 44.8%; Mean loss: 3.3219\n",
            "Iteration: 1791; Percent done: 44.8%; Mean loss: 3.0431\n",
            "Iteration: 1792; Percent done: 44.8%; Mean loss: 3.0203\n",
            "Iteration: 1793; Percent done: 44.8%; Mean loss: 3.3884\n",
            "Iteration: 1794; Percent done: 44.9%; Mean loss: 3.1006\n",
            "Iteration: 1795; Percent done: 44.9%; Mean loss: 3.0999\n",
            "Iteration: 1796; Percent done: 44.9%; Mean loss: 3.3866\n",
            "Iteration: 1797; Percent done: 44.9%; Mean loss: 3.0904\n",
            "Iteration: 1798; Percent done: 45.0%; Mean loss: 3.1200\n",
            "Iteration: 1799; Percent done: 45.0%; Mean loss: 3.1174\n",
            "Iteration: 1800; Percent done: 45.0%; Mean loss: 3.0640\n",
            "Iteration: 1801; Percent done: 45.0%; Mean loss: 3.3063\n",
            "Iteration: 1802; Percent done: 45.1%; Mean loss: 3.3610\n",
            "Iteration: 1803; Percent done: 45.1%; Mean loss: 3.1652\n",
            "Iteration: 1804; Percent done: 45.1%; Mean loss: 3.1145\n",
            "Iteration: 1805; Percent done: 45.1%; Mean loss: 3.1481\n",
            "Iteration: 1806; Percent done: 45.1%; Mean loss: 3.5149\n",
            "Iteration: 1807; Percent done: 45.2%; Mean loss: 2.9892\n",
            "Iteration: 1808; Percent done: 45.2%; Mean loss: 3.4152\n",
            "Iteration: 1809; Percent done: 45.2%; Mean loss: 3.2218\n",
            "Iteration: 1810; Percent done: 45.2%; Mean loss: 3.2614\n",
            "Iteration: 1811; Percent done: 45.3%; Mean loss: 3.1801\n",
            "Iteration: 1812; Percent done: 45.3%; Mean loss: 3.1626\n",
            "Iteration: 1813; Percent done: 45.3%; Mean loss: 3.0079\n",
            "Iteration: 1814; Percent done: 45.4%; Mean loss: 3.1913\n",
            "Iteration: 1815; Percent done: 45.4%; Mean loss: 3.4567\n",
            "Iteration: 1816; Percent done: 45.4%; Mean loss: 3.3484\n",
            "Iteration: 1817; Percent done: 45.4%; Mean loss: 3.0746\n",
            "Iteration: 1818; Percent done: 45.5%; Mean loss: 3.1764\n",
            "Iteration: 1819; Percent done: 45.5%; Mean loss: 3.2408\n",
            "Iteration: 1820; Percent done: 45.5%; Mean loss: 3.1532\n",
            "Iteration: 1821; Percent done: 45.5%; Mean loss: 3.1483\n",
            "Iteration: 1822; Percent done: 45.6%; Mean loss: 3.3430\n",
            "Iteration: 1823; Percent done: 45.6%; Mean loss: 3.2651\n",
            "Iteration: 1824; Percent done: 45.6%; Mean loss: 3.0654\n",
            "Iteration: 1825; Percent done: 45.6%; Mean loss: 3.2536\n",
            "Iteration: 1826; Percent done: 45.6%; Mean loss: 3.2999\n",
            "Iteration: 1827; Percent done: 45.7%; Mean loss: 3.3603\n",
            "Iteration: 1828; Percent done: 45.7%; Mean loss: 3.5356\n",
            "Iteration: 1829; Percent done: 45.7%; Mean loss: 3.0973\n",
            "Iteration: 1830; Percent done: 45.8%; Mean loss: 3.2237\n",
            "Iteration: 1831; Percent done: 45.8%; Mean loss: 3.3811\n",
            "Iteration: 1832; Percent done: 45.8%; Mean loss: 3.1175\n",
            "Iteration: 1833; Percent done: 45.8%; Mean loss: 3.0477\n",
            "Iteration: 1834; Percent done: 45.9%; Mean loss: 3.2891\n",
            "Iteration: 1835; Percent done: 45.9%; Mean loss: 3.2775\n",
            "Iteration: 1836; Percent done: 45.9%; Mean loss: 3.4208\n",
            "Iteration: 1837; Percent done: 45.9%; Mean loss: 3.3707\n",
            "Iteration: 1838; Percent done: 46.0%; Mean loss: 3.2510\n",
            "Iteration: 1839; Percent done: 46.0%; Mean loss: 3.1970\n",
            "Iteration: 1840; Percent done: 46.0%; Mean loss: 3.0526\n",
            "Iteration: 1841; Percent done: 46.0%; Mean loss: 3.2695\n",
            "Iteration: 1842; Percent done: 46.1%; Mean loss: 3.1558\n",
            "Iteration: 1843; Percent done: 46.1%; Mean loss: 3.2423\n",
            "Iteration: 1844; Percent done: 46.1%; Mean loss: 3.1936\n",
            "Iteration: 1845; Percent done: 46.1%; Mean loss: 3.1272\n",
            "Iteration: 1846; Percent done: 46.2%; Mean loss: 3.3506\n",
            "Iteration: 1847; Percent done: 46.2%; Mean loss: 3.3066\n",
            "Iteration: 1848; Percent done: 46.2%; Mean loss: 3.3584\n",
            "Iteration: 1849; Percent done: 46.2%; Mean loss: 3.2241\n",
            "Iteration: 1850; Percent done: 46.2%; Mean loss: 3.1555\n",
            "Iteration: 1851; Percent done: 46.3%; Mean loss: 3.3173\n",
            "Iteration: 1852; Percent done: 46.3%; Mean loss: 3.0203\n",
            "Iteration: 1853; Percent done: 46.3%; Mean loss: 3.0235\n",
            "Iteration: 1854; Percent done: 46.4%; Mean loss: 2.9932\n",
            "Iteration: 1855; Percent done: 46.4%; Mean loss: 3.4016\n",
            "Iteration: 1856; Percent done: 46.4%; Mean loss: 3.2393\n",
            "Iteration: 1857; Percent done: 46.4%; Mean loss: 3.2297\n",
            "Iteration: 1858; Percent done: 46.5%; Mean loss: 3.4057\n",
            "Iteration: 1859; Percent done: 46.5%; Mean loss: 3.0791\n",
            "Iteration: 1860; Percent done: 46.5%; Mean loss: 3.3371\n",
            "Iteration: 1861; Percent done: 46.5%; Mean loss: 3.3241\n",
            "Iteration: 1862; Percent done: 46.6%; Mean loss: 3.3451\n",
            "Iteration: 1863; Percent done: 46.6%; Mean loss: 2.9937\n",
            "Iteration: 1864; Percent done: 46.6%; Mean loss: 3.2690\n",
            "Iteration: 1865; Percent done: 46.6%; Mean loss: 3.2344\n",
            "Iteration: 1866; Percent done: 46.7%; Mean loss: 3.6338\n",
            "Iteration: 1867; Percent done: 46.7%; Mean loss: 3.2873\n",
            "Iteration: 1868; Percent done: 46.7%; Mean loss: 3.2447\n",
            "Iteration: 1869; Percent done: 46.7%; Mean loss: 3.1939\n",
            "Iteration: 1870; Percent done: 46.8%; Mean loss: 3.4005\n",
            "Iteration: 1871; Percent done: 46.8%; Mean loss: 3.3979\n",
            "Iteration: 1872; Percent done: 46.8%; Mean loss: 3.2158\n",
            "Iteration: 1873; Percent done: 46.8%; Mean loss: 3.2826\n",
            "Iteration: 1874; Percent done: 46.9%; Mean loss: 2.8703\n",
            "Iteration: 1875; Percent done: 46.9%; Mean loss: 3.3785\n",
            "Iteration: 1876; Percent done: 46.9%; Mean loss: 3.2209\n",
            "Iteration: 1877; Percent done: 46.9%; Mean loss: 3.2884\n",
            "Iteration: 1878; Percent done: 46.9%; Mean loss: 3.2591\n",
            "Iteration: 1879; Percent done: 47.0%; Mean loss: 3.3082\n",
            "Iteration: 1880; Percent done: 47.0%; Mean loss: 3.1676\n",
            "Iteration: 1881; Percent done: 47.0%; Mean loss: 3.2020\n",
            "Iteration: 1882; Percent done: 47.0%; Mean loss: 3.2609\n",
            "Iteration: 1883; Percent done: 47.1%; Mean loss: 3.1362\n",
            "Iteration: 1884; Percent done: 47.1%; Mean loss: 3.3150\n",
            "Iteration: 1885; Percent done: 47.1%; Mean loss: 3.2549\n",
            "Iteration: 1886; Percent done: 47.1%; Mean loss: 3.0880\n",
            "Iteration: 1887; Percent done: 47.2%; Mean loss: 3.1052\n",
            "Iteration: 1888; Percent done: 47.2%; Mean loss: 3.3396\n",
            "Iteration: 1889; Percent done: 47.2%; Mean loss: 3.0623\n",
            "Iteration: 1890; Percent done: 47.2%; Mean loss: 3.4946\n",
            "Iteration: 1891; Percent done: 47.3%; Mean loss: 3.2006\n",
            "Iteration: 1892; Percent done: 47.3%; Mean loss: 3.1792\n",
            "Iteration: 1893; Percent done: 47.3%; Mean loss: 3.1652\n",
            "Iteration: 1894; Percent done: 47.3%; Mean loss: 3.2878\n",
            "Iteration: 1895; Percent done: 47.4%; Mean loss: 3.0951\n",
            "Iteration: 1896; Percent done: 47.4%; Mean loss: 3.0531\n",
            "Iteration: 1897; Percent done: 47.4%; Mean loss: 3.2117\n",
            "Iteration: 1898; Percent done: 47.4%; Mean loss: 3.3411\n",
            "Iteration: 1899; Percent done: 47.5%; Mean loss: 3.4613\n",
            "Iteration: 1900; Percent done: 47.5%; Mean loss: 2.8701\n",
            "Iteration: 1901; Percent done: 47.5%; Mean loss: 3.5665\n",
            "Iteration: 1902; Percent done: 47.5%; Mean loss: 3.3304\n",
            "Iteration: 1903; Percent done: 47.6%; Mean loss: 3.3226\n",
            "Iteration: 1904; Percent done: 47.6%; Mean loss: 3.3731\n",
            "Iteration: 1905; Percent done: 47.6%; Mean loss: 3.2279\n",
            "Iteration: 1906; Percent done: 47.6%; Mean loss: 3.2124\n",
            "Iteration: 1907; Percent done: 47.7%; Mean loss: 3.3617\n",
            "Iteration: 1908; Percent done: 47.7%; Mean loss: 3.1453\n",
            "Iteration: 1909; Percent done: 47.7%; Mean loss: 3.4033\n",
            "Iteration: 1910; Percent done: 47.8%; Mean loss: 3.0030\n",
            "Iteration: 1911; Percent done: 47.8%; Mean loss: 3.2182\n",
            "Iteration: 1912; Percent done: 47.8%; Mean loss: 3.0987\n",
            "Iteration: 1913; Percent done: 47.8%; Mean loss: 3.1944\n",
            "Iteration: 1914; Percent done: 47.9%; Mean loss: 3.4660\n",
            "Iteration: 1915; Percent done: 47.9%; Mean loss: 3.1639\n",
            "Iteration: 1916; Percent done: 47.9%; Mean loss: 2.9994\n",
            "Iteration: 1917; Percent done: 47.9%; Mean loss: 3.3023\n",
            "Iteration: 1918; Percent done: 47.9%; Mean loss: 3.3124\n",
            "Iteration: 1919; Percent done: 48.0%; Mean loss: 3.2004\n",
            "Iteration: 1920; Percent done: 48.0%; Mean loss: 3.2705\n",
            "Iteration: 1921; Percent done: 48.0%; Mean loss: 3.3261\n",
            "Iteration: 1922; Percent done: 48.0%; Mean loss: 3.2008\n",
            "Iteration: 1923; Percent done: 48.1%; Mean loss: 3.3326\n",
            "Iteration: 1924; Percent done: 48.1%; Mean loss: 2.9192\n",
            "Iteration: 1925; Percent done: 48.1%; Mean loss: 3.0553\n",
            "Iteration: 1926; Percent done: 48.1%; Mean loss: 3.2524\n",
            "Iteration: 1927; Percent done: 48.2%; Mean loss: 3.2211\n",
            "Iteration: 1928; Percent done: 48.2%; Mean loss: 3.4009\n",
            "Iteration: 1929; Percent done: 48.2%; Mean loss: 3.1345\n",
            "Iteration: 1930; Percent done: 48.2%; Mean loss: 3.2177\n",
            "Iteration: 1931; Percent done: 48.3%; Mean loss: 3.1450\n",
            "Iteration: 1932; Percent done: 48.3%; Mean loss: 3.3065\n",
            "Iteration: 1933; Percent done: 48.3%; Mean loss: 3.0990\n",
            "Iteration: 1934; Percent done: 48.4%; Mean loss: 3.3752\n",
            "Iteration: 1935; Percent done: 48.4%; Mean loss: 3.2264\n",
            "Iteration: 1936; Percent done: 48.4%; Mean loss: 3.1768\n",
            "Iteration: 1937; Percent done: 48.4%; Mean loss: 3.2841\n",
            "Iteration: 1938; Percent done: 48.4%; Mean loss: 3.2042\n",
            "Iteration: 1939; Percent done: 48.5%; Mean loss: 3.4466\n",
            "Iteration: 1940; Percent done: 48.5%; Mean loss: 3.2057\n",
            "Iteration: 1941; Percent done: 48.5%; Mean loss: 3.2295\n",
            "Iteration: 1942; Percent done: 48.5%; Mean loss: 3.2412\n",
            "Iteration: 1943; Percent done: 48.6%; Mean loss: 3.2117\n",
            "Iteration: 1944; Percent done: 48.6%; Mean loss: 3.1355\n",
            "Iteration: 1945; Percent done: 48.6%; Mean loss: 3.1616\n",
            "Iteration: 1946; Percent done: 48.6%; Mean loss: 3.3937\n",
            "Iteration: 1947; Percent done: 48.7%; Mean loss: 3.2059\n",
            "Iteration: 1948; Percent done: 48.7%; Mean loss: 3.2775\n",
            "Iteration: 1949; Percent done: 48.7%; Mean loss: 3.2484\n",
            "Iteration: 1950; Percent done: 48.8%; Mean loss: 3.2887\n",
            "Iteration: 1951; Percent done: 48.8%; Mean loss: 3.1880\n",
            "Iteration: 1952; Percent done: 48.8%; Mean loss: 3.2115\n",
            "Iteration: 1953; Percent done: 48.8%; Mean loss: 3.2181\n",
            "Iteration: 1954; Percent done: 48.9%; Mean loss: 3.0384\n",
            "Iteration: 1955; Percent done: 48.9%; Mean loss: 3.2619\n",
            "Iteration: 1956; Percent done: 48.9%; Mean loss: 3.1118\n",
            "Iteration: 1957; Percent done: 48.9%; Mean loss: 2.9817\n",
            "Iteration: 1958; Percent done: 48.9%; Mean loss: 3.1178\n",
            "Iteration: 1959; Percent done: 49.0%; Mean loss: 3.0731\n",
            "Iteration: 1960; Percent done: 49.0%; Mean loss: 3.0911\n",
            "Iteration: 1961; Percent done: 49.0%; Mean loss: 3.3611\n",
            "Iteration: 1962; Percent done: 49.0%; Mean loss: 3.0094\n",
            "Iteration: 1963; Percent done: 49.1%; Mean loss: 3.3053\n",
            "Iteration: 1964; Percent done: 49.1%; Mean loss: 3.0422\n",
            "Iteration: 1965; Percent done: 49.1%; Mean loss: 2.9975\n",
            "Iteration: 1966; Percent done: 49.1%; Mean loss: 3.3677\n",
            "Iteration: 1967; Percent done: 49.2%; Mean loss: 3.2319\n",
            "Iteration: 1968; Percent done: 49.2%; Mean loss: 3.1887\n",
            "Iteration: 1969; Percent done: 49.2%; Mean loss: 3.3027\n",
            "Iteration: 1970; Percent done: 49.2%; Mean loss: 3.1230\n",
            "Iteration: 1971; Percent done: 49.3%; Mean loss: 3.1744\n",
            "Iteration: 1972; Percent done: 49.3%; Mean loss: 3.0420\n",
            "Iteration: 1973; Percent done: 49.3%; Mean loss: 3.2950\n",
            "Iteration: 1974; Percent done: 49.4%; Mean loss: 3.2782\n",
            "Iteration: 1975; Percent done: 49.4%; Mean loss: 3.0056\n",
            "Iteration: 1976; Percent done: 49.4%; Mean loss: 3.2782\n",
            "Iteration: 1977; Percent done: 49.4%; Mean loss: 3.1886\n",
            "Iteration: 1978; Percent done: 49.5%; Mean loss: 3.2661\n",
            "Iteration: 1979; Percent done: 49.5%; Mean loss: 3.0665\n",
            "Iteration: 1980; Percent done: 49.5%; Mean loss: 3.2339\n",
            "Iteration: 1981; Percent done: 49.5%; Mean loss: 3.1309\n",
            "Iteration: 1982; Percent done: 49.5%; Mean loss: 3.2030\n",
            "Iteration: 1983; Percent done: 49.6%; Mean loss: 3.3583\n",
            "Iteration: 1984; Percent done: 49.6%; Mean loss: 3.3182\n",
            "Iteration: 1985; Percent done: 49.6%; Mean loss: 3.0042\n",
            "Iteration: 1986; Percent done: 49.6%; Mean loss: 3.2393\n",
            "Iteration: 1987; Percent done: 49.7%; Mean loss: 3.0116\n",
            "Iteration: 1988; Percent done: 49.7%; Mean loss: 3.1783\n",
            "Iteration: 1989; Percent done: 49.7%; Mean loss: 2.9488\n",
            "Iteration: 1990; Percent done: 49.8%; Mean loss: 2.8990\n",
            "Iteration: 1991; Percent done: 49.8%; Mean loss: 3.1745\n",
            "Iteration: 1992; Percent done: 49.8%; Mean loss: 3.2645\n",
            "Iteration: 1993; Percent done: 49.8%; Mean loss: 3.1596\n",
            "Iteration: 1994; Percent done: 49.9%; Mean loss: 3.1207\n",
            "Iteration: 1995; Percent done: 49.9%; Mean loss: 3.2380\n",
            "Iteration: 1996; Percent done: 49.9%; Mean loss: 3.1520\n",
            "Iteration: 1997; Percent done: 49.9%; Mean loss: 3.1609\n",
            "Iteration: 1998; Percent done: 50.0%; Mean loss: 3.2909\n",
            "Iteration: 1999; Percent done: 50.0%; Mean loss: 3.3190\n",
            "Iteration: 2000; Percent done: 50.0%; Mean loss: 3.2804\n",
            "Iteration: 2001; Percent done: 50.0%; Mean loss: 3.0914\n",
            "Iteration: 2002; Percent done: 50.0%; Mean loss: 3.3705\n",
            "Iteration: 2003; Percent done: 50.1%; Mean loss: 3.2589\n",
            "Iteration: 2004; Percent done: 50.1%; Mean loss: 3.1533\n",
            "Iteration: 2005; Percent done: 50.1%; Mean loss: 3.1842\n",
            "Iteration: 2006; Percent done: 50.1%; Mean loss: 3.0057\n",
            "Iteration: 2007; Percent done: 50.2%; Mean loss: 3.3955\n",
            "Iteration: 2008; Percent done: 50.2%; Mean loss: 3.1836\n",
            "Iteration: 2009; Percent done: 50.2%; Mean loss: 3.4084\n",
            "Iteration: 2010; Percent done: 50.2%; Mean loss: 3.0470\n",
            "Iteration: 2011; Percent done: 50.3%; Mean loss: 3.0248\n",
            "Iteration: 2012; Percent done: 50.3%; Mean loss: 3.3521\n",
            "Iteration: 2013; Percent done: 50.3%; Mean loss: 3.1530\n",
            "Iteration: 2014; Percent done: 50.3%; Mean loss: 3.0971\n",
            "Iteration: 2015; Percent done: 50.4%; Mean loss: 3.2992\n",
            "Iteration: 2016; Percent done: 50.4%; Mean loss: 3.0709\n",
            "Iteration: 2017; Percent done: 50.4%; Mean loss: 3.1795\n",
            "Iteration: 2018; Percent done: 50.4%; Mean loss: 3.3224\n",
            "Iteration: 2019; Percent done: 50.5%; Mean loss: 3.2039\n",
            "Iteration: 2020; Percent done: 50.5%; Mean loss: 3.2404\n",
            "Iteration: 2021; Percent done: 50.5%; Mean loss: 2.8984\n",
            "Iteration: 2022; Percent done: 50.5%; Mean loss: 3.2380\n",
            "Iteration: 2023; Percent done: 50.6%; Mean loss: 3.0422\n",
            "Iteration: 2024; Percent done: 50.6%; Mean loss: 3.1596\n",
            "Iteration: 2025; Percent done: 50.6%; Mean loss: 3.2141\n",
            "Iteration: 2026; Percent done: 50.6%; Mean loss: 3.3299\n",
            "Iteration: 2027; Percent done: 50.7%; Mean loss: 2.9344\n",
            "Iteration: 2028; Percent done: 50.7%; Mean loss: 2.9267\n",
            "Iteration: 2029; Percent done: 50.7%; Mean loss: 3.0512\n",
            "Iteration: 2030; Percent done: 50.7%; Mean loss: 3.2239\n",
            "Iteration: 2031; Percent done: 50.8%; Mean loss: 3.2206\n",
            "Iteration: 2032; Percent done: 50.8%; Mean loss: 2.9962\n",
            "Iteration: 2033; Percent done: 50.8%; Mean loss: 2.9571\n",
            "Iteration: 2034; Percent done: 50.8%; Mean loss: 2.8909\n",
            "Iteration: 2035; Percent done: 50.9%; Mean loss: 3.3879\n",
            "Iteration: 2036; Percent done: 50.9%; Mean loss: 3.0612\n",
            "Iteration: 2037; Percent done: 50.9%; Mean loss: 3.3405\n",
            "Iteration: 2038; Percent done: 50.9%; Mean loss: 3.1316\n",
            "Iteration: 2039; Percent done: 51.0%; Mean loss: 3.0887\n",
            "Iteration: 2040; Percent done: 51.0%; Mean loss: 3.3914\n",
            "Iteration: 2041; Percent done: 51.0%; Mean loss: 3.3002\n",
            "Iteration: 2042; Percent done: 51.0%; Mean loss: 3.3002\n",
            "Iteration: 2043; Percent done: 51.1%; Mean loss: 3.2185\n",
            "Iteration: 2044; Percent done: 51.1%; Mean loss: 3.3308\n",
            "Iteration: 2045; Percent done: 51.1%; Mean loss: 3.0259\n",
            "Iteration: 2046; Percent done: 51.1%; Mean loss: 3.1398\n",
            "Iteration: 2047; Percent done: 51.2%; Mean loss: 3.3095\n",
            "Iteration: 2048; Percent done: 51.2%; Mean loss: 3.1285\n",
            "Iteration: 2049; Percent done: 51.2%; Mean loss: 3.2839\n",
            "Iteration: 2050; Percent done: 51.2%; Mean loss: 3.0280\n",
            "Iteration: 2051; Percent done: 51.3%; Mean loss: 3.0804\n",
            "Iteration: 2052; Percent done: 51.3%; Mean loss: 3.0952\n",
            "Iteration: 2053; Percent done: 51.3%; Mean loss: 3.2403\n",
            "Iteration: 2054; Percent done: 51.3%; Mean loss: 3.1653\n",
            "Iteration: 2055; Percent done: 51.4%; Mean loss: 3.1327\n",
            "Iteration: 2056; Percent done: 51.4%; Mean loss: 3.2082\n",
            "Iteration: 2057; Percent done: 51.4%; Mean loss: 3.1903\n",
            "Iteration: 2058; Percent done: 51.4%; Mean loss: 3.1447\n",
            "Iteration: 2059; Percent done: 51.5%; Mean loss: 3.3056\n",
            "Iteration: 2060; Percent done: 51.5%; Mean loss: 2.9357\n",
            "Iteration: 2061; Percent done: 51.5%; Mean loss: 3.2484\n",
            "Iteration: 2062; Percent done: 51.5%; Mean loss: 3.2848\n",
            "Iteration: 2063; Percent done: 51.6%; Mean loss: 3.3765\n",
            "Iteration: 2064; Percent done: 51.6%; Mean loss: 3.3530\n",
            "Iteration: 2065; Percent done: 51.6%; Mean loss: 2.9487\n",
            "Iteration: 2066; Percent done: 51.6%; Mean loss: 3.1306\n",
            "Iteration: 2067; Percent done: 51.7%; Mean loss: 3.2555\n",
            "Iteration: 2068; Percent done: 51.7%; Mean loss: 3.2981\n",
            "Iteration: 2069; Percent done: 51.7%; Mean loss: 3.4517\n",
            "Iteration: 2070; Percent done: 51.7%; Mean loss: 3.0760\n",
            "Iteration: 2071; Percent done: 51.8%; Mean loss: 2.9079\n",
            "Iteration: 2072; Percent done: 51.8%; Mean loss: 3.4145\n",
            "Iteration: 2073; Percent done: 51.8%; Mean loss: 3.1163\n",
            "Iteration: 2074; Percent done: 51.8%; Mean loss: 2.9391\n",
            "Iteration: 2075; Percent done: 51.9%; Mean loss: 3.0581\n",
            "Iteration: 2076; Percent done: 51.9%; Mean loss: 2.9010\n",
            "Iteration: 2077; Percent done: 51.9%; Mean loss: 3.0035\n",
            "Iteration: 2078; Percent done: 51.9%; Mean loss: 3.3631\n",
            "Iteration: 2079; Percent done: 52.0%; Mean loss: 3.1104\n",
            "Iteration: 2080; Percent done: 52.0%; Mean loss: 3.1352\n",
            "Iteration: 2081; Percent done: 52.0%; Mean loss: 3.1948\n",
            "Iteration: 2082; Percent done: 52.0%; Mean loss: 3.2635\n",
            "Iteration: 2083; Percent done: 52.1%; Mean loss: 3.2232\n",
            "Iteration: 2084; Percent done: 52.1%; Mean loss: 2.9544\n",
            "Iteration: 2085; Percent done: 52.1%; Mean loss: 3.2619\n",
            "Iteration: 2086; Percent done: 52.1%; Mean loss: 3.2380\n",
            "Iteration: 2087; Percent done: 52.2%; Mean loss: 3.0447\n",
            "Iteration: 2088; Percent done: 52.2%; Mean loss: 3.3615\n",
            "Iteration: 2089; Percent done: 52.2%; Mean loss: 3.1865\n",
            "Iteration: 2090; Percent done: 52.2%; Mean loss: 3.1599\n",
            "Iteration: 2091; Percent done: 52.3%; Mean loss: 3.0376\n",
            "Iteration: 2092; Percent done: 52.3%; Mean loss: 3.1290\n",
            "Iteration: 2093; Percent done: 52.3%; Mean loss: 3.0430\n",
            "Iteration: 2094; Percent done: 52.3%; Mean loss: 3.1930\n",
            "Iteration: 2095; Percent done: 52.4%; Mean loss: 3.2689\n",
            "Iteration: 2096; Percent done: 52.4%; Mean loss: 3.1263\n",
            "Iteration: 2097; Percent done: 52.4%; Mean loss: 3.3031\n",
            "Iteration: 2098; Percent done: 52.4%; Mean loss: 3.3585\n",
            "Iteration: 2099; Percent done: 52.5%; Mean loss: 2.9925\n",
            "Iteration: 2100; Percent done: 52.5%; Mean loss: 3.2792\n",
            "Iteration: 2101; Percent done: 52.5%; Mean loss: 3.0018\n",
            "Iteration: 2102; Percent done: 52.5%; Mean loss: 3.1811\n",
            "Iteration: 2103; Percent done: 52.6%; Mean loss: 2.9888\n",
            "Iteration: 2104; Percent done: 52.6%; Mean loss: 3.4519\n",
            "Iteration: 2105; Percent done: 52.6%; Mean loss: 3.2020\n",
            "Iteration: 2106; Percent done: 52.6%; Mean loss: 3.1943\n",
            "Iteration: 2107; Percent done: 52.7%; Mean loss: 3.2438\n",
            "Iteration: 2108; Percent done: 52.7%; Mean loss: 3.3267\n",
            "Iteration: 2109; Percent done: 52.7%; Mean loss: 3.0535\n",
            "Iteration: 2110; Percent done: 52.8%; Mean loss: 3.1319\n",
            "Iteration: 2111; Percent done: 52.8%; Mean loss: 3.2751\n",
            "Iteration: 2112; Percent done: 52.8%; Mean loss: 3.3665\n",
            "Iteration: 2113; Percent done: 52.8%; Mean loss: 3.1393\n",
            "Iteration: 2114; Percent done: 52.8%; Mean loss: 3.1410\n",
            "Iteration: 2115; Percent done: 52.9%; Mean loss: 3.2560\n",
            "Iteration: 2116; Percent done: 52.9%; Mean loss: 3.0401\n",
            "Iteration: 2117; Percent done: 52.9%; Mean loss: 2.9904\n",
            "Iteration: 2118; Percent done: 52.9%; Mean loss: 3.3228\n",
            "Iteration: 2119; Percent done: 53.0%; Mean loss: 3.3733\n",
            "Iteration: 2120; Percent done: 53.0%; Mean loss: 3.1929\n",
            "Iteration: 2121; Percent done: 53.0%; Mean loss: 3.1281\n",
            "Iteration: 2122; Percent done: 53.0%; Mean loss: 3.1358\n",
            "Iteration: 2123; Percent done: 53.1%; Mean loss: 3.0708\n",
            "Iteration: 2124; Percent done: 53.1%; Mean loss: 2.9461\n",
            "Iteration: 2125; Percent done: 53.1%; Mean loss: 3.2350\n",
            "Iteration: 2126; Percent done: 53.1%; Mean loss: 3.0966\n",
            "Iteration: 2127; Percent done: 53.2%; Mean loss: 3.2663\n",
            "Iteration: 2128; Percent done: 53.2%; Mean loss: 3.1965\n",
            "Iteration: 2129; Percent done: 53.2%; Mean loss: 3.1333\n",
            "Iteration: 2130; Percent done: 53.2%; Mean loss: 3.0852\n",
            "Iteration: 2131; Percent done: 53.3%; Mean loss: 2.9835\n",
            "Iteration: 2132; Percent done: 53.3%; Mean loss: 3.1395\n",
            "Iteration: 2133; Percent done: 53.3%; Mean loss: 3.1013\n",
            "Iteration: 2134; Percent done: 53.3%; Mean loss: 3.1660\n",
            "Iteration: 2135; Percent done: 53.4%; Mean loss: 3.2741\n",
            "Iteration: 2136; Percent done: 53.4%; Mean loss: 3.1143\n",
            "Iteration: 2137; Percent done: 53.4%; Mean loss: 3.2404\n",
            "Iteration: 2138; Percent done: 53.4%; Mean loss: 3.2462\n",
            "Iteration: 2139; Percent done: 53.5%; Mean loss: 3.3119\n",
            "Iteration: 2140; Percent done: 53.5%; Mean loss: 3.0037\n",
            "Iteration: 2141; Percent done: 53.5%; Mean loss: 3.1478\n",
            "Iteration: 2142; Percent done: 53.5%; Mean loss: 3.2317\n",
            "Iteration: 2143; Percent done: 53.6%; Mean loss: 3.5616\n",
            "Iteration: 2144; Percent done: 53.6%; Mean loss: 3.2337\n",
            "Iteration: 2145; Percent done: 53.6%; Mean loss: 3.0551\n",
            "Iteration: 2146; Percent done: 53.6%; Mean loss: 3.2092\n",
            "Iteration: 2147; Percent done: 53.7%; Mean loss: 3.3253\n",
            "Iteration: 2148; Percent done: 53.7%; Mean loss: 3.2318\n",
            "Iteration: 2149; Percent done: 53.7%; Mean loss: 3.1286\n",
            "Iteration: 2150; Percent done: 53.8%; Mean loss: 3.0047\n",
            "Iteration: 2151; Percent done: 53.8%; Mean loss: 3.1813\n",
            "Iteration: 2152; Percent done: 53.8%; Mean loss: 3.1606\n",
            "Iteration: 2153; Percent done: 53.8%; Mean loss: 3.2386\n",
            "Iteration: 2154; Percent done: 53.8%; Mean loss: 3.2996\n",
            "Iteration: 2155; Percent done: 53.9%; Mean loss: 3.2239\n",
            "Iteration: 2156; Percent done: 53.9%; Mean loss: 3.2896\n",
            "Iteration: 2157; Percent done: 53.9%; Mean loss: 3.1118\n",
            "Iteration: 2158; Percent done: 53.9%; Mean loss: 2.8812\n",
            "Iteration: 2159; Percent done: 54.0%; Mean loss: 2.9634\n",
            "Iteration: 2160; Percent done: 54.0%; Mean loss: 3.0432\n",
            "Iteration: 2161; Percent done: 54.0%; Mean loss: 3.2970\n",
            "Iteration: 2162; Percent done: 54.0%; Mean loss: 3.2794\n",
            "Iteration: 2163; Percent done: 54.1%; Mean loss: 2.7888\n",
            "Iteration: 2164; Percent done: 54.1%; Mean loss: 3.1323\n",
            "Iteration: 2165; Percent done: 54.1%; Mean loss: 3.2210\n",
            "Iteration: 2166; Percent done: 54.1%; Mean loss: 2.9133\n",
            "Iteration: 2167; Percent done: 54.2%; Mean loss: 3.2325\n",
            "Iteration: 2168; Percent done: 54.2%; Mean loss: 3.3988\n",
            "Iteration: 2169; Percent done: 54.2%; Mean loss: 3.2703\n",
            "Iteration: 2170; Percent done: 54.2%; Mean loss: 3.4091\n",
            "Iteration: 2171; Percent done: 54.3%; Mean loss: 3.2058\n",
            "Iteration: 2172; Percent done: 54.3%; Mean loss: 3.1050\n",
            "Iteration: 2173; Percent done: 54.3%; Mean loss: 2.9752\n",
            "Iteration: 2174; Percent done: 54.4%; Mean loss: 3.0873\n",
            "Iteration: 2175; Percent done: 54.4%; Mean loss: 3.1281\n",
            "Iteration: 2176; Percent done: 54.4%; Mean loss: 3.0343\n",
            "Iteration: 2177; Percent done: 54.4%; Mean loss: 3.1238\n",
            "Iteration: 2178; Percent done: 54.4%; Mean loss: 3.1879\n",
            "Iteration: 2179; Percent done: 54.5%; Mean loss: 3.3246\n",
            "Iteration: 2180; Percent done: 54.5%; Mean loss: 3.2982\n",
            "Iteration: 2181; Percent done: 54.5%; Mean loss: 2.9919\n",
            "Iteration: 2182; Percent done: 54.5%; Mean loss: 3.2619\n",
            "Iteration: 2183; Percent done: 54.6%; Mean loss: 3.2099\n",
            "Iteration: 2184; Percent done: 54.6%; Mean loss: 2.9119\n",
            "Iteration: 2185; Percent done: 54.6%; Mean loss: 3.0018\n",
            "Iteration: 2186; Percent done: 54.6%; Mean loss: 3.5186\n",
            "Iteration: 2187; Percent done: 54.7%; Mean loss: 2.8481\n",
            "Iteration: 2188; Percent done: 54.7%; Mean loss: 3.0413\n",
            "Iteration: 2189; Percent done: 54.7%; Mean loss: 2.9377\n",
            "Iteration: 2190; Percent done: 54.8%; Mean loss: 3.3234\n",
            "Iteration: 2191; Percent done: 54.8%; Mean loss: 3.0654\n",
            "Iteration: 2192; Percent done: 54.8%; Mean loss: 3.0265\n",
            "Iteration: 2193; Percent done: 54.8%; Mean loss: 3.2694\n",
            "Iteration: 2194; Percent done: 54.9%; Mean loss: 2.9493\n",
            "Iteration: 2195; Percent done: 54.9%; Mean loss: 3.1576\n",
            "Iteration: 2196; Percent done: 54.9%; Mean loss: 2.8312\n",
            "Iteration: 2197; Percent done: 54.9%; Mean loss: 3.1665\n",
            "Iteration: 2198; Percent done: 54.9%; Mean loss: 3.2597\n",
            "Iteration: 2199; Percent done: 55.0%; Mean loss: 3.0493\n",
            "Iteration: 2200; Percent done: 55.0%; Mean loss: 3.0087\n",
            "Iteration: 2201; Percent done: 55.0%; Mean loss: 3.0971\n",
            "Iteration: 2202; Percent done: 55.0%; Mean loss: 3.0309\n",
            "Iteration: 2203; Percent done: 55.1%; Mean loss: 2.8514\n",
            "Iteration: 2204; Percent done: 55.1%; Mean loss: 3.2160\n",
            "Iteration: 2205; Percent done: 55.1%; Mean loss: 3.1523\n",
            "Iteration: 2206; Percent done: 55.1%; Mean loss: 2.8160\n",
            "Iteration: 2207; Percent done: 55.2%; Mean loss: 3.2543\n",
            "Iteration: 2208; Percent done: 55.2%; Mean loss: 3.2283\n",
            "Iteration: 2209; Percent done: 55.2%; Mean loss: 3.0371\n",
            "Iteration: 2210; Percent done: 55.2%; Mean loss: 3.1393\n",
            "Iteration: 2211; Percent done: 55.3%; Mean loss: 3.1365\n",
            "Iteration: 2212; Percent done: 55.3%; Mean loss: 2.7708\n",
            "Iteration: 2213; Percent done: 55.3%; Mean loss: 3.1560\n",
            "Iteration: 2214; Percent done: 55.4%; Mean loss: 2.9189\n",
            "Iteration: 2215; Percent done: 55.4%; Mean loss: 3.1671\n",
            "Iteration: 2216; Percent done: 55.4%; Mean loss: 3.0868\n",
            "Iteration: 2217; Percent done: 55.4%; Mean loss: 3.1779\n",
            "Iteration: 2218; Percent done: 55.5%; Mean loss: 3.0994\n",
            "Iteration: 2219; Percent done: 55.5%; Mean loss: 3.1835\n",
            "Iteration: 2220; Percent done: 55.5%; Mean loss: 3.0742\n",
            "Iteration: 2221; Percent done: 55.5%; Mean loss: 3.0631\n",
            "Iteration: 2222; Percent done: 55.5%; Mean loss: 2.6664\n",
            "Iteration: 2223; Percent done: 55.6%; Mean loss: 3.0537\n",
            "Iteration: 2224; Percent done: 55.6%; Mean loss: 3.1772\n",
            "Iteration: 2225; Percent done: 55.6%; Mean loss: 3.2367\n",
            "Iteration: 2226; Percent done: 55.6%; Mean loss: 3.3311\n",
            "Iteration: 2227; Percent done: 55.7%; Mean loss: 3.2839\n",
            "Iteration: 2228; Percent done: 55.7%; Mean loss: 3.3874\n",
            "Iteration: 2229; Percent done: 55.7%; Mean loss: 2.9455\n",
            "Iteration: 2230; Percent done: 55.8%; Mean loss: 3.3142\n",
            "Iteration: 2231; Percent done: 55.8%; Mean loss: 3.0499\n",
            "Iteration: 2232; Percent done: 55.8%; Mean loss: 2.7865\n",
            "Iteration: 2233; Percent done: 55.8%; Mean loss: 3.0022\n",
            "Iteration: 2234; Percent done: 55.9%; Mean loss: 2.9415\n",
            "Iteration: 2235; Percent done: 55.9%; Mean loss: 3.0285\n",
            "Iteration: 2236; Percent done: 55.9%; Mean loss: 2.9617\n",
            "Iteration: 2237; Percent done: 55.9%; Mean loss: 2.9043\n",
            "Iteration: 2238; Percent done: 56.0%; Mean loss: 3.0829\n",
            "Iteration: 2239; Percent done: 56.0%; Mean loss: 2.9550\n",
            "Iteration: 2240; Percent done: 56.0%; Mean loss: 3.0514\n",
            "Iteration: 2241; Percent done: 56.0%; Mean loss: 3.2660\n",
            "Iteration: 2242; Percent done: 56.0%; Mean loss: 3.2273\n",
            "Iteration: 2243; Percent done: 56.1%; Mean loss: 3.1128\n",
            "Iteration: 2244; Percent done: 56.1%; Mean loss: 2.8750\n",
            "Iteration: 2245; Percent done: 56.1%; Mean loss: 3.2535\n",
            "Iteration: 2246; Percent done: 56.1%; Mean loss: 3.2640\n",
            "Iteration: 2247; Percent done: 56.2%; Mean loss: 3.2143\n",
            "Iteration: 2248; Percent done: 56.2%; Mean loss: 3.1066\n",
            "Iteration: 2249; Percent done: 56.2%; Mean loss: 3.0537\n",
            "Iteration: 2250; Percent done: 56.2%; Mean loss: 3.0663\n",
            "Iteration: 2251; Percent done: 56.3%; Mean loss: 3.0993\n",
            "Iteration: 2252; Percent done: 56.3%; Mean loss: 3.2000\n",
            "Iteration: 2253; Percent done: 56.3%; Mean loss: 3.0871\n",
            "Iteration: 2254; Percent done: 56.4%; Mean loss: 3.0489\n",
            "Iteration: 2255; Percent done: 56.4%; Mean loss: 3.2538\n",
            "Iteration: 2256; Percent done: 56.4%; Mean loss: 3.0675\n",
            "Iteration: 2257; Percent done: 56.4%; Mean loss: 3.4408\n",
            "Iteration: 2258; Percent done: 56.5%; Mean loss: 3.0932\n",
            "Iteration: 2259; Percent done: 56.5%; Mean loss: 2.8440\n",
            "Iteration: 2260; Percent done: 56.5%; Mean loss: 3.1785\n",
            "Iteration: 2261; Percent done: 56.5%; Mean loss: 3.0384\n",
            "Iteration: 2262; Percent done: 56.5%; Mean loss: 3.0786\n",
            "Iteration: 2263; Percent done: 56.6%; Mean loss: 2.8599\n",
            "Iteration: 2264; Percent done: 56.6%; Mean loss: 3.0407\n",
            "Iteration: 2265; Percent done: 56.6%; Mean loss: 3.0905\n",
            "Iteration: 2266; Percent done: 56.6%; Mean loss: 3.3058\n",
            "Iteration: 2267; Percent done: 56.7%; Mean loss: 3.1387\n",
            "Iteration: 2268; Percent done: 56.7%; Mean loss: 3.0824\n",
            "Iteration: 2269; Percent done: 56.7%; Mean loss: 3.2873\n",
            "Iteration: 2270; Percent done: 56.8%; Mean loss: 3.1163\n",
            "Iteration: 2271; Percent done: 56.8%; Mean loss: 3.2124\n",
            "Iteration: 2272; Percent done: 56.8%; Mean loss: 3.0339\n",
            "Iteration: 2273; Percent done: 56.8%; Mean loss: 3.1954\n",
            "Iteration: 2274; Percent done: 56.9%; Mean loss: 3.1335\n",
            "Iteration: 2275; Percent done: 56.9%; Mean loss: 3.2046\n",
            "Iteration: 2276; Percent done: 56.9%; Mean loss: 3.3710\n",
            "Iteration: 2277; Percent done: 56.9%; Mean loss: 2.9135\n",
            "Iteration: 2278; Percent done: 57.0%; Mean loss: 3.2045\n",
            "Iteration: 2279; Percent done: 57.0%; Mean loss: 3.1374\n",
            "Iteration: 2280; Percent done: 57.0%; Mean loss: 2.9874\n",
            "Iteration: 2281; Percent done: 57.0%; Mean loss: 3.1371\n",
            "Iteration: 2282; Percent done: 57.0%; Mean loss: 3.1201\n",
            "Iteration: 2283; Percent done: 57.1%; Mean loss: 3.2758\n",
            "Iteration: 2284; Percent done: 57.1%; Mean loss: 3.0276\n",
            "Iteration: 2285; Percent done: 57.1%; Mean loss: 3.2376\n",
            "Iteration: 2286; Percent done: 57.1%; Mean loss: 2.8447\n",
            "Iteration: 2287; Percent done: 57.2%; Mean loss: 3.2998\n",
            "Iteration: 2288; Percent done: 57.2%; Mean loss: 3.1186\n",
            "Iteration: 2289; Percent done: 57.2%; Mean loss: 3.4326\n",
            "Iteration: 2290; Percent done: 57.2%; Mean loss: 3.1309\n",
            "Iteration: 2291; Percent done: 57.3%; Mean loss: 3.1601\n",
            "Iteration: 2292; Percent done: 57.3%; Mean loss: 2.9069\n",
            "Iteration: 2293; Percent done: 57.3%; Mean loss: 3.0441\n",
            "Iteration: 2294; Percent done: 57.4%; Mean loss: 2.9218\n",
            "Iteration: 2295; Percent done: 57.4%; Mean loss: 3.1605\n",
            "Iteration: 2296; Percent done: 57.4%; Mean loss: 2.8278\n",
            "Iteration: 2297; Percent done: 57.4%; Mean loss: 2.8992\n",
            "Iteration: 2298; Percent done: 57.5%; Mean loss: 2.9592\n",
            "Iteration: 2299; Percent done: 57.5%; Mean loss: 2.9802\n",
            "Iteration: 2300; Percent done: 57.5%; Mean loss: 3.0346\n",
            "Iteration: 2301; Percent done: 57.5%; Mean loss: 3.0669\n",
            "Iteration: 2302; Percent done: 57.6%; Mean loss: 3.0940\n",
            "Iteration: 2303; Percent done: 57.6%; Mean loss: 2.9505\n",
            "Iteration: 2304; Percent done: 57.6%; Mean loss: 3.1118\n",
            "Iteration: 2305; Percent done: 57.6%; Mean loss: 2.9770\n",
            "Iteration: 2306; Percent done: 57.6%; Mean loss: 3.1787\n",
            "Iteration: 2307; Percent done: 57.7%; Mean loss: 2.9404\n",
            "Iteration: 2308; Percent done: 57.7%; Mean loss: 3.3875\n",
            "Iteration: 2309; Percent done: 57.7%; Mean loss: 3.1876\n",
            "Iteration: 2310; Percent done: 57.8%; Mean loss: 3.1371\n",
            "Iteration: 2311; Percent done: 57.8%; Mean loss: 3.2252\n",
            "Iteration: 2312; Percent done: 57.8%; Mean loss: 3.2131\n",
            "Iteration: 2313; Percent done: 57.8%; Mean loss: 3.0886\n",
            "Iteration: 2314; Percent done: 57.9%; Mean loss: 3.0974\n",
            "Iteration: 2315; Percent done: 57.9%; Mean loss: 3.3513\n",
            "Iteration: 2316; Percent done: 57.9%; Mean loss: 3.1948\n",
            "Iteration: 2317; Percent done: 57.9%; Mean loss: 3.3094\n",
            "Iteration: 2318; Percent done: 58.0%; Mean loss: 2.9438\n",
            "Iteration: 2319; Percent done: 58.0%; Mean loss: 2.9159\n",
            "Iteration: 2320; Percent done: 58.0%; Mean loss: 2.9988\n",
            "Iteration: 2321; Percent done: 58.0%; Mean loss: 3.1978\n",
            "Iteration: 2322; Percent done: 58.1%; Mean loss: 3.1637\n",
            "Iteration: 2323; Percent done: 58.1%; Mean loss: 3.1464\n",
            "Iteration: 2324; Percent done: 58.1%; Mean loss: 3.0044\n",
            "Iteration: 2325; Percent done: 58.1%; Mean loss: 3.2364\n",
            "Iteration: 2326; Percent done: 58.1%; Mean loss: 2.9110\n",
            "Iteration: 2327; Percent done: 58.2%; Mean loss: 2.9043\n",
            "Iteration: 2328; Percent done: 58.2%; Mean loss: 2.8056\n",
            "Iteration: 2329; Percent done: 58.2%; Mean loss: 3.2636\n",
            "Iteration: 2330; Percent done: 58.2%; Mean loss: 2.9057\n",
            "Iteration: 2331; Percent done: 58.3%; Mean loss: 3.0167\n",
            "Iteration: 2332; Percent done: 58.3%; Mean loss: 3.1205\n",
            "Iteration: 2333; Percent done: 58.3%; Mean loss: 3.2568\n",
            "Iteration: 2334; Percent done: 58.4%; Mean loss: 2.9577\n",
            "Iteration: 2335; Percent done: 58.4%; Mean loss: 3.0702\n",
            "Iteration: 2336; Percent done: 58.4%; Mean loss: 3.0694\n",
            "Iteration: 2337; Percent done: 58.4%; Mean loss: 3.0016\n",
            "Iteration: 2338; Percent done: 58.5%; Mean loss: 3.0337\n",
            "Iteration: 2339; Percent done: 58.5%; Mean loss: 3.0733\n",
            "Iteration: 2340; Percent done: 58.5%; Mean loss: 3.1205\n",
            "Iteration: 2341; Percent done: 58.5%; Mean loss: 3.2338\n",
            "Iteration: 2342; Percent done: 58.6%; Mean loss: 3.1439\n",
            "Iteration: 2343; Percent done: 58.6%; Mean loss: 3.1604\n",
            "Iteration: 2344; Percent done: 58.6%; Mean loss: 2.9970\n",
            "Iteration: 2345; Percent done: 58.6%; Mean loss: 3.1353\n",
            "Iteration: 2346; Percent done: 58.7%; Mean loss: 3.0823\n",
            "Iteration: 2347; Percent done: 58.7%; Mean loss: 2.8454\n",
            "Iteration: 2348; Percent done: 58.7%; Mean loss: 2.8964\n",
            "Iteration: 2349; Percent done: 58.7%; Mean loss: 3.2347\n",
            "Iteration: 2350; Percent done: 58.8%; Mean loss: 3.1271\n",
            "Iteration: 2351; Percent done: 58.8%; Mean loss: 3.2178\n",
            "Iteration: 2352; Percent done: 58.8%; Mean loss: 2.8520\n",
            "Iteration: 2353; Percent done: 58.8%; Mean loss: 3.1330\n",
            "Iteration: 2354; Percent done: 58.9%; Mean loss: 3.1396\n",
            "Iteration: 2355; Percent done: 58.9%; Mean loss: 3.1313\n",
            "Iteration: 2356; Percent done: 58.9%; Mean loss: 3.1247\n",
            "Iteration: 2357; Percent done: 58.9%; Mean loss: 3.0012\n",
            "Iteration: 2358; Percent done: 59.0%; Mean loss: 3.3522\n",
            "Iteration: 2359; Percent done: 59.0%; Mean loss: 2.8830\n",
            "Iteration: 2360; Percent done: 59.0%; Mean loss: 2.8211\n",
            "Iteration: 2361; Percent done: 59.0%; Mean loss: 3.1349\n",
            "Iteration: 2362; Percent done: 59.1%; Mean loss: 2.8844\n",
            "Iteration: 2363; Percent done: 59.1%; Mean loss: 3.0327\n",
            "Iteration: 2364; Percent done: 59.1%; Mean loss: 2.9697\n",
            "Iteration: 2365; Percent done: 59.1%; Mean loss: 2.9854\n",
            "Iteration: 2366; Percent done: 59.2%; Mean loss: 3.1342\n",
            "Iteration: 2367; Percent done: 59.2%; Mean loss: 2.9061\n",
            "Iteration: 2368; Percent done: 59.2%; Mean loss: 2.9930\n",
            "Iteration: 2369; Percent done: 59.2%; Mean loss: 3.1584\n",
            "Iteration: 2370; Percent done: 59.2%; Mean loss: 3.0951\n",
            "Iteration: 2371; Percent done: 59.3%; Mean loss: 3.0989\n",
            "Iteration: 2372; Percent done: 59.3%; Mean loss: 3.2012\n",
            "Iteration: 2373; Percent done: 59.3%; Mean loss: 3.0847\n",
            "Iteration: 2374; Percent done: 59.4%; Mean loss: 3.0695\n",
            "Iteration: 2375; Percent done: 59.4%; Mean loss: 3.0926\n",
            "Iteration: 2376; Percent done: 59.4%; Mean loss: 3.2511\n",
            "Iteration: 2377; Percent done: 59.4%; Mean loss: 3.0679\n",
            "Iteration: 2378; Percent done: 59.5%; Mean loss: 2.7754\n",
            "Iteration: 2379; Percent done: 59.5%; Mean loss: 3.2976\n",
            "Iteration: 2380; Percent done: 59.5%; Mean loss: 3.0823\n",
            "Iteration: 2381; Percent done: 59.5%; Mean loss: 3.0859\n",
            "Iteration: 2382; Percent done: 59.6%; Mean loss: 3.0901\n",
            "Iteration: 2383; Percent done: 59.6%; Mean loss: 3.3595\n",
            "Iteration: 2384; Percent done: 59.6%; Mean loss: 2.7919\n",
            "Iteration: 2385; Percent done: 59.6%; Mean loss: 3.1811\n",
            "Iteration: 2386; Percent done: 59.7%; Mean loss: 3.2557\n",
            "Iteration: 2387; Percent done: 59.7%; Mean loss: 2.9831\n",
            "Iteration: 2388; Percent done: 59.7%; Mean loss: 3.0339\n",
            "Iteration: 2389; Percent done: 59.7%; Mean loss: 2.8415\n",
            "Iteration: 2390; Percent done: 59.8%; Mean loss: 2.9581\n",
            "Iteration: 2391; Percent done: 59.8%; Mean loss: 2.8835\n",
            "Iteration: 2392; Percent done: 59.8%; Mean loss: 2.8528\n",
            "Iteration: 2393; Percent done: 59.8%; Mean loss: 3.2910\n",
            "Iteration: 2394; Percent done: 59.9%; Mean loss: 3.1530\n",
            "Iteration: 2395; Percent done: 59.9%; Mean loss: 2.9972\n",
            "Iteration: 2396; Percent done: 59.9%; Mean loss: 3.0484\n",
            "Iteration: 2397; Percent done: 59.9%; Mean loss: 2.9259\n",
            "Iteration: 2398; Percent done: 60.0%; Mean loss: 3.3997\n",
            "Iteration: 2399; Percent done: 60.0%; Mean loss: 3.1240\n",
            "Iteration: 2400; Percent done: 60.0%; Mean loss: 3.0441\n",
            "Iteration: 2401; Percent done: 60.0%; Mean loss: 3.2604\n",
            "Iteration: 2402; Percent done: 60.1%; Mean loss: 3.1383\n",
            "Iteration: 2403; Percent done: 60.1%; Mean loss: 3.1144\n",
            "Iteration: 2404; Percent done: 60.1%; Mean loss: 2.8174\n",
            "Iteration: 2405; Percent done: 60.1%; Mean loss: 2.9953\n",
            "Iteration: 2406; Percent done: 60.2%; Mean loss: 3.2556\n",
            "Iteration: 2407; Percent done: 60.2%; Mean loss: 2.9218\n",
            "Iteration: 2408; Percent done: 60.2%; Mean loss: 3.0367\n",
            "Iteration: 2409; Percent done: 60.2%; Mean loss: 3.0882\n",
            "Iteration: 2410; Percent done: 60.2%; Mean loss: 2.9389\n",
            "Iteration: 2411; Percent done: 60.3%; Mean loss: 3.0525\n",
            "Iteration: 2412; Percent done: 60.3%; Mean loss: 3.0309\n",
            "Iteration: 2413; Percent done: 60.3%; Mean loss: 3.0751\n",
            "Iteration: 2414; Percent done: 60.4%; Mean loss: 3.1169\n",
            "Iteration: 2415; Percent done: 60.4%; Mean loss: 3.1405\n",
            "Iteration: 2416; Percent done: 60.4%; Mean loss: 3.1106\n",
            "Iteration: 2417; Percent done: 60.4%; Mean loss: 3.0237\n",
            "Iteration: 2418; Percent done: 60.5%; Mean loss: 2.9592\n",
            "Iteration: 2419; Percent done: 60.5%; Mean loss: 3.2961\n",
            "Iteration: 2420; Percent done: 60.5%; Mean loss: 3.1656\n",
            "Iteration: 2421; Percent done: 60.5%; Mean loss: 2.9245\n",
            "Iteration: 2422; Percent done: 60.6%; Mean loss: 3.2051\n",
            "Iteration: 2423; Percent done: 60.6%; Mean loss: 3.1377\n",
            "Iteration: 2424; Percent done: 60.6%; Mean loss: 3.1896\n",
            "Iteration: 2425; Percent done: 60.6%; Mean loss: 3.0200\n",
            "Iteration: 2426; Percent done: 60.7%; Mean loss: 3.0927\n",
            "Iteration: 2427; Percent done: 60.7%; Mean loss: 3.4323\n",
            "Iteration: 2428; Percent done: 60.7%; Mean loss: 2.9498\n",
            "Iteration: 2429; Percent done: 60.7%; Mean loss: 3.2462\n",
            "Iteration: 2430; Percent done: 60.8%; Mean loss: 3.2577\n",
            "Iteration: 2431; Percent done: 60.8%; Mean loss: 3.1397\n",
            "Iteration: 2432; Percent done: 60.8%; Mean loss: 3.0293\n",
            "Iteration: 2433; Percent done: 60.8%; Mean loss: 2.9085\n",
            "Iteration: 2434; Percent done: 60.9%; Mean loss: 3.2616\n",
            "Iteration: 2435; Percent done: 60.9%; Mean loss: 2.9273\n",
            "Iteration: 2436; Percent done: 60.9%; Mean loss: 2.7895\n",
            "Iteration: 2437; Percent done: 60.9%; Mean loss: 3.0236\n",
            "Iteration: 2438; Percent done: 61.0%; Mean loss: 2.9007\n",
            "Iteration: 2439; Percent done: 61.0%; Mean loss: 3.1884\n",
            "Iteration: 2440; Percent done: 61.0%; Mean loss: 3.2298\n",
            "Iteration: 2441; Percent done: 61.0%; Mean loss: 3.2295\n",
            "Iteration: 2442; Percent done: 61.1%; Mean loss: 2.8894\n",
            "Iteration: 2443; Percent done: 61.1%; Mean loss: 2.9389\n",
            "Iteration: 2444; Percent done: 61.1%; Mean loss: 2.8308\n",
            "Iteration: 2445; Percent done: 61.1%; Mean loss: 2.8644\n",
            "Iteration: 2446; Percent done: 61.2%; Mean loss: 3.1718\n",
            "Iteration: 2447; Percent done: 61.2%; Mean loss: 3.0202\n",
            "Iteration: 2448; Percent done: 61.2%; Mean loss: 3.1411\n",
            "Iteration: 2449; Percent done: 61.2%; Mean loss: 2.8684\n",
            "Iteration: 2450; Percent done: 61.3%; Mean loss: 3.0576\n",
            "Iteration: 2451; Percent done: 61.3%; Mean loss: 3.0054\n",
            "Iteration: 2452; Percent done: 61.3%; Mean loss: 2.9661\n",
            "Iteration: 2453; Percent done: 61.3%; Mean loss: 3.0520\n",
            "Iteration: 2454; Percent done: 61.4%; Mean loss: 3.3537\n",
            "Iteration: 2455; Percent done: 61.4%; Mean loss: 3.2840\n",
            "Iteration: 2456; Percent done: 61.4%; Mean loss: 3.0003\n",
            "Iteration: 2457; Percent done: 61.4%; Mean loss: 3.0003\n",
            "Iteration: 2458; Percent done: 61.5%; Mean loss: 3.0814\n",
            "Iteration: 2459; Percent done: 61.5%; Mean loss: 2.9625\n",
            "Iteration: 2460; Percent done: 61.5%; Mean loss: 2.8509\n",
            "Iteration: 2461; Percent done: 61.5%; Mean loss: 2.8068\n",
            "Iteration: 2462; Percent done: 61.6%; Mean loss: 3.2830\n",
            "Iteration: 2463; Percent done: 61.6%; Mean loss: 3.1837\n",
            "Iteration: 2464; Percent done: 61.6%; Mean loss: 3.0708\n",
            "Iteration: 2465; Percent done: 61.6%; Mean loss: 2.9691\n",
            "Iteration: 2466; Percent done: 61.7%; Mean loss: 3.1576\n",
            "Iteration: 2467; Percent done: 61.7%; Mean loss: 3.1053\n",
            "Iteration: 2468; Percent done: 61.7%; Mean loss: 2.9676\n",
            "Iteration: 2469; Percent done: 61.7%; Mean loss: 2.6860\n",
            "Iteration: 2470; Percent done: 61.8%; Mean loss: 2.9859\n",
            "Iteration: 2471; Percent done: 61.8%; Mean loss: 3.1411\n",
            "Iteration: 2472; Percent done: 61.8%; Mean loss: 3.0891\n",
            "Iteration: 2473; Percent done: 61.8%; Mean loss: 3.0256\n",
            "Iteration: 2474; Percent done: 61.9%; Mean loss: 3.1263\n",
            "Iteration: 2475; Percent done: 61.9%; Mean loss: 3.0545\n",
            "Iteration: 2476; Percent done: 61.9%; Mean loss: 2.9305\n",
            "Iteration: 2477; Percent done: 61.9%; Mean loss: 3.1455\n",
            "Iteration: 2478; Percent done: 62.0%; Mean loss: 2.9457\n",
            "Iteration: 2479; Percent done: 62.0%; Mean loss: 3.2601\n",
            "Iteration: 2480; Percent done: 62.0%; Mean loss: 3.1200\n",
            "Iteration: 2481; Percent done: 62.0%; Mean loss: 3.1552\n",
            "Iteration: 2482; Percent done: 62.1%; Mean loss: 3.0006\n",
            "Iteration: 2483; Percent done: 62.1%; Mean loss: 2.9738\n",
            "Iteration: 2484; Percent done: 62.1%; Mean loss: 3.1889\n",
            "Iteration: 2485; Percent done: 62.1%; Mean loss: 3.1461\n",
            "Iteration: 2486; Percent done: 62.2%; Mean loss: 3.1074\n",
            "Iteration: 2487; Percent done: 62.2%; Mean loss: 3.2657\n",
            "Iteration: 2488; Percent done: 62.2%; Mean loss: 3.1830\n",
            "Iteration: 2489; Percent done: 62.2%; Mean loss: 3.0190\n",
            "Iteration: 2490; Percent done: 62.3%; Mean loss: 3.1812\n",
            "Iteration: 2491; Percent done: 62.3%; Mean loss: 2.8337\n",
            "Iteration: 2492; Percent done: 62.3%; Mean loss: 3.1119\n",
            "Iteration: 2493; Percent done: 62.3%; Mean loss: 2.9277\n",
            "Iteration: 2494; Percent done: 62.4%; Mean loss: 2.8057\n",
            "Iteration: 2495; Percent done: 62.4%; Mean loss: 3.0657\n",
            "Iteration: 2496; Percent done: 62.4%; Mean loss: 2.8912\n",
            "Iteration: 2497; Percent done: 62.4%; Mean loss: 3.0921\n",
            "Iteration: 2498; Percent done: 62.5%; Mean loss: 3.0957\n",
            "Iteration: 2499; Percent done: 62.5%; Mean loss: 2.9848\n",
            "Iteration: 2500; Percent done: 62.5%; Mean loss: 3.1941\n",
            "Iteration: 2501; Percent done: 62.5%; Mean loss: 3.0999\n",
            "Iteration: 2502; Percent done: 62.5%; Mean loss: 2.8137\n",
            "Iteration: 2503; Percent done: 62.6%; Mean loss: 3.0222\n",
            "Iteration: 2504; Percent done: 62.6%; Mean loss: 3.2577\n",
            "Iteration: 2505; Percent done: 62.6%; Mean loss: 2.9382\n",
            "Iteration: 2506; Percent done: 62.6%; Mean loss: 2.9062\n",
            "Iteration: 2507; Percent done: 62.7%; Mean loss: 2.6802\n",
            "Iteration: 2508; Percent done: 62.7%; Mean loss: 3.2167\n",
            "Iteration: 2509; Percent done: 62.7%; Mean loss: 3.2018\n",
            "Iteration: 2510; Percent done: 62.7%; Mean loss: 2.9125\n",
            "Iteration: 2511; Percent done: 62.8%; Mean loss: 2.9179\n",
            "Iteration: 2512; Percent done: 62.8%; Mean loss: 3.0321\n",
            "Iteration: 2513; Percent done: 62.8%; Mean loss: 3.0262\n",
            "Iteration: 2514; Percent done: 62.8%; Mean loss: 2.9131\n",
            "Iteration: 2515; Percent done: 62.9%; Mean loss: 3.1622\n",
            "Iteration: 2516; Percent done: 62.9%; Mean loss: 3.1260\n",
            "Iteration: 2517; Percent done: 62.9%; Mean loss: 3.0463\n",
            "Iteration: 2518; Percent done: 62.9%; Mean loss: 2.8251\n",
            "Iteration: 2519; Percent done: 63.0%; Mean loss: 3.0087\n",
            "Iteration: 2520; Percent done: 63.0%; Mean loss: 3.0258\n",
            "Iteration: 2521; Percent done: 63.0%; Mean loss: 3.2567\n",
            "Iteration: 2522; Percent done: 63.0%; Mean loss: 2.9775\n",
            "Iteration: 2523; Percent done: 63.1%; Mean loss: 3.1340\n",
            "Iteration: 2524; Percent done: 63.1%; Mean loss: 2.9616\n",
            "Iteration: 2525; Percent done: 63.1%; Mean loss: 3.0828\n",
            "Iteration: 2526; Percent done: 63.1%; Mean loss: 2.7544\n",
            "Iteration: 2527; Percent done: 63.2%; Mean loss: 3.1162\n",
            "Iteration: 2528; Percent done: 63.2%; Mean loss: 2.9638\n",
            "Iteration: 2529; Percent done: 63.2%; Mean loss: 2.8447\n",
            "Iteration: 2530; Percent done: 63.2%; Mean loss: 3.1090\n",
            "Iteration: 2531; Percent done: 63.3%; Mean loss: 2.9992\n",
            "Iteration: 2532; Percent done: 63.3%; Mean loss: 2.8400\n",
            "Iteration: 2533; Percent done: 63.3%; Mean loss: 2.8503\n",
            "Iteration: 2534; Percent done: 63.3%; Mean loss: 2.9460\n",
            "Iteration: 2535; Percent done: 63.4%; Mean loss: 3.1065\n",
            "Iteration: 2536; Percent done: 63.4%; Mean loss: 3.0502\n",
            "Iteration: 2537; Percent done: 63.4%; Mean loss: 2.7498\n",
            "Iteration: 2538; Percent done: 63.4%; Mean loss: 3.0984\n",
            "Iteration: 2539; Percent done: 63.5%; Mean loss: 2.9987\n",
            "Iteration: 2540; Percent done: 63.5%; Mean loss: 3.0199\n",
            "Iteration: 2541; Percent done: 63.5%; Mean loss: 3.1362\n",
            "Iteration: 2542; Percent done: 63.5%; Mean loss: 2.9924\n",
            "Iteration: 2543; Percent done: 63.6%; Mean loss: 2.8290\n",
            "Iteration: 2544; Percent done: 63.6%; Mean loss: 3.3229\n",
            "Iteration: 2545; Percent done: 63.6%; Mean loss: 2.9251\n",
            "Iteration: 2546; Percent done: 63.6%; Mean loss: 2.8833\n",
            "Iteration: 2547; Percent done: 63.7%; Mean loss: 2.9096\n",
            "Iteration: 2548; Percent done: 63.7%; Mean loss: 3.1271\n",
            "Iteration: 2549; Percent done: 63.7%; Mean loss: 2.6061\n",
            "Iteration: 2550; Percent done: 63.7%; Mean loss: 2.8899\n",
            "Iteration: 2551; Percent done: 63.8%; Mean loss: 3.1900\n",
            "Iteration: 2552; Percent done: 63.8%; Mean loss: 2.9702\n",
            "Iteration: 2553; Percent done: 63.8%; Mean loss: 2.8576\n",
            "Iteration: 2554; Percent done: 63.8%; Mean loss: 2.9405\n",
            "Iteration: 2555; Percent done: 63.9%; Mean loss: 2.8308\n",
            "Iteration: 2556; Percent done: 63.9%; Mean loss: 3.0151\n",
            "Iteration: 2557; Percent done: 63.9%; Mean loss: 2.8067\n",
            "Iteration: 2558; Percent done: 63.9%; Mean loss: 3.1417\n",
            "Iteration: 2559; Percent done: 64.0%; Mean loss: 3.0169\n",
            "Iteration: 2560; Percent done: 64.0%; Mean loss: 2.8716\n",
            "Iteration: 2561; Percent done: 64.0%; Mean loss: 2.8932\n",
            "Iteration: 2562; Percent done: 64.0%; Mean loss: 3.1523\n",
            "Iteration: 2563; Percent done: 64.1%; Mean loss: 2.9205\n",
            "Iteration: 2564; Percent done: 64.1%; Mean loss: 3.2569\n",
            "Iteration: 2565; Percent done: 64.1%; Mean loss: 3.0678\n",
            "Iteration: 2566; Percent done: 64.1%; Mean loss: 3.1361\n",
            "Iteration: 2567; Percent done: 64.2%; Mean loss: 3.0344\n",
            "Iteration: 2568; Percent done: 64.2%; Mean loss: 3.3993\n",
            "Iteration: 2569; Percent done: 64.2%; Mean loss: 2.9394\n",
            "Iteration: 2570; Percent done: 64.2%; Mean loss: 2.7996\n",
            "Iteration: 2571; Percent done: 64.3%; Mean loss: 3.1390\n",
            "Iteration: 2572; Percent done: 64.3%; Mean loss: 3.1769\n",
            "Iteration: 2573; Percent done: 64.3%; Mean loss: 3.0796\n",
            "Iteration: 2574; Percent done: 64.3%; Mean loss: 3.1509\n",
            "Iteration: 2575; Percent done: 64.4%; Mean loss: 2.9896\n",
            "Iteration: 2576; Percent done: 64.4%; Mean loss: 3.0705\n",
            "Iteration: 2577; Percent done: 64.4%; Mean loss: 2.9600\n",
            "Iteration: 2578; Percent done: 64.5%; Mean loss: 3.1038\n",
            "Iteration: 2579; Percent done: 64.5%; Mean loss: 2.8129\n",
            "Iteration: 2580; Percent done: 64.5%; Mean loss: 3.3790\n",
            "Iteration: 2581; Percent done: 64.5%; Mean loss: 2.9753\n",
            "Iteration: 2582; Percent done: 64.5%; Mean loss: 3.1299\n",
            "Iteration: 2583; Percent done: 64.6%; Mean loss: 3.0854\n",
            "Iteration: 2584; Percent done: 64.6%; Mean loss: 3.1628\n",
            "Iteration: 2585; Percent done: 64.6%; Mean loss: 3.1900\n",
            "Iteration: 2586; Percent done: 64.6%; Mean loss: 3.3173\n",
            "Iteration: 2587; Percent done: 64.7%; Mean loss: 3.2268\n",
            "Iteration: 2588; Percent done: 64.7%; Mean loss: 2.9653\n",
            "Iteration: 2589; Percent done: 64.7%; Mean loss: 2.9563\n",
            "Iteration: 2590; Percent done: 64.8%; Mean loss: 2.9396\n",
            "Iteration: 2591; Percent done: 64.8%; Mean loss: 3.0772\n",
            "Iteration: 2592; Percent done: 64.8%; Mean loss: 3.1293\n",
            "Iteration: 2593; Percent done: 64.8%; Mean loss: 3.0569\n",
            "Iteration: 2594; Percent done: 64.8%; Mean loss: 3.0287\n",
            "Iteration: 2595; Percent done: 64.9%; Mean loss: 3.1140\n",
            "Iteration: 2596; Percent done: 64.9%; Mean loss: 2.6928\n",
            "Iteration: 2597; Percent done: 64.9%; Mean loss: 2.8070\n",
            "Iteration: 2598; Percent done: 65.0%; Mean loss: 3.0705\n",
            "Iteration: 2599; Percent done: 65.0%; Mean loss: 2.8020\n",
            "Iteration: 2600; Percent done: 65.0%; Mean loss: 3.0508\n",
            "Iteration: 2601; Percent done: 65.0%; Mean loss: 3.0862\n",
            "Iteration: 2602; Percent done: 65.0%; Mean loss: 2.9419\n",
            "Iteration: 2603; Percent done: 65.1%; Mean loss: 2.8677\n",
            "Iteration: 2604; Percent done: 65.1%; Mean loss: 2.8467\n",
            "Iteration: 2605; Percent done: 65.1%; Mean loss: 2.9654\n",
            "Iteration: 2606; Percent done: 65.1%; Mean loss: 3.0364\n",
            "Iteration: 2607; Percent done: 65.2%; Mean loss: 2.9154\n",
            "Iteration: 2608; Percent done: 65.2%; Mean loss: 2.9153\n",
            "Iteration: 2609; Percent done: 65.2%; Mean loss: 3.0133\n",
            "Iteration: 2610; Percent done: 65.2%; Mean loss: 2.9806\n",
            "Iteration: 2611; Percent done: 65.3%; Mean loss: 2.9002\n",
            "Iteration: 2612; Percent done: 65.3%; Mean loss: 2.6765\n",
            "Iteration: 2613; Percent done: 65.3%; Mean loss: 2.9870\n",
            "Iteration: 2614; Percent done: 65.3%; Mean loss: 3.0730\n",
            "Iteration: 2615; Percent done: 65.4%; Mean loss: 2.8303\n",
            "Iteration: 2616; Percent done: 65.4%; Mean loss: 3.0957\n",
            "Iteration: 2617; Percent done: 65.4%; Mean loss: 2.7974\n",
            "Iteration: 2618; Percent done: 65.5%; Mean loss: 2.7690\n",
            "Iteration: 2619; Percent done: 65.5%; Mean loss: 2.8861\n",
            "Iteration: 2620; Percent done: 65.5%; Mean loss: 2.9377\n",
            "Iteration: 2621; Percent done: 65.5%; Mean loss: 3.0289\n",
            "Iteration: 2622; Percent done: 65.5%; Mean loss: 3.0566\n",
            "Iteration: 2623; Percent done: 65.6%; Mean loss: 3.0636\n",
            "Iteration: 2624; Percent done: 65.6%; Mean loss: 2.9086\n",
            "Iteration: 2625; Percent done: 65.6%; Mean loss: 2.9736\n",
            "Iteration: 2626; Percent done: 65.6%; Mean loss: 3.1800\n",
            "Iteration: 2627; Percent done: 65.7%; Mean loss: 2.9630\n",
            "Iteration: 2628; Percent done: 65.7%; Mean loss: 2.7295\n",
            "Iteration: 2629; Percent done: 65.7%; Mean loss: 2.7735\n",
            "Iteration: 2630; Percent done: 65.8%; Mean loss: 3.0121\n",
            "Iteration: 2631; Percent done: 65.8%; Mean loss: 2.8943\n",
            "Iteration: 2632; Percent done: 65.8%; Mean loss: 2.7902\n",
            "Iteration: 2633; Percent done: 65.8%; Mean loss: 3.1010\n",
            "Iteration: 2634; Percent done: 65.8%; Mean loss: 3.0729\n",
            "Iteration: 2635; Percent done: 65.9%; Mean loss: 3.0549\n",
            "Iteration: 2636; Percent done: 65.9%; Mean loss: 3.0340\n",
            "Iteration: 2637; Percent done: 65.9%; Mean loss: 2.9655\n",
            "Iteration: 2638; Percent done: 66.0%; Mean loss: 3.2291\n",
            "Iteration: 2639; Percent done: 66.0%; Mean loss: 3.1511\n",
            "Iteration: 2640; Percent done: 66.0%; Mean loss: 2.9350\n",
            "Iteration: 2641; Percent done: 66.0%; Mean loss: 3.0276\n",
            "Iteration: 2642; Percent done: 66.0%; Mean loss: 3.0094\n",
            "Iteration: 2643; Percent done: 66.1%; Mean loss: 3.0369\n",
            "Iteration: 2644; Percent done: 66.1%; Mean loss: 3.0365\n",
            "Iteration: 2645; Percent done: 66.1%; Mean loss: 3.0717\n",
            "Iteration: 2646; Percent done: 66.1%; Mean loss: 3.0431\n",
            "Iteration: 2647; Percent done: 66.2%; Mean loss: 2.8223\n",
            "Iteration: 2648; Percent done: 66.2%; Mean loss: 2.8679\n",
            "Iteration: 2649; Percent done: 66.2%; Mean loss: 2.9222\n",
            "Iteration: 2650; Percent done: 66.2%; Mean loss: 2.8949\n",
            "Iteration: 2651; Percent done: 66.3%; Mean loss: 3.2206\n",
            "Iteration: 2652; Percent done: 66.3%; Mean loss: 2.7644\n",
            "Iteration: 2653; Percent done: 66.3%; Mean loss: 2.7260\n",
            "Iteration: 2654; Percent done: 66.3%; Mean loss: 2.8362\n",
            "Iteration: 2655; Percent done: 66.4%; Mean loss: 3.0372\n",
            "Iteration: 2656; Percent done: 66.4%; Mean loss: 3.1776\n",
            "Iteration: 2657; Percent done: 66.4%; Mean loss: 3.1889\n",
            "Iteration: 2658; Percent done: 66.5%; Mean loss: 2.7770\n",
            "Iteration: 2659; Percent done: 66.5%; Mean loss: 3.1442\n",
            "Iteration: 2660; Percent done: 66.5%; Mean loss: 3.0375\n",
            "Iteration: 2661; Percent done: 66.5%; Mean loss: 3.0933\n",
            "Iteration: 2662; Percent done: 66.5%; Mean loss: 2.8319\n",
            "Iteration: 2663; Percent done: 66.6%; Mean loss: 3.1228\n",
            "Iteration: 2664; Percent done: 66.6%; Mean loss: 3.2172\n",
            "Iteration: 2665; Percent done: 66.6%; Mean loss: 3.0771\n",
            "Iteration: 2666; Percent done: 66.6%; Mean loss: 3.0943\n",
            "Iteration: 2667; Percent done: 66.7%; Mean loss: 3.0701\n",
            "Iteration: 2668; Percent done: 66.7%; Mean loss: 3.0962\n",
            "Iteration: 2669; Percent done: 66.7%; Mean loss: 3.0622\n",
            "Iteration: 2670; Percent done: 66.8%; Mean loss: 2.8993\n",
            "Iteration: 2671; Percent done: 66.8%; Mean loss: 3.0500\n",
            "Iteration: 2672; Percent done: 66.8%; Mean loss: 2.8398\n",
            "Iteration: 2673; Percent done: 66.8%; Mean loss: 2.9179\n",
            "Iteration: 2674; Percent done: 66.8%; Mean loss: 2.8809\n",
            "Iteration: 2675; Percent done: 66.9%; Mean loss: 2.9406\n",
            "Iteration: 2676; Percent done: 66.9%; Mean loss: 2.7472\n",
            "Iteration: 2677; Percent done: 66.9%; Mean loss: 3.4247\n",
            "Iteration: 2678; Percent done: 67.0%; Mean loss: 2.8258\n",
            "Iteration: 2679; Percent done: 67.0%; Mean loss: 2.8758\n",
            "Iteration: 2680; Percent done: 67.0%; Mean loss: 3.2169\n",
            "Iteration: 2681; Percent done: 67.0%; Mean loss: 3.0337\n",
            "Iteration: 2682; Percent done: 67.0%; Mean loss: 2.9630\n",
            "Iteration: 2683; Percent done: 67.1%; Mean loss: 3.1875\n",
            "Iteration: 2684; Percent done: 67.1%; Mean loss: 3.1823\n",
            "Iteration: 2685; Percent done: 67.1%; Mean loss: 3.0554\n",
            "Iteration: 2686; Percent done: 67.2%; Mean loss: 2.7535\n",
            "Iteration: 2687; Percent done: 67.2%; Mean loss: 3.1149\n",
            "Iteration: 2688; Percent done: 67.2%; Mean loss: 3.0820\n",
            "Iteration: 2689; Percent done: 67.2%; Mean loss: 3.0039\n",
            "Iteration: 2690; Percent done: 67.2%; Mean loss: 2.8247\n",
            "Iteration: 2691; Percent done: 67.3%; Mean loss: 3.2178\n",
            "Iteration: 2692; Percent done: 67.3%; Mean loss: 3.1265\n",
            "Iteration: 2693; Percent done: 67.3%; Mean loss: 2.7730\n",
            "Iteration: 2694; Percent done: 67.3%; Mean loss: 2.8867\n",
            "Iteration: 2695; Percent done: 67.4%; Mean loss: 2.8881\n",
            "Iteration: 2696; Percent done: 67.4%; Mean loss: 3.0634\n",
            "Iteration: 2697; Percent done: 67.4%; Mean loss: 2.9847\n",
            "Iteration: 2698; Percent done: 67.5%; Mean loss: 2.8039\n",
            "Iteration: 2699; Percent done: 67.5%; Mean loss: 2.9322\n",
            "Iteration: 2700; Percent done: 67.5%; Mean loss: 2.9504\n",
            "Iteration: 2701; Percent done: 67.5%; Mean loss: 2.9680\n",
            "Iteration: 2702; Percent done: 67.5%; Mean loss: 3.0161\n",
            "Iteration: 2703; Percent done: 67.6%; Mean loss: 3.0127\n",
            "Iteration: 2704; Percent done: 67.6%; Mean loss: 2.9780\n",
            "Iteration: 2705; Percent done: 67.6%; Mean loss: 2.9587\n",
            "Iteration: 2706; Percent done: 67.7%; Mean loss: 2.9796\n",
            "Iteration: 2707; Percent done: 67.7%; Mean loss: 3.3108\n",
            "Iteration: 2708; Percent done: 67.7%; Mean loss: 3.2011\n",
            "Iteration: 2709; Percent done: 67.7%; Mean loss: 2.9047\n",
            "Iteration: 2710; Percent done: 67.8%; Mean loss: 2.8723\n",
            "Iteration: 2711; Percent done: 67.8%; Mean loss: 2.8717\n",
            "Iteration: 2712; Percent done: 67.8%; Mean loss: 2.6694\n",
            "Iteration: 2713; Percent done: 67.8%; Mean loss: 3.2508\n",
            "Iteration: 2714; Percent done: 67.8%; Mean loss: 2.9674\n",
            "Iteration: 2715; Percent done: 67.9%; Mean loss: 2.7394\n",
            "Iteration: 2716; Percent done: 67.9%; Mean loss: 2.7920\n",
            "Iteration: 2717; Percent done: 67.9%; Mean loss: 3.2871\n",
            "Iteration: 2718; Percent done: 68.0%; Mean loss: 2.7410\n",
            "Iteration: 2719; Percent done: 68.0%; Mean loss: 3.0072\n",
            "Iteration: 2720; Percent done: 68.0%; Mean loss: 2.9045\n",
            "Iteration: 2721; Percent done: 68.0%; Mean loss: 2.8860\n",
            "Iteration: 2722; Percent done: 68.0%; Mean loss: 2.8143\n",
            "Iteration: 2723; Percent done: 68.1%; Mean loss: 2.7090\n",
            "Iteration: 2724; Percent done: 68.1%; Mean loss: 2.8881\n",
            "Iteration: 2725; Percent done: 68.1%; Mean loss: 3.1518\n",
            "Iteration: 2726; Percent done: 68.2%; Mean loss: 2.9605\n",
            "Iteration: 2727; Percent done: 68.2%; Mean loss: 2.9697\n",
            "Iteration: 2728; Percent done: 68.2%; Mean loss: 3.0381\n",
            "Iteration: 2729; Percent done: 68.2%; Mean loss: 2.9214\n",
            "Iteration: 2730; Percent done: 68.2%; Mean loss: 2.9209\n",
            "Iteration: 2731; Percent done: 68.3%; Mean loss: 3.0603\n",
            "Iteration: 2732; Percent done: 68.3%; Mean loss: 2.8931\n",
            "Iteration: 2733; Percent done: 68.3%; Mean loss: 2.7828\n",
            "Iteration: 2734; Percent done: 68.3%; Mean loss: 2.8457\n",
            "Iteration: 2735; Percent done: 68.4%; Mean loss: 2.7710\n",
            "Iteration: 2736; Percent done: 68.4%; Mean loss: 2.9450\n",
            "Iteration: 2737; Percent done: 68.4%; Mean loss: 3.1708\n",
            "Iteration: 2738; Percent done: 68.5%; Mean loss: 2.8818\n",
            "Iteration: 2739; Percent done: 68.5%; Mean loss: 3.0273\n",
            "Iteration: 2740; Percent done: 68.5%; Mean loss: 3.0741\n",
            "Iteration: 2741; Percent done: 68.5%; Mean loss: 2.9644\n",
            "Iteration: 2742; Percent done: 68.5%; Mean loss: 3.1347\n",
            "Iteration: 2743; Percent done: 68.6%; Mean loss: 2.9381\n",
            "Iteration: 2744; Percent done: 68.6%; Mean loss: 2.7629\n",
            "Iteration: 2745; Percent done: 68.6%; Mean loss: 2.9403\n",
            "Iteration: 2746; Percent done: 68.7%; Mean loss: 3.1152\n",
            "Iteration: 2747; Percent done: 68.7%; Mean loss: 2.9513\n",
            "Iteration: 2748; Percent done: 68.7%; Mean loss: 3.0895\n",
            "Iteration: 2749; Percent done: 68.7%; Mean loss: 2.8758\n",
            "Iteration: 2750; Percent done: 68.8%; Mean loss: 2.7196\n",
            "Iteration: 2751; Percent done: 68.8%; Mean loss: 3.1568\n",
            "Iteration: 2752; Percent done: 68.8%; Mean loss: 2.9472\n",
            "Iteration: 2753; Percent done: 68.8%; Mean loss: 3.0804\n",
            "Iteration: 2754; Percent done: 68.8%; Mean loss: 2.9427\n",
            "Iteration: 2755; Percent done: 68.9%; Mean loss: 2.9777\n",
            "Iteration: 2756; Percent done: 68.9%; Mean loss: 3.0230\n",
            "Iteration: 2757; Percent done: 68.9%; Mean loss: 2.8232\n",
            "Iteration: 2758; Percent done: 69.0%; Mean loss: 3.0478\n",
            "Iteration: 2759; Percent done: 69.0%; Mean loss: 3.1043\n",
            "Iteration: 2760; Percent done: 69.0%; Mean loss: 2.7369\n",
            "Iteration: 2761; Percent done: 69.0%; Mean loss: 2.7987\n",
            "Iteration: 2762; Percent done: 69.0%; Mean loss: 2.8560\n",
            "Iteration: 2763; Percent done: 69.1%; Mean loss: 3.0448\n",
            "Iteration: 2764; Percent done: 69.1%; Mean loss: 2.9828\n",
            "Iteration: 2765; Percent done: 69.1%; Mean loss: 2.8777\n",
            "Iteration: 2766; Percent done: 69.2%; Mean loss: 3.0843\n",
            "Iteration: 2767; Percent done: 69.2%; Mean loss: 2.9706\n",
            "Iteration: 2768; Percent done: 69.2%; Mean loss: 2.8151\n",
            "Iteration: 2769; Percent done: 69.2%; Mean loss: 2.9259\n",
            "Iteration: 2770; Percent done: 69.2%; Mean loss: 3.1085\n",
            "Iteration: 2771; Percent done: 69.3%; Mean loss: 3.0581\n",
            "Iteration: 2772; Percent done: 69.3%; Mean loss: 2.9922\n",
            "Iteration: 2773; Percent done: 69.3%; Mean loss: 2.8292\n",
            "Iteration: 2774; Percent done: 69.3%; Mean loss: 2.7432\n",
            "Iteration: 2775; Percent done: 69.4%; Mean loss: 2.9799\n",
            "Iteration: 2776; Percent done: 69.4%; Mean loss: 2.7693\n",
            "Iteration: 2777; Percent done: 69.4%; Mean loss: 3.0992\n",
            "Iteration: 2778; Percent done: 69.5%; Mean loss: 3.0151\n",
            "Iteration: 2779; Percent done: 69.5%; Mean loss: 2.6431\n",
            "Iteration: 2780; Percent done: 69.5%; Mean loss: 3.2729\n",
            "Iteration: 2781; Percent done: 69.5%; Mean loss: 2.7551\n",
            "Iteration: 2782; Percent done: 69.5%; Mean loss: 3.1504\n",
            "Iteration: 2783; Percent done: 69.6%; Mean loss: 2.8647\n",
            "Iteration: 2784; Percent done: 69.6%; Mean loss: 3.2376\n",
            "Iteration: 2785; Percent done: 69.6%; Mean loss: 3.1761\n",
            "Iteration: 2786; Percent done: 69.7%; Mean loss: 3.0030\n",
            "Iteration: 2787; Percent done: 69.7%; Mean loss: 3.1281\n",
            "Iteration: 2788; Percent done: 69.7%; Mean loss: 2.7189\n",
            "Iteration: 2789; Percent done: 69.7%; Mean loss: 3.2439\n",
            "Iteration: 2790; Percent done: 69.8%; Mean loss: 3.0162\n",
            "Iteration: 2791; Percent done: 69.8%; Mean loss: 2.8080\n",
            "Iteration: 2792; Percent done: 69.8%; Mean loss: 3.0298\n",
            "Iteration: 2793; Percent done: 69.8%; Mean loss: 2.8997\n",
            "Iteration: 2794; Percent done: 69.8%; Mean loss: 3.1961\n",
            "Iteration: 2795; Percent done: 69.9%; Mean loss: 2.9940\n",
            "Iteration: 2796; Percent done: 69.9%; Mean loss: 3.0099\n",
            "Iteration: 2797; Percent done: 69.9%; Mean loss: 3.0459\n",
            "Iteration: 2798; Percent done: 70.0%; Mean loss: 2.9762\n",
            "Iteration: 2799; Percent done: 70.0%; Mean loss: 2.8787\n",
            "Iteration: 2800; Percent done: 70.0%; Mean loss: 3.1212\n",
            "Iteration: 2801; Percent done: 70.0%; Mean loss: 2.7930\n",
            "Iteration: 2802; Percent done: 70.0%; Mean loss: 3.0208\n",
            "Iteration: 2803; Percent done: 70.1%; Mean loss: 3.0835\n",
            "Iteration: 2804; Percent done: 70.1%; Mean loss: 3.0599\n",
            "Iteration: 2805; Percent done: 70.1%; Mean loss: 2.8989\n",
            "Iteration: 2806; Percent done: 70.2%; Mean loss: 3.0874\n",
            "Iteration: 2807; Percent done: 70.2%; Mean loss: 3.1810\n",
            "Iteration: 2808; Percent done: 70.2%; Mean loss: 2.8989\n",
            "Iteration: 2809; Percent done: 70.2%; Mean loss: 3.0525\n",
            "Iteration: 2810; Percent done: 70.2%; Mean loss: 3.0373\n",
            "Iteration: 2811; Percent done: 70.3%; Mean loss: 3.0767\n",
            "Iteration: 2812; Percent done: 70.3%; Mean loss: 2.9890\n",
            "Iteration: 2813; Percent done: 70.3%; Mean loss: 2.6431\n",
            "Iteration: 2814; Percent done: 70.3%; Mean loss: 2.7807\n",
            "Iteration: 2815; Percent done: 70.4%; Mean loss: 3.0590\n",
            "Iteration: 2816; Percent done: 70.4%; Mean loss: 2.8637\n",
            "Iteration: 2817; Percent done: 70.4%; Mean loss: 2.9901\n",
            "Iteration: 2818; Percent done: 70.5%; Mean loss: 3.0073\n",
            "Iteration: 2819; Percent done: 70.5%; Mean loss: 2.9559\n",
            "Iteration: 2820; Percent done: 70.5%; Mean loss: 2.9379\n",
            "Iteration: 2821; Percent done: 70.5%; Mean loss: 2.9432\n",
            "Iteration: 2822; Percent done: 70.5%; Mean loss: 3.0048\n",
            "Iteration: 2823; Percent done: 70.6%; Mean loss: 3.1194\n",
            "Iteration: 2824; Percent done: 70.6%; Mean loss: 2.8096\n",
            "Iteration: 2825; Percent done: 70.6%; Mean loss: 3.1406\n",
            "Iteration: 2826; Percent done: 70.7%; Mean loss: 3.0752\n",
            "Iteration: 2827; Percent done: 70.7%; Mean loss: 2.7214\n",
            "Iteration: 2828; Percent done: 70.7%; Mean loss: 2.7165\n",
            "Iteration: 2829; Percent done: 70.7%; Mean loss: 3.2041\n",
            "Iteration: 2830; Percent done: 70.8%; Mean loss: 2.7799\n",
            "Iteration: 2831; Percent done: 70.8%; Mean loss: 2.8535\n",
            "Iteration: 2832; Percent done: 70.8%; Mean loss: 2.8341\n",
            "Iteration: 2833; Percent done: 70.8%; Mean loss: 3.1943\n",
            "Iteration: 2834; Percent done: 70.9%; Mean loss: 2.8317\n",
            "Iteration: 2835; Percent done: 70.9%; Mean loss: 2.9516\n",
            "Iteration: 2836; Percent done: 70.9%; Mean loss: 2.8929\n",
            "Iteration: 2837; Percent done: 70.9%; Mean loss: 2.9212\n",
            "Iteration: 2838; Percent done: 71.0%; Mean loss: 2.6862\n",
            "Iteration: 2839; Percent done: 71.0%; Mean loss: 3.1772\n",
            "Iteration: 2840; Percent done: 71.0%; Mean loss: 3.1555\n",
            "Iteration: 2841; Percent done: 71.0%; Mean loss: 2.8292\n",
            "Iteration: 2842; Percent done: 71.0%; Mean loss: 2.8583\n",
            "Iteration: 2843; Percent done: 71.1%; Mean loss: 2.8530\n",
            "Iteration: 2844; Percent done: 71.1%; Mean loss: 2.8734\n",
            "Iteration: 2845; Percent done: 71.1%; Mean loss: 2.9574\n",
            "Iteration: 2846; Percent done: 71.2%; Mean loss: 3.0617\n",
            "Iteration: 2847; Percent done: 71.2%; Mean loss: 2.9464\n",
            "Iteration: 2848; Percent done: 71.2%; Mean loss: 2.9504\n",
            "Iteration: 2849; Percent done: 71.2%; Mean loss: 3.1759\n",
            "Iteration: 2850; Percent done: 71.2%; Mean loss: 2.9828\n",
            "Iteration: 2851; Percent done: 71.3%; Mean loss: 3.2846\n",
            "Iteration: 2852; Percent done: 71.3%; Mean loss: 3.1240\n",
            "Iteration: 2853; Percent done: 71.3%; Mean loss: 2.8096\n",
            "Iteration: 2854; Percent done: 71.4%; Mean loss: 2.9921\n",
            "Iteration: 2855; Percent done: 71.4%; Mean loss: 3.1338\n",
            "Iteration: 2856; Percent done: 71.4%; Mean loss: 3.2908\n",
            "Iteration: 2857; Percent done: 71.4%; Mean loss: 3.2097\n",
            "Iteration: 2858; Percent done: 71.5%; Mean loss: 2.8767\n",
            "Iteration: 2859; Percent done: 71.5%; Mean loss: 2.8830\n",
            "Iteration: 2860; Percent done: 71.5%; Mean loss: 2.9639\n",
            "Iteration: 2861; Percent done: 71.5%; Mean loss: 2.8944\n",
            "Iteration: 2862; Percent done: 71.5%; Mean loss: 3.0508\n",
            "Iteration: 2863; Percent done: 71.6%; Mean loss: 2.8795\n",
            "Iteration: 2864; Percent done: 71.6%; Mean loss: 2.9506\n",
            "Iteration: 2865; Percent done: 71.6%; Mean loss: 2.9826\n",
            "Iteration: 2866; Percent done: 71.7%; Mean loss: 3.1189\n",
            "Iteration: 2867; Percent done: 71.7%; Mean loss: 3.0968\n",
            "Iteration: 2868; Percent done: 71.7%; Mean loss: 2.6590\n",
            "Iteration: 2869; Percent done: 71.7%; Mean loss: 2.8953\n",
            "Iteration: 2870; Percent done: 71.8%; Mean loss: 2.8097\n",
            "Iteration: 2871; Percent done: 71.8%; Mean loss: 2.5178\n",
            "Iteration: 2872; Percent done: 71.8%; Mean loss: 3.1115\n",
            "Iteration: 2873; Percent done: 71.8%; Mean loss: 2.9801\n",
            "Iteration: 2874; Percent done: 71.9%; Mean loss: 2.7868\n",
            "Iteration: 2875; Percent done: 71.9%; Mean loss: 2.9331\n",
            "Iteration: 2876; Percent done: 71.9%; Mean loss: 2.9070\n",
            "Iteration: 2877; Percent done: 71.9%; Mean loss: 2.9482\n",
            "Iteration: 2878; Percent done: 72.0%; Mean loss: 2.9632\n",
            "Iteration: 2879; Percent done: 72.0%; Mean loss: 3.0620\n",
            "Iteration: 2880; Percent done: 72.0%; Mean loss: 2.9045\n",
            "Iteration: 2881; Percent done: 72.0%; Mean loss: 2.9936\n",
            "Iteration: 2882; Percent done: 72.0%; Mean loss: 2.9404\n",
            "Iteration: 2883; Percent done: 72.1%; Mean loss: 3.1152\n",
            "Iteration: 2884; Percent done: 72.1%; Mean loss: 2.7011\n",
            "Iteration: 2885; Percent done: 72.1%; Mean loss: 2.9682\n",
            "Iteration: 2886; Percent done: 72.2%; Mean loss: 2.9567\n",
            "Iteration: 2887; Percent done: 72.2%; Mean loss: 2.9622\n",
            "Iteration: 2888; Percent done: 72.2%; Mean loss: 2.8551\n",
            "Iteration: 2889; Percent done: 72.2%; Mean loss: 2.7720\n",
            "Iteration: 2890; Percent done: 72.2%; Mean loss: 2.9180\n",
            "Iteration: 2891; Percent done: 72.3%; Mean loss: 3.0683\n",
            "Iteration: 2892; Percent done: 72.3%; Mean loss: 2.9160\n",
            "Iteration: 2893; Percent done: 72.3%; Mean loss: 3.0273\n",
            "Iteration: 2894; Percent done: 72.4%; Mean loss: 2.7502\n",
            "Iteration: 2895; Percent done: 72.4%; Mean loss: 3.0525\n",
            "Iteration: 2896; Percent done: 72.4%; Mean loss: 2.8130\n",
            "Iteration: 2897; Percent done: 72.4%; Mean loss: 2.7203\n",
            "Iteration: 2898; Percent done: 72.5%; Mean loss: 3.0835\n",
            "Iteration: 2899; Percent done: 72.5%; Mean loss: 3.0342\n",
            "Iteration: 2900; Percent done: 72.5%; Mean loss: 3.1234\n",
            "Iteration: 2901; Percent done: 72.5%; Mean loss: 2.7657\n",
            "Iteration: 2902; Percent done: 72.5%; Mean loss: 2.8347\n",
            "Iteration: 2903; Percent done: 72.6%; Mean loss: 3.0527\n",
            "Iteration: 2904; Percent done: 72.6%; Mean loss: 2.7825\n",
            "Iteration: 2905; Percent done: 72.6%; Mean loss: 2.8134\n",
            "Iteration: 2906; Percent done: 72.7%; Mean loss: 2.9519\n",
            "Iteration: 2907; Percent done: 72.7%; Mean loss: 3.0243\n",
            "Iteration: 2908; Percent done: 72.7%; Mean loss: 2.7484\n",
            "Iteration: 2909; Percent done: 72.7%; Mean loss: 2.9093\n",
            "Iteration: 2910; Percent done: 72.8%; Mean loss: 2.8928\n",
            "Iteration: 2911; Percent done: 72.8%; Mean loss: 3.0247\n",
            "Iteration: 2912; Percent done: 72.8%; Mean loss: 3.1351\n",
            "Iteration: 2913; Percent done: 72.8%; Mean loss: 3.0263\n",
            "Iteration: 2914; Percent done: 72.9%; Mean loss: 3.2361\n",
            "Iteration: 2915; Percent done: 72.9%; Mean loss: 2.9288\n",
            "Iteration: 2916; Percent done: 72.9%; Mean loss: 2.7728\n",
            "Iteration: 2917; Percent done: 72.9%; Mean loss: 3.0501\n",
            "Iteration: 2918; Percent done: 73.0%; Mean loss: 3.0399\n",
            "Iteration: 2919; Percent done: 73.0%; Mean loss: 2.9476\n",
            "Iteration: 2920; Percent done: 73.0%; Mean loss: 2.8049\n",
            "Iteration: 2921; Percent done: 73.0%; Mean loss: 3.0137\n",
            "Iteration: 2922; Percent done: 73.0%; Mean loss: 2.9034\n",
            "Iteration: 2923; Percent done: 73.1%; Mean loss: 2.9471\n",
            "Iteration: 2924; Percent done: 73.1%; Mean loss: 2.7484\n",
            "Iteration: 2925; Percent done: 73.1%; Mean loss: 2.8663\n",
            "Iteration: 2926; Percent done: 73.2%; Mean loss: 2.9002\n",
            "Iteration: 2927; Percent done: 73.2%; Mean loss: 3.0478\n",
            "Iteration: 2928; Percent done: 73.2%; Mean loss: 2.9341\n",
            "Iteration: 2929; Percent done: 73.2%; Mean loss: 2.8647\n",
            "Iteration: 2930; Percent done: 73.2%; Mean loss: 3.0611\n",
            "Iteration: 2931; Percent done: 73.3%; Mean loss: 2.5677\n",
            "Iteration: 2932; Percent done: 73.3%; Mean loss: 2.9440\n",
            "Iteration: 2933; Percent done: 73.3%; Mean loss: 2.6898\n",
            "Iteration: 2934; Percent done: 73.4%; Mean loss: 2.9912\n",
            "Iteration: 2935; Percent done: 73.4%; Mean loss: 2.9260\n",
            "Iteration: 2936; Percent done: 73.4%; Mean loss: 2.8995\n",
            "Iteration: 2937; Percent done: 73.4%; Mean loss: 2.9458\n",
            "Iteration: 2938; Percent done: 73.5%; Mean loss: 2.9704\n",
            "Iteration: 2939; Percent done: 73.5%; Mean loss: 2.8123\n",
            "Iteration: 2940; Percent done: 73.5%; Mean loss: 3.0239\n",
            "Iteration: 2941; Percent done: 73.5%; Mean loss: 2.8185\n",
            "Iteration: 2942; Percent done: 73.6%; Mean loss: 2.9665\n",
            "Iteration: 2943; Percent done: 73.6%; Mean loss: 2.6407\n",
            "Iteration: 2944; Percent done: 73.6%; Mean loss: 2.8336\n",
            "Iteration: 2945; Percent done: 73.6%; Mean loss: 3.1936\n",
            "Iteration: 2946; Percent done: 73.7%; Mean loss: 3.0434\n",
            "Iteration: 2947; Percent done: 73.7%; Mean loss: 3.0371\n",
            "Iteration: 2948; Percent done: 73.7%; Mean loss: 2.8707\n",
            "Iteration: 2949; Percent done: 73.7%; Mean loss: 3.0614\n",
            "Iteration: 2950; Percent done: 73.8%; Mean loss: 2.7286\n",
            "Iteration: 2951; Percent done: 73.8%; Mean loss: 2.7769\n",
            "Iteration: 2952; Percent done: 73.8%; Mean loss: 3.0938\n",
            "Iteration: 2953; Percent done: 73.8%; Mean loss: 3.0393\n",
            "Iteration: 2954; Percent done: 73.9%; Mean loss: 2.9720\n",
            "Iteration: 2955; Percent done: 73.9%; Mean loss: 2.9956\n",
            "Iteration: 2956; Percent done: 73.9%; Mean loss: 2.8264\n",
            "Iteration: 2957; Percent done: 73.9%; Mean loss: 2.8584\n",
            "Iteration: 2958; Percent done: 74.0%; Mean loss: 2.9365\n",
            "Iteration: 2959; Percent done: 74.0%; Mean loss: 2.8283\n",
            "Iteration: 2960; Percent done: 74.0%; Mean loss: 3.0005\n",
            "Iteration: 2961; Percent done: 74.0%; Mean loss: 2.9252\n",
            "Iteration: 2962; Percent done: 74.1%; Mean loss: 2.7899\n",
            "Iteration: 2963; Percent done: 74.1%; Mean loss: 3.0170\n",
            "Iteration: 2964; Percent done: 74.1%; Mean loss: 3.0221\n",
            "Iteration: 2965; Percent done: 74.1%; Mean loss: 3.0653\n",
            "Iteration: 2966; Percent done: 74.2%; Mean loss: 2.9357\n",
            "Iteration: 2967; Percent done: 74.2%; Mean loss: 3.1349\n",
            "Iteration: 2968; Percent done: 74.2%; Mean loss: 2.7943\n",
            "Iteration: 2969; Percent done: 74.2%; Mean loss: 2.9548\n",
            "Iteration: 2970; Percent done: 74.2%; Mean loss: 2.9212\n",
            "Iteration: 2971; Percent done: 74.3%; Mean loss: 2.8686\n",
            "Iteration: 2972; Percent done: 74.3%; Mean loss: 2.8239\n",
            "Iteration: 2973; Percent done: 74.3%; Mean loss: 2.8108\n",
            "Iteration: 2974; Percent done: 74.4%; Mean loss: 2.9981\n",
            "Iteration: 2975; Percent done: 74.4%; Mean loss: 2.8802\n",
            "Iteration: 2976; Percent done: 74.4%; Mean loss: 3.1054\n",
            "Iteration: 2977; Percent done: 74.4%; Mean loss: 3.0657\n",
            "Iteration: 2978; Percent done: 74.5%; Mean loss: 2.9015\n",
            "Iteration: 2979; Percent done: 74.5%; Mean loss: 3.0339\n",
            "Iteration: 2980; Percent done: 74.5%; Mean loss: 2.6825\n",
            "Iteration: 2981; Percent done: 74.5%; Mean loss: 2.7366\n",
            "Iteration: 2982; Percent done: 74.6%; Mean loss: 2.9605\n",
            "Iteration: 2983; Percent done: 74.6%; Mean loss: 2.8094\n",
            "Iteration: 2984; Percent done: 74.6%; Mean loss: 2.9197\n",
            "Iteration: 2985; Percent done: 74.6%; Mean loss: 3.0589\n",
            "Iteration: 2986; Percent done: 74.7%; Mean loss: 2.7661\n",
            "Iteration: 2987; Percent done: 74.7%; Mean loss: 2.7413\n",
            "Iteration: 2988; Percent done: 74.7%; Mean loss: 3.0913\n",
            "Iteration: 2989; Percent done: 74.7%; Mean loss: 2.9411\n",
            "Iteration: 2990; Percent done: 74.8%; Mean loss: 2.9938\n",
            "Iteration: 2991; Percent done: 74.8%; Mean loss: 2.6281\n",
            "Iteration: 2992; Percent done: 74.8%; Mean loss: 2.8686\n",
            "Iteration: 2993; Percent done: 74.8%; Mean loss: 2.9527\n",
            "Iteration: 2994; Percent done: 74.9%; Mean loss: 3.0190\n",
            "Iteration: 2995; Percent done: 74.9%; Mean loss: 2.9277\n",
            "Iteration: 2996; Percent done: 74.9%; Mean loss: 3.1477\n",
            "Iteration: 2997; Percent done: 74.9%; Mean loss: 2.8695\n",
            "Iteration: 2998; Percent done: 75.0%; Mean loss: 2.9906\n",
            "Iteration: 2999; Percent done: 75.0%; Mean loss: 2.8859\n",
            "Iteration: 3000; Percent done: 75.0%; Mean loss: 2.8893\n",
            "Iteration: 3001; Percent done: 75.0%; Mean loss: 2.8836\n",
            "Iteration: 3002; Percent done: 75.0%; Mean loss: 3.0124\n",
            "Iteration: 3003; Percent done: 75.1%; Mean loss: 2.9799\n",
            "Iteration: 3004; Percent done: 75.1%; Mean loss: 2.7720\n",
            "Iteration: 3005; Percent done: 75.1%; Mean loss: 3.1515\n",
            "Iteration: 3006; Percent done: 75.1%; Mean loss: 2.7810\n",
            "Iteration: 3007; Percent done: 75.2%; Mean loss: 2.8687\n",
            "Iteration: 3008; Percent done: 75.2%; Mean loss: 2.8851\n",
            "Iteration: 3009; Percent done: 75.2%; Mean loss: 2.9078\n",
            "Iteration: 3010; Percent done: 75.2%; Mean loss: 2.9882\n",
            "Iteration: 3011; Percent done: 75.3%; Mean loss: 2.8272\n",
            "Iteration: 3012; Percent done: 75.3%; Mean loss: 2.7478\n",
            "Iteration: 3013; Percent done: 75.3%; Mean loss: 2.8846\n",
            "Iteration: 3014; Percent done: 75.3%; Mean loss: 2.8099\n",
            "Iteration: 3015; Percent done: 75.4%; Mean loss: 2.7544\n",
            "Iteration: 3016; Percent done: 75.4%; Mean loss: 3.1463\n",
            "Iteration: 3017; Percent done: 75.4%; Mean loss: 2.9387\n",
            "Iteration: 3018; Percent done: 75.4%; Mean loss: 2.9358\n",
            "Iteration: 3019; Percent done: 75.5%; Mean loss: 2.8120\n",
            "Iteration: 3020; Percent done: 75.5%; Mean loss: 2.6676\n",
            "Iteration: 3021; Percent done: 75.5%; Mean loss: 3.0468\n",
            "Iteration: 3022; Percent done: 75.5%; Mean loss: 2.7621\n",
            "Iteration: 3023; Percent done: 75.6%; Mean loss: 3.0801\n",
            "Iteration: 3024; Percent done: 75.6%; Mean loss: 2.8558\n",
            "Iteration: 3025; Percent done: 75.6%; Mean loss: 2.7168\n",
            "Iteration: 3026; Percent done: 75.6%; Mean loss: 2.8551\n",
            "Iteration: 3027; Percent done: 75.7%; Mean loss: 2.7232\n",
            "Iteration: 3028; Percent done: 75.7%; Mean loss: 2.8540\n",
            "Iteration: 3029; Percent done: 75.7%; Mean loss: 3.0069\n",
            "Iteration: 3030; Percent done: 75.8%; Mean loss: 3.0231\n",
            "Iteration: 3031; Percent done: 75.8%; Mean loss: 3.0109\n",
            "Iteration: 3032; Percent done: 75.8%; Mean loss: 2.7242\n",
            "Iteration: 3033; Percent done: 75.8%; Mean loss: 2.9670\n",
            "Iteration: 3034; Percent done: 75.8%; Mean loss: 3.1571\n",
            "Iteration: 3035; Percent done: 75.9%; Mean loss: 2.8652\n",
            "Iteration: 3036; Percent done: 75.9%; Mean loss: 3.0727\n",
            "Iteration: 3037; Percent done: 75.9%; Mean loss: 2.8371\n",
            "Iteration: 3038; Percent done: 75.9%; Mean loss: 3.1065\n",
            "Iteration: 3039; Percent done: 76.0%; Mean loss: 2.7437\n",
            "Iteration: 3040; Percent done: 76.0%; Mean loss: 3.0843\n",
            "Iteration: 3041; Percent done: 76.0%; Mean loss: 2.6617\n",
            "Iteration: 3042; Percent done: 76.0%; Mean loss: 2.8845\n",
            "Iteration: 3043; Percent done: 76.1%; Mean loss: 3.0505\n",
            "Iteration: 3044; Percent done: 76.1%; Mean loss: 2.5429\n",
            "Iteration: 3045; Percent done: 76.1%; Mean loss: 2.7446\n",
            "Iteration: 3046; Percent done: 76.1%; Mean loss: 2.8032\n",
            "Iteration: 3047; Percent done: 76.2%; Mean loss: 3.0313\n",
            "Iteration: 3048; Percent done: 76.2%; Mean loss: 2.8203\n",
            "Iteration: 3049; Percent done: 76.2%; Mean loss: 2.7212\n",
            "Iteration: 3050; Percent done: 76.2%; Mean loss: 3.0039\n",
            "Iteration: 3051; Percent done: 76.3%; Mean loss: 2.8674\n",
            "Iteration: 3052; Percent done: 76.3%; Mean loss: 2.9642\n",
            "Iteration: 3053; Percent done: 76.3%; Mean loss: 2.9134\n",
            "Iteration: 3054; Percent done: 76.3%; Mean loss: 2.9915\n",
            "Iteration: 3055; Percent done: 76.4%; Mean loss: 2.8153\n",
            "Iteration: 3056; Percent done: 76.4%; Mean loss: 2.7901\n",
            "Iteration: 3057; Percent done: 76.4%; Mean loss: 3.0706\n",
            "Iteration: 3058; Percent done: 76.4%; Mean loss: 2.8683\n",
            "Iteration: 3059; Percent done: 76.5%; Mean loss: 2.8268\n",
            "Iteration: 3060; Percent done: 76.5%; Mean loss: 2.7292\n",
            "Iteration: 3061; Percent done: 76.5%; Mean loss: 2.7633\n",
            "Iteration: 3062; Percent done: 76.5%; Mean loss: 2.8855\n",
            "Iteration: 3063; Percent done: 76.6%; Mean loss: 2.9376\n",
            "Iteration: 3064; Percent done: 76.6%; Mean loss: 3.0146\n",
            "Iteration: 3065; Percent done: 76.6%; Mean loss: 2.7907\n",
            "Iteration: 3066; Percent done: 76.6%; Mean loss: 2.9731\n",
            "Iteration: 3067; Percent done: 76.7%; Mean loss: 2.9983\n",
            "Iteration: 3068; Percent done: 76.7%; Mean loss: 2.8692\n",
            "Iteration: 3069; Percent done: 76.7%; Mean loss: 2.8395\n",
            "Iteration: 3070; Percent done: 76.8%; Mean loss: 2.9351\n",
            "Iteration: 3071; Percent done: 76.8%; Mean loss: 2.8092\n",
            "Iteration: 3072; Percent done: 76.8%; Mean loss: 2.9066\n",
            "Iteration: 3073; Percent done: 76.8%; Mean loss: 2.8832\n",
            "Iteration: 3074; Percent done: 76.8%; Mean loss: 2.8344\n",
            "Iteration: 3075; Percent done: 76.9%; Mean loss: 2.7933\n",
            "Iteration: 3076; Percent done: 76.9%; Mean loss: 3.2129\n",
            "Iteration: 3077; Percent done: 76.9%; Mean loss: 2.5314\n",
            "Iteration: 3078; Percent done: 77.0%; Mean loss: 3.0311\n",
            "Iteration: 3079; Percent done: 77.0%; Mean loss: 2.8359\n",
            "Iteration: 3080; Percent done: 77.0%; Mean loss: 2.8529\n",
            "Iteration: 3081; Percent done: 77.0%; Mean loss: 2.9566\n",
            "Iteration: 3082; Percent done: 77.0%; Mean loss: 2.6653\n",
            "Iteration: 3083; Percent done: 77.1%; Mean loss: 2.8912\n",
            "Iteration: 3084; Percent done: 77.1%; Mean loss: 2.5954\n",
            "Iteration: 3085; Percent done: 77.1%; Mean loss: 2.9801\n",
            "Iteration: 3086; Percent done: 77.1%; Mean loss: 2.9593\n",
            "Iteration: 3087; Percent done: 77.2%; Mean loss: 2.8498\n",
            "Iteration: 3088; Percent done: 77.2%; Mean loss: 3.0669\n",
            "Iteration: 3089; Percent done: 77.2%; Mean loss: 2.8089\n",
            "Iteration: 3090; Percent done: 77.2%; Mean loss: 2.6339\n",
            "Iteration: 3091; Percent done: 77.3%; Mean loss: 2.7587\n",
            "Iteration: 3092; Percent done: 77.3%; Mean loss: 2.8881\n",
            "Iteration: 3093; Percent done: 77.3%; Mean loss: 2.7608\n",
            "Iteration: 3094; Percent done: 77.3%; Mean loss: 2.8635\n",
            "Iteration: 3095; Percent done: 77.4%; Mean loss: 2.9102\n",
            "Iteration: 3096; Percent done: 77.4%; Mean loss: 2.9409\n",
            "Iteration: 3097; Percent done: 77.4%; Mean loss: 3.0865\n",
            "Iteration: 3098; Percent done: 77.5%; Mean loss: 3.0370\n",
            "Iteration: 3099; Percent done: 77.5%; Mean loss: 2.9364\n",
            "Iteration: 3100; Percent done: 77.5%; Mean loss: 2.9638\n",
            "Iteration: 3101; Percent done: 77.5%; Mean loss: 2.7527\n",
            "Iteration: 3102; Percent done: 77.5%; Mean loss: 2.8258\n",
            "Iteration: 3103; Percent done: 77.6%; Mean loss: 2.8296\n",
            "Iteration: 3104; Percent done: 77.6%; Mean loss: 2.9033\n",
            "Iteration: 3105; Percent done: 77.6%; Mean loss: 2.9571\n",
            "Iteration: 3106; Percent done: 77.6%; Mean loss: 2.8515\n",
            "Iteration: 3107; Percent done: 77.7%; Mean loss: 2.9357\n",
            "Iteration: 3108; Percent done: 77.7%; Mean loss: 2.9203\n",
            "Iteration: 3109; Percent done: 77.7%; Mean loss: 3.1010\n",
            "Iteration: 3110; Percent done: 77.8%; Mean loss: 3.0255\n",
            "Iteration: 3111; Percent done: 77.8%; Mean loss: 3.0193\n",
            "Iteration: 3112; Percent done: 77.8%; Mean loss: 3.0011\n",
            "Iteration: 3113; Percent done: 77.8%; Mean loss: 2.8617\n",
            "Iteration: 3114; Percent done: 77.8%; Mean loss: 3.1314\n",
            "Iteration: 3115; Percent done: 77.9%; Mean loss: 2.8704\n",
            "Iteration: 3116; Percent done: 77.9%; Mean loss: 3.0932\n",
            "Iteration: 3117; Percent done: 77.9%; Mean loss: 2.8215\n",
            "Iteration: 3118; Percent done: 78.0%; Mean loss: 2.9417\n",
            "Iteration: 3119; Percent done: 78.0%; Mean loss: 2.7544\n",
            "Iteration: 3120; Percent done: 78.0%; Mean loss: 2.8541\n",
            "Iteration: 3121; Percent done: 78.0%; Mean loss: 2.7802\n",
            "Iteration: 3122; Percent done: 78.0%; Mean loss: 3.0452\n",
            "Iteration: 3123; Percent done: 78.1%; Mean loss: 2.8603\n",
            "Iteration: 3124; Percent done: 78.1%; Mean loss: 2.9201\n",
            "Iteration: 3125; Percent done: 78.1%; Mean loss: 2.8186\n",
            "Iteration: 3126; Percent done: 78.1%; Mean loss: 2.8289\n",
            "Iteration: 3127; Percent done: 78.2%; Mean loss: 3.1468\n",
            "Iteration: 3128; Percent done: 78.2%; Mean loss: 2.6340\n",
            "Iteration: 3129; Percent done: 78.2%; Mean loss: 2.6352\n",
            "Iteration: 3130; Percent done: 78.2%; Mean loss: 2.8655\n",
            "Iteration: 3131; Percent done: 78.3%; Mean loss: 2.9935\n",
            "Iteration: 3132; Percent done: 78.3%; Mean loss: 2.9879\n",
            "Iteration: 3133; Percent done: 78.3%; Mean loss: 3.2821\n",
            "Iteration: 3134; Percent done: 78.3%; Mean loss: 2.9313\n",
            "Iteration: 3135; Percent done: 78.4%; Mean loss: 2.9263\n",
            "Iteration: 3136; Percent done: 78.4%; Mean loss: 2.8649\n",
            "Iteration: 3137; Percent done: 78.4%; Mean loss: 2.8826\n",
            "Iteration: 3138; Percent done: 78.5%; Mean loss: 3.0416\n",
            "Iteration: 3139; Percent done: 78.5%; Mean loss: 2.8659\n",
            "Iteration: 3140; Percent done: 78.5%; Mean loss: 2.7299\n",
            "Iteration: 3141; Percent done: 78.5%; Mean loss: 2.8990\n",
            "Iteration: 3142; Percent done: 78.5%; Mean loss: 2.6493\n",
            "Iteration: 3143; Percent done: 78.6%; Mean loss: 2.5659\n",
            "Iteration: 3144; Percent done: 78.6%; Mean loss: 2.6500\n",
            "Iteration: 3145; Percent done: 78.6%; Mean loss: 2.8790\n",
            "Iteration: 3146; Percent done: 78.6%; Mean loss: 2.6906\n",
            "Iteration: 3147; Percent done: 78.7%; Mean loss: 2.9290\n",
            "Iteration: 3148; Percent done: 78.7%; Mean loss: 2.6143\n",
            "Iteration: 3149; Percent done: 78.7%; Mean loss: 2.7493\n",
            "Iteration: 3150; Percent done: 78.8%; Mean loss: 2.9715\n",
            "Iteration: 3151; Percent done: 78.8%; Mean loss: 3.1577\n",
            "Iteration: 3152; Percent done: 78.8%; Mean loss: 2.7689\n",
            "Iteration: 3153; Percent done: 78.8%; Mean loss: 2.9769\n",
            "Iteration: 3154; Percent done: 78.8%; Mean loss: 2.8189\n",
            "Iteration: 3155; Percent done: 78.9%; Mean loss: 2.9545\n",
            "Iteration: 3156; Percent done: 78.9%; Mean loss: 3.0117\n",
            "Iteration: 3157; Percent done: 78.9%; Mean loss: 2.8503\n",
            "Iteration: 3158; Percent done: 79.0%; Mean loss: 2.9116\n",
            "Iteration: 3159; Percent done: 79.0%; Mean loss: 3.1343\n",
            "Iteration: 3160; Percent done: 79.0%; Mean loss: 2.7444\n",
            "Iteration: 3161; Percent done: 79.0%; Mean loss: 2.9681\n",
            "Iteration: 3162; Percent done: 79.0%; Mean loss: 2.8236\n",
            "Iteration: 3163; Percent done: 79.1%; Mean loss: 2.8441\n",
            "Iteration: 3164; Percent done: 79.1%; Mean loss: 2.8947\n",
            "Iteration: 3165; Percent done: 79.1%; Mean loss: 2.8676\n",
            "Iteration: 3166; Percent done: 79.1%; Mean loss: 2.7750\n",
            "Iteration: 3167; Percent done: 79.2%; Mean loss: 3.0633\n",
            "Iteration: 3168; Percent done: 79.2%; Mean loss: 3.0242\n",
            "Iteration: 3169; Percent done: 79.2%; Mean loss: 2.9894\n",
            "Iteration: 3170; Percent done: 79.2%; Mean loss: 3.0822\n",
            "Iteration: 3171; Percent done: 79.3%; Mean loss: 2.9998\n",
            "Iteration: 3172; Percent done: 79.3%; Mean loss: 3.0362\n",
            "Iteration: 3173; Percent done: 79.3%; Mean loss: 2.8540\n",
            "Iteration: 3174; Percent done: 79.3%; Mean loss: 2.7370\n",
            "Iteration: 3175; Percent done: 79.4%; Mean loss: 2.9943\n",
            "Iteration: 3176; Percent done: 79.4%; Mean loss: 3.0885\n",
            "Iteration: 3177; Percent done: 79.4%; Mean loss: 3.0331\n",
            "Iteration: 3178; Percent done: 79.5%; Mean loss: 2.9508\n",
            "Iteration: 3179; Percent done: 79.5%; Mean loss: 2.7274\n",
            "Iteration: 3180; Percent done: 79.5%; Mean loss: 3.0144\n",
            "Iteration: 3181; Percent done: 79.5%; Mean loss: 2.7695\n",
            "Iteration: 3182; Percent done: 79.5%; Mean loss: 3.0671\n",
            "Iteration: 3183; Percent done: 79.6%; Mean loss: 3.1564\n",
            "Iteration: 3184; Percent done: 79.6%; Mean loss: 2.8071\n",
            "Iteration: 3185; Percent done: 79.6%; Mean loss: 2.6198\n",
            "Iteration: 3186; Percent done: 79.7%; Mean loss: 2.6620\n",
            "Iteration: 3187; Percent done: 79.7%; Mean loss: 2.9607\n",
            "Iteration: 3188; Percent done: 79.7%; Mean loss: 2.8623\n",
            "Iteration: 3189; Percent done: 79.7%; Mean loss: 2.8967\n",
            "Iteration: 3190; Percent done: 79.8%; Mean loss: 2.9281\n",
            "Iteration: 3191; Percent done: 79.8%; Mean loss: 2.8171\n",
            "Iteration: 3192; Percent done: 79.8%; Mean loss: 2.9743\n",
            "Iteration: 3193; Percent done: 79.8%; Mean loss: 2.7395\n",
            "Iteration: 3194; Percent done: 79.8%; Mean loss: 2.7772\n",
            "Iteration: 3195; Percent done: 79.9%; Mean loss: 3.0185\n",
            "Iteration: 3196; Percent done: 79.9%; Mean loss: 2.9082\n",
            "Iteration: 3197; Percent done: 79.9%; Mean loss: 3.1019\n",
            "Iteration: 3198; Percent done: 80.0%; Mean loss: 2.7321\n",
            "Iteration: 3199; Percent done: 80.0%; Mean loss: 2.5970\n",
            "Iteration: 3200; Percent done: 80.0%; Mean loss: 2.8078\n",
            "Iteration: 3201; Percent done: 80.0%; Mean loss: 2.9595\n",
            "Iteration: 3202; Percent done: 80.0%; Mean loss: 2.7940\n",
            "Iteration: 3203; Percent done: 80.1%; Mean loss: 2.6910\n",
            "Iteration: 3204; Percent done: 80.1%; Mean loss: 3.0094\n",
            "Iteration: 3205; Percent done: 80.1%; Mean loss: 2.8540\n",
            "Iteration: 3206; Percent done: 80.2%; Mean loss: 2.6752\n",
            "Iteration: 3207; Percent done: 80.2%; Mean loss: 2.9459\n",
            "Iteration: 3208; Percent done: 80.2%; Mean loss: 2.7154\n",
            "Iteration: 3209; Percent done: 80.2%; Mean loss: 2.6640\n",
            "Iteration: 3210; Percent done: 80.2%; Mean loss: 2.7833\n",
            "Iteration: 3211; Percent done: 80.3%; Mean loss: 2.7131\n",
            "Iteration: 3212; Percent done: 80.3%; Mean loss: 2.7304\n",
            "Iteration: 3213; Percent done: 80.3%; Mean loss: 2.6911\n",
            "Iteration: 3214; Percent done: 80.3%; Mean loss: 2.6674\n",
            "Iteration: 3215; Percent done: 80.4%; Mean loss: 2.7359\n",
            "Iteration: 3216; Percent done: 80.4%; Mean loss: 2.9068\n",
            "Iteration: 3217; Percent done: 80.4%; Mean loss: 2.9139\n",
            "Iteration: 3218; Percent done: 80.5%; Mean loss: 2.8164\n",
            "Iteration: 3219; Percent done: 80.5%; Mean loss: 2.5330\n",
            "Iteration: 3220; Percent done: 80.5%; Mean loss: 2.7035\n",
            "Iteration: 3221; Percent done: 80.5%; Mean loss: 2.9411\n",
            "Iteration: 3222; Percent done: 80.5%; Mean loss: 2.7681\n",
            "Iteration: 3223; Percent done: 80.6%; Mean loss: 2.9100\n",
            "Iteration: 3224; Percent done: 80.6%; Mean loss: 2.8708\n",
            "Iteration: 3225; Percent done: 80.6%; Mean loss: 2.9987\n",
            "Iteration: 3226; Percent done: 80.7%; Mean loss: 2.6335\n",
            "Iteration: 3227; Percent done: 80.7%; Mean loss: 2.8181\n",
            "Iteration: 3228; Percent done: 80.7%; Mean loss: 2.8490\n",
            "Iteration: 3229; Percent done: 80.7%; Mean loss: 2.8234\n",
            "Iteration: 3230; Percent done: 80.8%; Mean loss: 2.8407\n",
            "Iteration: 3231; Percent done: 80.8%; Mean loss: 2.7762\n",
            "Iteration: 3232; Percent done: 80.8%; Mean loss: 2.7911\n",
            "Iteration: 3233; Percent done: 80.8%; Mean loss: 2.5280\n",
            "Iteration: 3234; Percent done: 80.8%; Mean loss: 2.6807\n",
            "Iteration: 3235; Percent done: 80.9%; Mean loss: 2.9091\n",
            "Iteration: 3236; Percent done: 80.9%; Mean loss: 2.7226\n",
            "Iteration: 3237; Percent done: 80.9%; Mean loss: 2.8647\n",
            "Iteration: 3238; Percent done: 81.0%; Mean loss: 2.6273\n",
            "Iteration: 3239; Percent done: 81.0%; Mean loss: 2.7549\n",
            "Iteration: 3240; Percent done: 81.0%; Mean loss: 2.8281\n",
            "Iteration: 3241; Percent done: 81.0%; Mean loss: 2.9150\n",
            "Iteration: 3242; Percent done: 81.0%; Mean loss: 2.8259\n",
            "Iteration: 3243; Percent done: 81.1%; Mean loss: 2.8554\n",
            "Iteration: 3244; Percent done: 81.1%; Mean loss: 2.9233\n",
            "Iteration: 3245; Percent done: 81.1%; Mean loss: 2.7815\n",
            "Iteration: 3246; Percent done: 81.2%; Mean loss: 2.9840\n",
            "Iteration: 3247; Percent done: 81.2%; Mean loss: 2.7934\n",
            "Iteration: 3248; Percent done: 81.2%; Mean loss: 2.9271\n",
            "Iteration: 3249; Percent done: 81.2%; Mean loss: 2.8852\n",
            "Iteration: 3250; Percent done: 81.2%; Mean loss: 2.7734\n",
            "Iteration: 3251; Percent done: 81.3%; Mean loss: 3.1926\n",
            "Iteration: 3252; Percent done: 81.3%; Mean loss: 2.7000\n",
            "Iteration: 3253; Percent done: 81.3%; Mean loss: 2.7951\n",
            "Iteration: 3254; Percent done: 81.3%; Mean loss: 2.6678\n",
            "Iteration: 3255; Percent done: 81.4%; Mean loss: 2.8366\n",
            "Iteration: 3256; Percent done: 81.4%; Mean loss: 2.8611\n",
            "Iteration: 3257; Percent done: 81.4%; Mean loss: 2.8731\n",
            "Iteration: 3258; Percent done: 81.5%; Mean loss: 2.6674\n",
            "Iteration: 3259; Percent done: 81.5%; Mean loss: 2.9155\n",
            "Iteration: 3260; Percent done: 81.5%; Mean loss: 2.9843\n",
            "Iteration: 3261; Percent done: 81.5%; Mean loss: 2.8375\n",
            "Iteration: 3262; Percent done: 81.5%; Mean loss: 2.6332\n",
            "Iteration: 3263; Percent done: 81.6%; Mean loss: 3.0234\n",
            "Iteration: 3264; Percent done: 81.6%; Mean loss: 2.5310\n",
            "Iteration: 3265; Percent done: 81.6%; Mean loss: 2.7247\n",
            "Iteration: 3266; Percent done: 81.7%; Mean loss: 2.8415\n",
            "Iteration: 3267; Percent done: 81.7%; Mean loss: 2.8014\n",
            "Iteration: 3268; Percent done: 81.7%; Mean loss: 3.0290\n",
            "Iteration: 3269; Percent done: 81.7%; Mean loss: 2.7089\n",
            "Iteration: 3270; Percent done: 81.8%; Mean loss: 2.7943\n",
            "Iteration: 3271; Percent done: 81.8%; Mean loss: 2.9793\n",
            "Iteration: 3272; Percent done: 81.8%; Mean loss: 2.6366\n",
            "Iteration: 3273; Percent done: 81.8%; Mean loss: 2.9169\n",
            "Iteration: 3274; Percent done: 81.8%; Mean loss: 2.6920\n",
            "Iteration: 3275; Percent done: 81.9%; Mean loss: 2.8842\n",
            "Iteration: 3276; Percent done: 81.9%; Mean loss: 2.9436\n",
            "Iteration: 3277; Percent done: 81.9%; Mean loss: 2.8666\n",
            "Iteration: 3278; Percent done: 82.0%; Mean loss: 2.8877\n",
            "Iteration: 3279; Percent done: 82.0%; Mean loss: 2.9316\n",
            "Iteration: 3280; Percent done: 82.0%; Mean loss: 2.7976\n",
            "Iteration: 3281; Percent done: 82.0%; Mean loss: 2.7603\n",
            "Iteration: 3282; Percent done: 82.0%; Mean loss: 3.0736\n",
            "Iteration: 3283; Percent done: 82.1%; Mean loss: 2.8855\n",
            "Iteration: 3284; Percent done: 82.1%; Mean loss: 2.8141\n",
            "Iteration: 3285; Percent done: 82.1%; Mean loss: 2.8477\n",
            "Iteration: 3286; Percent done: 82.2%; Mean loss: 2.6995\n",
            "Iteration: 3287; Percent done: 82.2%; Mean loss: 2.7629\n",
            "Iteration: 3288; Percent done: 82.2%; Mean loss: 2.7411\n",
            "Iteration: 3289; Percent done: 82.2%; Mean loss: 2.7833\n",
            "Iteration: 3290; Percent done: 82.2%; Mean loss: 2.8340\n",
            "Iteration: 3291; Percent done: 82.3%; Mean loss: 2.6757\n",
            "Iteration: 3292; Percent done: 82.3%; Mean loss: 3.1713\n",
            "Iteration: 3293; Percent done: 82.3%; Mean loss: 2.8608\n",
            "Iteration: 3294; Percent done: 82.3%; Mean loss: 3.1122\n",
            "Iteration: 3295; Percent done: 82.4%; Mean loss: 2.9617\n",
            "Iteration: 3296; Percent done: 82.4%; Mean loss: 2.8096\n",
            "Iteration: 3297; Percent done: 82.4%; Mean loss: 2.9554\n",
            "Iteration: 3298; Percent done: 82.5%; Mean loss: 2.6173\n",
            "Iteration: 3299; Percent done: 82.5%; Mean loss: 2.8961\n",
            "Iteration: 3300; Percent done: 82.5%; Mean loss: 2.8853\n",
            "Iteration: 3301; Percent done: 82.5%; Mean loss: 2.7973\n",
            "Iteration: 3302; Percent done: 82.5%; Mean loss: 2.8749\n",
            "Iteration: 3303; Percent done: 82.6%; Mean loss: 2.6965\n",
            "Iteration: 3304; Percent done: 82.6%; Mean loss: 2.7935\n",
            "Iteration: 3305; Percent done: 82.6%; Mean loss: 2.9472\n",
            "Iteration: 3306; Percent done: 82.7%; Mean loss: 3.0909\n",
            "Iteration: 3307; Percent done: 82.7%; Mean loss: 2.6690\n",
            "Iteration: 3308; Percent done: 82.7%; Mean loss: 2.6003\n",
            "Iteration: 3309; Percent done: 82.7%; Mean loss: 2.6887\n",
            "Iteration: 3310; Percent done: 82.8%; Mean loss: 2.7514\n",
            "Iteration: 3311; Percent done: 82.8%; Mean loss: 2.8646\n",
            "Iteration: 3312; Percent done: 82.8%; Mean loss: 2.7076\n",
            "Iteration: 3313; Percent done: 82.8%; Mean loss: 2.9255\n",
            "Iteration: 3314; Percent done: 82.8%; Mean loss: 2.8762\n",
            "Iteration: 3315; Percent done: 82.9%; Mean loss: 2.7192\n",
            "Iteration: 3316; Percent done: 82.9%; Mean loss: 2.6201\n",
            "Iteration: 3317; Percent done: 82.9%; Mean loss: 2.8719\n",
            "Iteration: 3318; Percent done: 83.0%; Mean loss: 2.6879\n",
            "Iteration: 3319; Percent done: 83.0%; Mean loss: 2.6115\n",
            "Iteration: 3320; Percent done: 83.0%; Mean loss: 2.7571\n",
            "Iteration: 3321; Percent done: 83.0%; Mean loss: 2.6699\n",
            "Iteration: 3322; Percent done: 83.0%; Mean loss: 2.9196\n",
            "Iteration: 3323; Percent done: 83.1%; Mean loss: 3.0440\n",
            "Iteration: 3324; Percent done: 83.1%; Mean loss: 2.8591\n",
            "Iteration: 3325; Percent done: 83.1%; Mean loss: 2.8942\n",
            "Iteration: 3326; Percent done: 83.2%; Mean loss: 2.6749\n",
            "Iteration: 3327; Percent done: 83.2%; Mean loss: 3.1821\n",
            "Iteration: 3328; Percent done: 83.2%; Mean loss: 2.8700\n",
            "Iteration: 3329; Percent done: 83.2%; Mean loss: 2.7483\n",
            "Iteration: 3330; Percent done: 83.2%; Mean loss: 2.9073\n",
            "Iteration: 3331; Percent done: 83.3%; Mean loss: 3.0224\n",
            "Iteration: 3332; Percent done: 83.3%; Mean loss: 2.9558\n",
            "Iteration: 3333; Percent done: 83.3%; Mean loss: 2.8677\n",
            "Iteration: 3334; Percent done: 83.4%; Mean loss: 2.8290\n",
            "Iteration: 3335; Percent done: 83.4%; Mean loss: 2.8108\n",
            "Iteration: 3336; Percent done: 83.4%; Mean loss: 2.8529\n",
            "Iteration: 3337; Percent done: 83.4%; Mean loss: 2.8591\n",
            "Iteration: 3338; Percent done: 83.5%; Mean loss: 2.6911\n",
            "Iteration: 3339; Percent done: 83.5%; Mean loss: 2.8065\n",
            "Iteration: 3340; Percent done: 83.5%; Mean loss: 2.8558\n",
            "Iteration: 3341; Percent done: 83.5%; Mean loss: 2.7263\n",
            "Iteration: 3342; Percent done: 83.5%; Mean loss: 2.9490\n",
            "Iteration: 3343; Percent done: 83.6%; Mean loss: 3.0428\n",
            "Iteration: 3344; Percent done: 83.6%; Mean loss: 2.6713\n",
            "Iteration: 3345; Percent done: 83.6%; Mean loss: 3.0727\n",
            "Iteration: 3346; Percent done: 83.7%; Mean loss: 2.8787\n",
            "Iteration: 3347; Percent done: 83.7%; Mean loss: 2.6803\n",
            "Iteration: 3348; Percent done: 83.7%; Mean loss: 2.8828\n",
            "Iteration: 3349; Percent done: 83.7%; Mean loss: 2.8732\n",
            "Iteration: 3350; Percent done: 83.8%; Mean loss: 2.8831\n",
            "Iteration: 3351; Percent done: 83.8%; Mean loss: 2.5183\n",
            "Iteration: 3352; Percent done: 83.8%; Mean loss: 2.7328\n",
            "Iteration: 3353; Percent done: 83.8%; Mean loss: 3.1253\n",
            "Iteration: 3354; Percent done: 83.9%; Mean loss: 2.9317\n",
            "Iteration: 3355; Percent done: 83.9%; Mean loss: 2.8141\n",
            "Iteration: 3356; Percent done: 83.9%; Mean loss: 2.8204\n",
            "Iteration: 3357; Percent done: 83.9%; Mean loss: 2.8109\n",
            "Iteration: 3358; Percent done: 84.0%; Mean loss: 2.8346\n",
            "Iteration: 3359; Percent done: 84.0%; Mean loss: 2.5946\n",
            "Iteration: 3360; Percent done: 84.0%; Mean loss: 2.7753\n",
            "Iteration: 3361; Percent done: 84.0%; Mean loss: 2.9958\n",
            "Iteration: 3362; Percent done: 84.0%; Mean loss: 2.9360\n",
            "Iteration: 3363; Percent done: 84.1%; Mean loss: 2.8851\n",
            "Iteration: 3364; Percent done: 84.1%; Mean loss: 2.8342\n",
            "Iteration: 3365; Percent done: 84.1%; Mean loss: 2.8164\n",
            "Iteration: 3366; Percent done: 84.2%; Mean loss: 2.8909\n",
            "Iteration: 3367; Percent done: 84.2%; Mean loss: 2.9289\n",
            "Iteration: 3368; Percent done: 84.2%; Mean loss: 2.5473\n",
            "Iteration: 3369; Percent done: 84.2%; Mean loss: 2.8044\n",
            "Iteration: 3370; Percent done: 84.2%; Mean loss: 2.7411\n",
            "Iteration: 3371; Percent done: 84.3%; Mean loss: 2.7589\n",
            "Iteration: 3372; Percent done: 84.3%; Mean loss: 2.8843\n",
            "Iteration: 3373; Percent done: 84.3%; Mean loss: 2.5243\n",
            "Iteration: 3374; Percent done: 84.4%; Mean loss: 2.7304\n",
            "Iteration: 3375; Percent done: 84.4%; Mean loss: 2.6224\n",
            "Iteration: 3376; Percent done: 84.4%; Mean loss: 2.9179\n",
            "Iteration: 3377; Percent done: 84.4%; Mean loss: 2.9933\n",
            "Iteration: 3378; Percent done: 84.5%; Mean loss: 2.9888\n",
            "Iteration: 3379; Percent done: 84.5%; Mean loss: 2.7432\n",
            "Iteration: 3380; Percent done: 84.5%; Mean loss: 2.9417\n",
            "Iteration: 3381; Percent done: 84.5%; Mean loss: 3.0031\n",
            "Iteration: 3382; Percent done: 84.5%; Mean loss: 2.7962\n",
            "Iteration: 3383; Percent done: 84.6%; Mean loss: 2.6888\n",
            "Iteration: 3384; Percent done: 84.6%; Mean loss: 2.7353\n",
            "Iteration: 3385; Percent done: 84.6%; Mean loss: 2.7771\n",
            "Iteration: 3386; Percent done: 84.7%; Mean loss: 2.6921\n",
            "Iteration: 3387; Percent done: 84.7%; Mean loss: 2.5738\n",
            "Iteration: 3388; Percent done: 84.7%; Mean loss: 2.8096\n",
            "Iteration: 3389; Percent done: 84.7%; Mean loss: 2.9336\n",
            "Iteration: 3390; Percent done: 84.8%; Mean loss: 2.7962\n",
            "Iteration: 3391; Percent done: 84.8%; Mean loss: 2.7403\n",
            "Iteration: 3392; Percent done: 84.8%; Mean loss: 2.9431\n",
            "Iteration: 3393; Percent done: 84.8%; Mean loss: 2.7010\n",
            "Iteration: 3394; Percent done: 84.9%; Mean loss: 2.8750\n",
            "Iteration: 3395; Percent done: 84.9%; Mean loss: 2.9740\n",
            "Iteration: 3396; Percent done: 84.9%; Mean loss: 2.8061\n",
            "Iteration: 3397; Percent done: 84.9%; Mean loss: 3.0369\n",
            "Iteration: 3398; Percent done: 85.0%; Mean loss: 2.6472\n",
            "Iteration: 3399; Percent done: 85.0%; Mean loss: 2.7634\n",
            "Iteration: 3400; Percent done: 85.0%; Mean loss: 2.6195\n",
            "Iteration: 3401; Percent done: 85.0%; Mean loss: 2.6703\n",
            "Iteration: 3402; Percent done: 85.0%; Mean loss: 2.7298\n",
            "Iteration: 3403; Percent done: 85.1%; Mean loss: 3.0224\n",
            "Iteration: 3404; Percent done: 85.1%; Mean loss: 2.9420\n",
            "Iteration: 3405; Percent done: 85.1%; Mean loss: 2.9544\n",
            "Iteration: 3406; Percent done: 85.2%; Mean loss: 2.7473\n",
            "Iteration: 3407; Percent done: 85.2%; Mean loss: 3.0076\n",
            "Iteration: 3408; Percent done: 85.2%; Mean loss: 2.8806\n",
            "Iteration: 3409; Percent done: 85.2%; Mean loss: 2.8162\n",
            "Iteration: 3410; Percent done: 85.2%; Mean loss: 2.8825\n",
            "Iteration: 3411; Percent done: 85.3%; Mean loss: 2.8310\n",
            "Iteration: 3412; Percent done: 85.3%; Mean loss: 2.9138\n",
            "Iteration: 3413; Percent done: 85.3%; Mean loss: 2.5461\n",
            "Iteration: 3414; Percent done: 85.4%; Mean loss: 2.7784\n",
            "Iteration: 3415; Percent done: 85.4%; Mean loss: 2.6896\n",
            "Iteration: 3416; Percent done: 85.4%; Mean loss: 2.9412\n",
            "Iteration: 3417; Percent done: 85.4%; Mean loss: 2.6822\n",
            "Iteration: 3418; Percent done: 85.5%; Mean loss: 2.9672\n",
            "Iteration: 3419; Percent done: 85.5%; Mean loss: 2.6745\n",
            "Iteration: 3420; Percent done: 85.5%; Mean loss: 2.8665\n",
            "Iteration: 3421; Percent done: 85.5%; Mean loss: 2.7770\n",
            "Iteration: 3422; Percent done: 85.5%; Mean loss: 2.7675\n",
            "Iteration: 3423; Percent done: 85.6%; Mean loss: 3.0039\n",
            "Iteration: 3424; Percent done: 85.6%; Mean loss: 2.7389\n",
            "Iteration: 3425; Percent done: 85.6%; Mean loss: 2.7985\n",
            "Iteration: 3426; Percent done: 85.7%; Mean loss: 2.4480\n",
            "Iteration: 3427; Percent done: 85.7%; Mean loss: 2.9549\n",
            "Iteration: 3428; Percent done: 85.7%; Mean loss: 2.9051\n",
            "Iteration: 3429; Percent done: 85.7%; Mean loss: 2.6703\n",
            "Iteration: 3430; Percent done: 85.8%; Mean loss: 2.7055\n",
            "Iteration: 3431; Percent done: 85.8%; Mean loss: 2.8328\n",
            "Iteration: 3432; Percent done: 85.8%; Mean loss: 2.7602\n",
            "Iteration: 3433; Percent done: 85.8%; Mean loss: 2.7190\n",
            "Iteration: 3434; Percent done: 85.9%; Mean loss: 2.8040\n",
            "Iteration: 3435; Percent done: 85.9%; Mean loss: 2.6898\n",
            "Iteration: 3436; Percent done: 85.9%; Mean loss: 2.6261\n",
            "Iteration: 3437; Percent done: 85.9%; Mean loss: 2.7691\n",
            "Iteration: 3438; Percent done: 86.0%; Mean loss: 2.8597\n",
            "Iteration: 3439; Percent done: 86.0%; Mean loss: 3.1861\n",
            "Iteration: 3440; Percent done: 86.0%; Mean loss: 2.8612\n",
            "Iteration: 3441; Percent done: 86.0%; Mean loss: 2.5950\n",
            "Iteration: 3442; Percent done: 86.1%; Mean loss: 2.8397\n",
            "Iteration: 3443; Percent done: 86.1%; Mean loss: 2.6863\n",
            "Iteration: 3444; Percent done: 86.1%; Mean loss: 2.7738\n",
            "Iteration: 3445; Percent done: 86.1%; Mean loss: 2.7964\n",
            "Iteration: 3446; Percent done: 86.2%; Mean loss: 2.7051\n",
            "Iteration: 3447; Percent done: 86.2%; Mean loss: 2.7794\n",
            "Iteration: 3448; Percent done: 86.2%; Mean loss: 2.8222\n",
            "Iteration: 3449; Percent done: 86.2%; Mean loss: 2.8499\n",
            "Iteration: 3450; Percent done: 86.2%; Mean loss: 2.7314\n",
            "Iteration: 3451; Percent done: 86.3%; Mean loss: 2.9636\n",
            "Iteration: 3452; Percent done: 86.3%; Mean loss: 2.8873\n",
            "Iteration: 3453; Percent done: 86.3%; Mean loss: 2.8603\n",
            "Iteration: 3454; Percent done: 86.4%; Mean loss: 2.7468\n",
            "Iteration: 3455; Percent done: 86.4%; Mean loss: 2.8770\n",
            "Iteration: 3456; Percent done: 86.4%; Mean loss: 2.6752\n",
            "Iteration: 3457; Percent done: 86.4%; Mean loss: 2.6694\n",
            "Iteration: 3458; Percent done: 86.5%; Mean loss: 2.7190\n",
            "Iteration: 3459; Percent done: 86.5%; Mean loss: 2.7617\n",
            "Iteration: 3460; Percent done: 86.5%; Mean loss: 2.8825\n",
            "Iteration: 3461; Percent done: 86.5%; Mean loss: 2.7297\n",
            "Iteration: 3462; Percent done: 86.6%; Mean loss: 3.0574\n",
            "Iteration: 3463; Percent done: 86.6%; Mean loss: 2.9524\n",
            "Iteration: 3464; Percent done: 86.6%; Mean loss: 2.3872\n",
            "Iteration: 3465; Percent done: 86.6%; Mean loss: 2.8272\n",
            "Iteration: 3466; Percent done: 86.7%; Mean loss: 3.0703\n",
            "Iteration: 3467; Percent done: 86.7%; Mean loss: 2.7666\n",
            "Iteration: 3468; Percent done: 86.7%; Mean loss: 2.6008\n",
            "Iteration: 3469; Percent done: 86.7%; Mean loss: 2.7081\n",
            "Iteration: 3470; Percent done: 86.8%; Mean loss: 2.6813\n",
            "Iteration: 3471; Percent done: 86.8%; Mean loss: 2.9166\n",
            "Iteration: 3472; Percent done: 86.8%; Mean loss: 2.9478\n",
            "Iteration: 3473; Percent done: 86.8%; Mean loss: 2.7775\n",
            "Iteration: 3474; Percent done: 86.9%; Mean loss: 2.7893\n",
            "Iteration: 3475; Percent done: 86.9%; Mean loss: 2.7562\n",
            "Iteration: 3476; Percent done: 86.9%; Mean loss: 2.5857\n",
            "Iteration: 3477; Percent done: 86.9%; Mean loss: 3.0121\n",
            "Iteration: 3478; Percent done: 87.0%; Mean loss: 2.7652\n",
            "Iteration: 3479; Percent done: 87.0%; Mean loss: 2.8427\n",
            "Iteration: 3480; Percent done: 87.0%; Mean loss: 2.9526\n",
            "Iteration: 3481; Percent done: 87.0%; Mean loss: 2.7881\n",
            "Iteration: 3482; Percent done: 87.1%; Mean loss: 2.5887\n",
            "Iteration: 3483; Percent done: 87.1%; Mean loss: 2.8863\n",
            "Iteration: 3484; Percent done: 87.1%; Mean loss: 2.7045\n",
            "Iteration: 3485; Percent done: 87.1%; Mean loss: 2.6586\n",
            "Iteration: 3486; Percent done: 87.2%; Mean loss: 2.8233\n",
            "Iteration: 3487; Percent done: 87.2%; Mean loss: 2.7860\n",
            "Iteration: 3488; Percent done: 87.2%; Mean loss: 2.9871\n",
            "Iteration: 3489; Percent done: 87.2%; Mean loss: 2.7623\n",
            "Iteration: 3490; Percent done: 87.2%; Mean loss: 2.6531\n",
            "Iteration: 3491; Percent done: 87.3%; Mean loss: 3.0139\n",
            "Iteration: 3492; Percent done: 87.3%; Mean loss: 3.0019\n",
            "Iteration: 3493; Percent done: 87.3%; Mean loss: 2.6979\n",
            "Iteration: 3494; Percent done: 87.4%; Mean loss: 2.6631\n",
            "Iteration: 3495; Percent done: 87.4%; Mean loss: 2.6997\n",
            "Iteration: 3496; Percent done: 87.4%; Mean loss: 2.9514\n",
            "Iteration: 3497; Percent done: 87.4%; Mean loss: 2.7199\n",
            "Iteration: 3498; Percent done: 87.5%; Mean loss: 3.0386\n",
            "Iteration: 3499; Percent done: 87.5%; Mean loss: 2.5204\n",
            "Iteration: 3500; Percent done: 87.5%; Mean loss: 2.7865\n",
            "Iteration: 3501; Percent done: 87.5%; Mean loss: 2.9023\n",
            "Iteration: 3502; Percent done: 87.5%; Mean loss: 2.6084\n",
            "Iteration: 3503; Percent done: 87.6%; Mean loss: 2.6882\n",
            "Iteration: 3504; Percent done: 87.6%; Mean loss: 2.7705\n",
            "Iteration: 3505; Percent done: 87.6%; Mean loss: 2.8650\n",
            "Iteration: 3506; Percent done: 87.6%; Mean loss: 2.6500\n",
            "Iteration: 3507; Percent done: 87.7%; Mean loss: 2.5723\n",
            "Iteration: 3508; Percent done: 87.7%; Mean loss: 2.6967\n",
            "Iteration: 3509; Percent done: 87.7%; Mean loss: 2.8113\n",
            "Iteration: 3510; Percent done: 87.8%; Mean loss: 2.7488\n",
            "Iteration: 3511; Percent done: 87.8%; Mean loss: 2.9496\n",
            "Iteration: 3512; Percent done: 87.8%; Mean loss: 2.7418\n",
            "Iteration: 3513; Percent done: 87.8%; Mean loss: 2.6563\n",
            "Iteration: 3514; Percent done: 87.8%; Mean loss: 2.7946\n",
            "Iteration: 3515; Percent done: 87.9%; Mean loss: 2.7630\n",
            "Iteration: 3516; Percent done: 87.9%; Mean loss: 2.9045\n",
            "Iteration: 3517; Percent done: 87.9%; Mean loss: 2.7071\n",
            "Iteration: 3518; Percent done: 87.9%; Mean loss: 2.8302\n",
            "Iteration: 3519; Percent done: 88.0%; Mean loss: 2.5583\n",
            "Iteration: 3520; Percent done: 88.0%; Mean loss: 2.6190\n",
            "Iteration: 3521; Percent done: 88.0%; Mean loss: 2.9925\n",
            "Iteration: 3522; Percent done: 88.0%; Mean loss: 2.5974\n",
            "Iteration: 3523; Percent done: 88.1%; Mean loss: 2.7474\n",
            "Iteration: 3524; Percent done: 88.1%; Mean loss: 2.8263\n",
            "Iteration: 3525; Percent done: 88.1%; Mean loss: 2.9197\n",
            "Iteration: 3526; Percent done: 88.1%; Mean loss: 2.8731\n",
            "Iteration: 3527; Percent done: 88.2%; Mean loss: 2.7222\n",
            "Iteration: 3528; Percent done: 88.2%; Mean loss: 2.8763\n",
            "Iteration: 3529; Percent done: 88.2%; Mean loss: 2.7345\n",
            "Iteration: 3530; Percent done: 88.2%; Mean loss: 2.9886\n",
            "Iteration: 3531; Percent done: 88.3%; Mean loss: 2.6470\n",
            "Iteration: 3532; Percent done: 88.3%; Mean loss: 2.6151\n",
            "Iteration: 3533; Percent done: 88.3%; Mean loss: 2.9324\n",
            "Iteration: 3534; Percent done: 88.3%; Mean loss: 2.6175\n",
            "Iteration: 3535; Percent done: 88.4%; Mean loss: 2.7654\n",
            "Iteration: 3536; Percent done: 88.4%; Mean loss: 2.8618\n",
            "Iteration: 3537; Percent done: 88.4%; Mean loss: 2.7859\n",
            "Iteration: 3538; Percent done: 88.4%; Mean loss: 2.6475\n",
            "Iteration: 3539; Percent done: 88.5%; Mean loss: 2.9440\n",
            "Iteration: 3540; Percent done: 88.5%; Mean loss: 2.8020\n",
            "Iteration: 3541; Percent done: 88.5%; Mean loss: 2.8925\n",
            "Iteration: 3542; Percent done: 88.5%; Mean loss: 2.5592\n",
            "Iteration: 3543; Percent done: 88.6%; Mean loss: 2.6417\n",
            "Iteration: 3544; Percent done: 88.6%; Mean loss: 2.5506\n",
            "Iteration: 3545; Percent done: 88.6%; Mean loss: 2.9105\n",
            "Iteration: 3546; Percent done: 88.6%; Mean loss: 2.9674\n",
            "Iteration: 3547; Percent done: 88.7%; Mean loss: 2.7459\n",
            "Iteration: 3548; Percent done: 88.7%; Mean loss: 2.7218\n",
            "Iteration: 3549; Percent done: 88.7%; Mean loss: 2.8295\n",
            "Iteration: 3550; Percent done: 88.8%; Mean loss: 2.6710\n",
            "Iteration: 3551; Percent done: 88.8%; Mean loss: 2.5019\n",
            "Iteration: 3552; Percent done: 88.8%; Mean loss: 2.5639\n",
            "Iteration: 3553; Percent done: 88.8%; Mean loss: 2.8773\n",
            "Iteration: 3554; Percent done: 88.8%; Mean loss: 2.6753\n",
            "Iteration: 3555; Percent done: 88.9%; Mean loss: 2.8446\n",
            "Iteration: 3556; Percent done: 88.9%; Mean loss: 2.6210\n",
            "Iteration: 3557; Percent done: 88.9%; Mean loss: 2.6291\n",
            "Iteration: 3558; Percent done: 88.9%; Mean loss: 2.8251\n",
            "Iteration: 3559; Percent done: 89.0%; Mean loss: 2.6495\n",
            "Iteration: 3560; Percent done: 89.0%; Mean loss: 2.3808\n",
            "Iteration: 3561; Percent done: 89.0%; Mean loss: 2.7725\n",
            "Iteration: 3562; Percent done: 89.0%; Mean loss: 2.4978\n",
            "Iteration: 3563; Percent done: 89.1%; Mean loss: 2.7192\n",
            "Iteration: 3564; Percent done: 89.1%; Mean loss: 2.8237\n",
            "Iteration: 3565; Percent done: 89.1%; Mean loss: 2.8003\n",
            "Iteration: 3566; Percent done: 89.1%; Mean loss: 2.8143\n",
            "Iteration: 3567; Percent done: 89.2%; Mean loss: 2.5996\n",
            "Iteration: 3568; Percent done: 89.2%; Mean loss: 2.7924\n",
            "Iteration: 3569; Percent done: 89.2%; Mean loss: 2.7180\n",
            "Iteration: 3570; Percent done: 89.2%; Mean loss: 2.7921\n",
            "Iteration: 3571; Percent done: 89.3%; Mean loss: 2.9827\n",
            "Iteration: 3572; Percent done: 89.3%; Mean loss: 2.8150\n",
            "Iteration: 3573; Percent done: 89.3%; Mean loss: 2.9700\n",
            "Iteration: 3574; Percent done: 89.3%; Mean loss: 2.8332\n",
            "Iteration: 3575; Percent done: 89.4%; Mean loss: 2.8830\n",
            "Iteration: 3576; Percent done: 89.4%; Mean loss: 2.8441\n",
            "Iteration: 3577; Percent done: 89.4%; Mean loss: 2.8015\n",
            "Iteration: 3578; Percent done: 89.5%; Mean loss: 2.8420\n",
            "Iteration: 3579; Percent done: 89.5%; Mean loss: 3.0607\n",
            "Iteration: 3580; Percent done: 89.5%; Mean loss: 2.7778\n",
            "Iteration: 3581; Percent done: 89.5%; Mean loss: 2.9387\n",
            "Iteration: 3582; Percent done: 89.5%; Mean loss: 2.6828\n",
            "Iteration: 3583; Percent done: 89.6%; Mean loss: 2.9523\n",
            "Iteration: 3584; Percent done: 89.6%; Mean loss: 2.5175\n",
            "Iteration: 3585; Percent done: 89.6%; Mean loss: 2.6687\n",
            "Iteration: 3586; Percent done: 89.6%; Mean loss: 2.7387\n",
            "Iteration: 3587; Percent done: 89.7%; Mean loss: 2.7956\n",
            "Iteration: 3588; Percent done: 89.7%; Mean loss: 2.7484\n",
            "Iteration: 3589; Percent done: 89.7%; Mean loss: 2.7172\n",
            "Iteration: 3590; Percent done: 89.8%; Mean loss: 2.7751\n",
            "Iteration: 3591; Percent done: 89.8%; Mean loss: 2.7022\n",
            "Iteration: 3592; Percent done: 89.8%; Mean loss: 2.8656\n",
            "Iteration: 3593; Percent done: 89.8%; Mean loss: 2.7883\n",
            "Iteration: 3594; Percent done: 89.8%; Mean loss: 2.8943\n",
            "Iteration: 3595; Percent done: 89.9%; Mean loss: 2.5646\n",
            "Iteration: 3596; Percent done: 89.9%; Mean loss: 2.9794\n",
            "Iteration: 3597; Percent done: 89.9%; Mean loss: 2.8076\n",
            "Iteration: 3598; Percent done: 90.0%; Mean loss: 2.5054\n",
            "Iteration: 3599; Percent done: 90.0%; Mean loss: 2.9660\n",
            "Iteration: 3600; Percent done: 90.0%; Mean loss: 2.9136\n",
            "Iteration: 3601; Percent done: 90.0%; Mean loss: 2.8132\n",
            "Iteration: 3602; Percent done: 90.0%; Mean loss: 2.6355\n",
            "Iteration: 3603; Percent done: 90.1%; Mean loss: 2.9340\n",
            "Iteration: 3604; Percent done: 90.1%; Mean loss: 3.1099\n",
            "Iteration: 3605; Percent done: 90.1%; Mean loss: 2.8790\n",
            "Iteration: 3606; Percent done: 90.1%; Mean loss: 2.7980\n",
            "Iteration: 3607; Percent done: 90.2%; Mean loss: 3.0759\n",
            "Iteration: 3608; Percent done: 90.2%; Mean loss: 2.6085\n",
            "Iteration: 3609; Percent done: 90.2%; Mean loss: 2.5497\n",
            "Iteration: 3610; Percent done: 90.2%; Mean loss: 2.8003\n",
            "Iteration: 3611; Percent done: 90.3%; Mean loss: 2.7152\n",
            "Iteration: 3612; Percent done: 90.3%; Mean loss: 2.9311\n",
            "Iteration: 3613; Percent done: 90.3%; Mean loss: 2.6880\n",
            "Iteration: 3614; Percent done: 90.3%; Mean loss: 2.5864\n",
            "Iteration: 3615; Percent done: 90.4%; Mean loss: 2.7237\n",
            "Iteration: 3616; Percent done: 90.4%; Mean loss: 2.8187\n",
            "Iteration: 3617; Percent done: 90.4%; Mean loss: 2.8290\n",
            "Iteration: 3618; Percent done: 90.5%; Mean loss: 2.5301\n",
            "Iteration: 3619; Percent done: 90.5%; Mean loss: 2.7694\n",
            "Iteration: 3620; Percent done: 90.5%; Mean loss: 2.7468\n",
            "Iteration: 3621; Percent done: 90.5%; Mean loss: 3.0169\n",
            "Iteration: 3622; Percent done: 90.5%; Mean loss: 2.6763\n",
            "Iteration: 3623; Percent done: 90.6%; Mean loss: 2.5859\n",
            "Iteration: 3624; Percent done: 90.6%; Mean loss: 2.7274\n",
            "Iteration: 3625; Percent done: 90.6%; Mean loss: 2.7614\n",
            "Iteration: 3626; Percent done: 90.6%; Mean loss: 2.7058\n",
            "Iteration: 3627; Percent done: 90.7%; Mean loss: 2.5328\n",
            "Iteration: 3628; Percent done: 90.7%; Mean loss: 2.8266\n",
            "Iteration: 3629; Percent done: 90.7%; Mean loss: 2.5425\n",
            "Iteration: 3630; Percent done: 90.8%; Mean loss: 2.8628\n",
            "Iteration: 3631; Percent done: 90.8%; Mean loss: 2.8845\n",
            "Iteration: 3632; Percent done: 90.8%; Mean loss: 3.0659\n",
            "Iteration: 3633; Percent done: 90.8%; Mean loss: 2.7183\n",
            "Iteration: 3634; Percent done: 90.8%; Mean loss: 2.9079\n",
            "Iteration: 3635; Percent done: 90.9%; Mean loss: 2.7673\n",
            "Iteration: 3636; Percent done: 90.9%; Mean loss: 2.5578\n",
            "Iteration: 3637; Percent done: 90.9%; Mean loss: 2.7124\n",
            "Iteration: 3638; Percent done: 91.0%; Mean loss: 2.6210\n",
            "Iteration: 3639; Percent done: 91.0%; Mean loss: 2.5489\n",
            "Iteration: 3640; Percent done: 91.0%; Mean loss: 2.9552\n",
            "Iteration: 3641; Percent done: 91.0%; Mean loss: 2.6973\n",
            "Iteration: 3642; Percent done: 91.0%; Mean loss: 2.8452\n",
            "Iteration: 3643; Percent done: 91.1%; Mean loss: 2.7505\n",
            "Iteration: 3644; Percent done: 91.1%; Mean loss: 2.6097\n",
            "Iteration: 3645; Percent done: 91.1%; Mean loss: 2.6983\n",
            "Iteration: 3646; Percent done: 91.1%; Mean loss: 2.6852\n",
            "Iteration: 3647; Percent done: 91.2%; Mean loss: 2.8140\n",
            "Iteration: 3648; Percent done: 91.2%; Mean loss: 2.7789\n",
            "Iteration: 3649; Percent done: 91.2%; Mean loss: 2.7419\n",
            "Iteration: 3650; Percent done: 91.2%; Mean loss: 2.8159\n",
            "Iteration: 3651; Percent done: 91.3%; Mean loss: 2.8389\n",
            "Iteration: 3652; Percent done: 91.3%; Mean loss: 2.5695\n",
            "Iteration: 3653; Percent done: 91.3%; Mean loss: 2.6836\n",
            "Iteration: 3654; Percent done: 91.3%; Mean loss: 2.7398\n",
            "Iteration: 3655; Percent done: 91.4%; Mean loss: 2.9267\n",
            "Iteration: 3656; Percent done: 91.4%; Mean loss: 2.4502\n",
            "Iteration: 3657; Percent done: 91.4%; Mean loss: 2.9399\n",
            "Iteration: 3658; Percent done: 91.5%; Mean loss: 2.7861\n",
            "Iteration: 3659; Percent done: 91.5%; Mean loss: 2.6034\n",
            "Iteration: 3660; Percent done: 91.5%; Mean loss: 2.8175\n",
            "Iteration: 3661; Percent done: 91.5%; Mean loss: 2.8068\n",
            "Iteration: 3662; Percent done: 91.5%; Mean loss: 2.9591\n",
            "Iteration: 3663; Percent done: 91.6%; Mean loss: 3.0182\n",
            "Iteration: 3664; Percent done: 91.6%; Mean loss: 2.8210\n",
            "Iteration: 3665; Percent done: 91.6%; Mean loss: 2.6390\n",
            "Iteration: 3666; Percent done: 91.6%; Mean loss: 2.9464\n",
            "Iteration: 3667; Percent done: 91.7%; Mean loss: 2.8470\n",
            "Iteration: 3668; Percent done: 91.7%; Mean loss: 2.7170\n",
            "Iteration: 3669; Percent done: 91.7%; Mean loss: 2.8708\n",
            "Iteration: 3670; Percent done: 91.8%; Mean loss: 2.6889\n",
            "Iteration: 3671; Percent done: 91.8%; Mean loss: 2.7563\n",
            "Iteration: 3672; Percent done: 91.8%; Mean loss: 3.0307\n",
            "Iteration: 3673; Percent done: 91.8%; Mean loss: 2.6548\n",
            "Iteration: 3674; Percent done: 91.8%; Mean loss: 2.6890\n",
            "Iteration: 3675; Percent done: 91.9%; Mean loss: 2.4181\n",
            "Iteration: 3676; Percent done: 91.9%; Mean loss: 2.5727\n",
            "Iteration: 3677; Percent done: 91.9%; Mean loss: 2.8820\n",
            "Iteration: 3678; Percent done: 92.0%; Mean loss: 2.6140\n",
            "Iteration: 3679; Percent done: 92.0%; Mean loss: 2.7902\n",
            "Iteration: 3680; Percent done: 92.0%; Mean loss: 2.9491\n",
            "Iteration: 3681; Percent done: 92.0%; Mean loss: 2.6389\n",
            "Iteration: 3682; Percent done: 92.0%; Mean loss: 2.7906\n",
            "Iteration: 3683; Percent done: 92.1%; Mean loss: 2.9288\n",
            "Iteration: 3684; Percent done: 92.1%; Mean loss: 2.6774\n",
            "Iteration: 3685; Percent done: 92.1%; Mean loss: 2.5749\n",
            "Iteration: 3686; Percent done: 92.2%; Mean loss: 2.6580\n",
            "Iteration: 3687; Percent done: 92.2%; Mean loss: 2.8510\n",
            "Iteration: 3688; Percent done: 92.2%; Mean loss: 2.7569\n",
            "Iteration: 3689; Percent done: 92.2%; Mean loss: 2.7172\n",
            "Iteration: 3690; Percent done: 92.2%; Mean loss: 2.7053\n",
            "Iteration: 3691; Percent done: 92.3%; Mean loss: 2.8570\n",
            "Iteration: 3692; Percent done: 92.3%; Mean loss: 2.8807\n",
            "Iteration: 3693; Percent done: 92.3%; Mean loss: 2.5524\n",
            "Iteration: 3694; Percent done: 92.3%; Mean loss: 2.7744\n",
            "Iteration: 3695; Percent done: 92.4%; Mean loss: 2.7137\n",
            "Iteration: 3696; Percent done: 92.4%; Mean loss: 2.6859\n",
            "Iteration: 3697; Percent done: 92.4%; Mean loss: 2.7747\n",
            "Iteration: 3698; Percent done: 92.5%; Mean loss: 2.6419\n",
            "Iteration: 3699; Percent done: 92.5%; Mean loss: 2.5398\n",
            "Iteration: 3700; Percent done: 92.5%; Mean loss: 2.6854\n",
            "Iteration: 3701; Percent done: 92.5%; Mean loss: 2.7832\n",
            "Iteration: 3702; Percent done: 92.5%; Mean loss: 2.5735\n",
            "Iteration: 3703; Percent done: 92.6%; Mean loss: 2.8804\n",
            "Iteration: 3704; Percent done: 92.6%; Mean loss: 2.5482\n",
            "Iteration: 3705; Percent done: 92.6%; Mean loss: 2.7593\n",
            "Iteration: 3706; Percent done: 92.7%; Mean loss: 2.8748\n",
            "Iteration: 3707; Percent done: 92.7%; Mean loss: 2.7633\n",
            "Iteration: 3708; Percent done: 92.7%; Mean loss: 2.8875\n",
            "Iteration: 3709; Percent done: 92.7%; Mean loss: 2.5926\n",
            "Iteration: 3710; Percent done: 92.8%; Mean loss: 2.8733\n",
            "Iteration: 3711; Percent done: 92.8%; Mean loss: 2.6214\n",
            "Iteration: 3712; Percent done: 92.8%; Mean loss: 2.7423\n",
            "Iteration: 3713; Percent done: 92.8%; Mean loss: 2.6961\n",
            "Iteration: 3714; Percent done: 92.8%; Mean loss: 2.6125\n",
            "Iteration: 3715; Percent done: 92.9%; Mean loss: 2.9293\n",
            "Iteration: 3716; Percent done: 92.9%; Mean loss: 2.9003\n",
            "Iteration: 3717; Percent done: 92.9%; Mean loss: 2.8330\n",
            "Iteration: 3718; Percent done: 93.0%; Mean loss: 2.6994\n",
            "Iteration: 3719; Percent done: 93.0%; Mean loss: 2.7871\n",
            "Iteration: 3720; Percent done: 93.0%; Mean loss: 2.8748\n",
            "Iteration: 3721; Percent done: 93.0%; Mean loss: 2.5653\n",
            "Iteration: 3722; Percent done: 93.0%; Mean loss: 2.8306\n",
            "Iteration: 3723; Percent done: 93.1%; Mean loss: 2.7658\n",
            "Iteration: 3724; Percent done: 93.1%; Mean loss: 2.7414\n",
            "Iteration: 3725; Percent done: 93.1%; Mean loss: 2.6445\n",
            "Iteration: 3726; Percent done: 93.2%; Mean loss: 2.8243\n",
            "Iteration: 3727; Percent done: 93.2%; Mean loss: 2.7674\n",
            "Iteration: 3728; Percent done: 93.2%; Mean loss: 2.5095\n",
            "Iteration: 3729; Percent done: 93.2%; Mean loss: 2.9063\n",
            "Iteration: 3730; Percent done: 93.2%; Mean loss: 2.8082\n",
            "Iteration: 3731; Percent done: 93.3%; Mean loss: 2.7725\n",
            "Iteration: 3732; Percent done: 93.3%; Mean loss: 2.5267\n",
            "Iteration: 3733; Percent done: 93.3%; Mean loss: 2.8331\n",
            "Iteration: 3734; Percent done: 93.3%; Mean loss: 2.5542\n",
            "Iteration: 3735; Percent done: 93.4%; Mean loss: 2.7388\n",
            "Iteration: 3736; Percent done: 93.4%; Mean loss: 2.6127\n",
            "Iteration: 3737; Percent done: 93.4%; Mean loss: 2.8151\n",
            "Iteration: 3738; Percent done: 93.5%; Mean loss: 2.5510\n",
            "Iteration: 3739; Percent done: 93.5%; Mean loss: 2.7194\n",
            "Iteration: 3740; Percent done: 93.5%; Mean loss: 2.6206\n",
            "Iteration: 3741; Percent done: 93.5%; Mean loss: 2.8254\n",
            "Iteration: 3742; Percent done: 93.5%; Mean loss: 2.6951\n",
            "Iteration: 3743; Percent done: 93.6%; Mean loss: 2.4467\n",
            "Iteration: 3744; Percent done: 93.6%; Mean loss: 2.6217\n",
            "Iteration: 3745; Percent done: 93.6%; Mean loss: 3.0735\n",
            "Iteration: 3746; Percent done: 93.7%; Mean loss: 2.6525\n",
            "Iteration: 3747; Percent done: 93.7%; Mean loss: 2.3058\n",
            "Iteration: 3748; Percent done: 93.7%; Mean loss: 2.7489\n",
            "Iteration: 3749; Percent done: 93.7%; Mean loss: 2.6204\n",
            "Iteration: 3750; Percent done: 93.8%; Mean loss: 2.6275\n",
            "Iteration: 3751; Percent done: 93.8%; Mean loss: 2.6391\n",
            "Iteration: 3752; Percent done: 93.8%; Mean loss: 2.7153\n",
            "Iteration: 3753; Percent done: 93.8%; Mean loss: 2.8386\n",
            "Iteration: 3754; Percent done: 93.8%; Mean loss: 2.6147\n",
            "Iteration: 3755; Percent done: 93.9%; Mean loss: 2.8567\n",
            "Iteration: 3756; Percent done: 93.9%; Mean loss: 2.5849\n",
            "Iteration: 3757; Percent done: 93.9%; Mean loss: 2.7193\n",
            "Iteration: 3758; Percent done: 94.0%; Mean loss: 2.8494\n",
            "Iteration: 3759; Percent done: 94.0%; Mean loss: 2.7970\n",
            "Iteration: 3760; Percent done: 94.0%; Mean loss: 2.7588\n",
            "Iteration: 3761; Percent done: 94.0%; Mean loss: 2.5587\n",
            "Iteration: 3762; Percent done: 94.0%; Mean loss: 2.7987\n",
            "Iteration: 3763; Percent done: 94.1%; Mean loss: 2.6994\n",
            "Iteration: 3764; Percent done: 94.1%; Mean loss: 2.6284\n",
            "Iteration: 3765; Percent done: 94.1%; Mean loss: 2.8253\n",
            "Iteration: 3766; Percent done: 94.2%; Mean loss: 2.6187\n",
            "Iteration: 3767; Percent done: 94.2%; Mean loss: 2.6926\n",
            "Iteration: 3768; Percent done: 94.2%; Mean loss: 2.6526\n",
            "Iteration: 3769; Percent done: 94.2%; Mean loss: 2.8512\n",
            "Iteration: 3770; Percent done: 94.2%; Mean loss: 2.6983\n",
            "Iteration: 3771; Percent done: 94.3%; Mean loss: 2.9676\n",
            "Iteration: 3772; Percent done: 94.3%; Mean loss: 2.6144\n",
            "Iteration: 3773; Percent done: 94.3%; Mean loss: 2.7518\n",
            "Iteration: 3774; Percent done: 94.3%; Mean loss: 2.7107\n",
            "Iteration: 3775; Percent done: 94.4%; Mean loss: 2.5781\n",
            "Iteration: 3776; Percent done: 94.4%; Mean loss: 2.6602\n",
            "Iteration: 3777; Percent done: 94.4%; Mean loss: 2.7283\n",
            "Iteration: 3778; Percent done: 94.5%; Mean loss: 2.7317\n",
            "Iteration: 3779; Percent done: 94.5%; Mean loss: 2.8058\n",
            "Iteration: 3780; Percent done: 94.5%; Mean loss: 2.7153\n",
            "Iteration: 3781; Percent done: 94.5%; Mean loss: 2.9487\n",
            "Iteration: 3782; Percent done: 94.5%; Mean loss: 2.8464\n",
            "Iteration: 3783; Percent done: 94.6%; Mean loss: 2.7280\n",
            "Iteration: 3784; Percent done: 94.6%; Mean loss: 2.8863\n",
            "Iteration: 3785; Percent done: 94.6%; Mean loss: 2.7780\n",
            "Iteration: 3786; Percent done: 94.7%; Mean loss: 2.7151\n",
            "Iteration: 3787; Percent done: 94.7%; Mean loss: 2.7922\n",
            "Iteration: 3788; Percent done: 94.7%; Mean loss: 2.7914\n",
            "Iteration: 3789; Percent done: 94.7%; Mean loss: 2.7594\n",
            "Iteration: 3790; Percent done: 94.8%; Mean loss: 2.8960\n",
            "Iteration: 3791; Percent done: 94.8%; Mean loss: 2.8720\n",
            "Iteration: 3792; Percent done: 94.8%; Mean loss: 2.6049\n",
            "Iteration: 3793; Percent done: 94.8%; Mean loss: 2.6062\n",
            "Iteration: 3794; Percent done: 94.8%; Mean loss: 2.6319\n",
            "Iteration: 3795; Percent done: 94.9%; Mean loss: 2.3849\n",
            "Iteration: 3796; Percent done: 94.9%; Mean loss: 2.5934\n",
            "Iteration: 3797; Percent done: 94.9%; Mean loss: 2.7523\n",
            "Iteration: 3798; Percent done: 95.0%; Mean loss: 2.7441\n",
            "Iteration: 3799; Percent done: 95.0%; Mean loss: 2.7570\n",
            "Iteration: 3800; Percent done: 95.0%; Mean loss: 2.6990\n",
            "Iteration: 3801; Percent done: 95.0%; Mean loss: 2.8318\n",
            "Iteration: 3802; Percent done: 95.0%; Mean loss: 2.6781\n",
            "Iteration: 3803; Percent done: 95.1%; Mean loss: 2.7466\n",
            "Iteration: 3804; Percent done: 95.1%; Mean loss: 2.8395\n",
            "Iteration: 3805; Percent done: 95.1%; Mean loss: 2.6377\n",
            "Iteration: 3806; Percent done: 95.2%; Mean loss: 2.7808\n",
            "Iteration: 3807; Percent done: 95.2%; Mean loss: 2.8095\n",
            "Iteration: 3808; Percent done: 95.2%; Mean loss: 2.8199\n",
            "Iteration: 3809; Percent done: 95.2%; Mean loss: 2.4047\n",
            "Iteration: 3810; Percent done: 95.2%; Mean loss: 2.8863\n",
            "Iteration: 3811; Percent done: 95.3%; Mean loss: 2.5631\n",
            "Iteration: 3812; Percent done: 95.3%; Mean loss: 2.9208\n",
            "Iteration: 3813; Percent done: 95.3%; Mean loss: 2.6534\n",
            "Iteration: 3814; Percent done: 95.3%; Mean loss: 2.7753\n",
            "Iteration: 3815; Percent done: 95.4%; Mean loss: 2.5016\n",
            "Iteration: 3816; Percent done: 95.4%; Mean loss: 2.9827\n",
            "Iteration: 3817; Percent done: 95.4%; Mean loss: 2.6750\n",
            "Iteration: 3818; Percent done: 95.5%; Mean loss: 2.6665\n",
            "Iteration: 3819; Percent done: 95.5%; Mean loss: 2.4617\n",
            "Iteration: 3820; Percent done: 95.5%; Mean loss: 2.5353\n",
            "Iteration: 3821; Percent done: 95.5%; Mean loss: 2.7762\n",
            "Iteration: 3822; Percent done: 95.5%; Mean loss: 2.6385\n",
            "Iteration: 3823; Percent done: 95.6%; Mean loss: 2.7729\n",
            "Iteration: 3824; Percent done: 95.6%; Mean loss: 2.6866\n",
            "Iteration: 3825; Percent done: 95.6%; Mean loss: 2.5948\n",
            "Iteration: 3826; Percent done: 95.7%; Mean loss: 2.8161\n",
            "Iteration: 3827; Percent done: 95.7%; Mean loss: 2.5553\n",
            "Iteration: 3828; Percent done: 95.7%; Mean loss: 2.5078\n",
            "Iteration: 3829; Percent done: 95.7%; Mean loss: 2.6834\n",
            "Iteration: 3830; Percent done: 95.8%; Mean loss: 2.7976\n",
            "Iteration: 3831; Percent done: 95.8%; Mean loss: 2.7404\n",
            "Iteration: 3832; Percent done: 95.8%; Mean loss: 2.9367\n",
            "Iteration: 3833; Percent done: 95.8%; Mean loss: 2.9288\n",
            "Iteration: 3834; Percent done: 95.9%; Mean loss: 2.6307\n",
            "Iteration: 3835; Percent done: 95.9%; Mean loss: 2.6826\n",
            "Iteration: 3836; Percent done: 95.9%; Mean loss: 2.6754\n",
            "Iteration: 3837; Percent done: 95.9%; Mean loss: 2.8440\n",
            "Iteration: 3838; Percent done: 96.0%; Mean loss: 2.6772\n",
            "Iteration: 3839; Percent done: 96.0%; Mean loss: 2.7228\n",
            "Iteration: 3840; Percent done: 96.0%; Mean loss: 2.4836\n",
            "Iteration: 3841; Percent done: 96.0%; Mean loss: 2.6772\n",
            "Iteration: 3842; Percent done: 96.0%; Mean loss: 2.5604\n",
            "Iteration: 3843; Percent done: 96.1%; Mean loss: 2.6062\n",
            "Iteration: 3844; Percent done: 96.1%; Mean loss: 2.5993\n",
            "Iteration: 3845; Percent done: 96.1%; Mean loss: 2.5041\n",
            "Iteration: 3846; Percent done: 96.2%; Mean loss: 2.8873\n",
            "Iteration: 3847; Percent done: 96.2%; Mean loss: 2.4738\n",
            "Iteration: 3848; Percent done: 96.2%; Mean loss: 2.6701\n",
            "Iteration: 3849; Percent done: 96.2%; Mean loss: 3.0320\n",
            "Iteration: 3850; Percent done: 96.2%; Mean loss: 2.7294\n",
            "Iteration: 3851; Percent done: 96.3%; Mean loss: 2.5726\n",
            "Iteration: 3852; Percent done: 96.3%; Mean loss: 2.7218\n",
            "Iteration: 3853; Percent done: 96.3%; Mean loss: 2.6622\n",
            "Iteration: 3854; Percent done: 96.4%; Mean loss: 2.6700\n",
            "Iteration: 3855; Percent done: 96.4%; Mean loss: 2.6259\n",
            "Iteration: 3856; Percent done: 96.4%; Mean loss: 2.6229\n",
            "Iteration: 3857; Percent done: 96.4%; Mean loss: 2.7977\n",
            "Iteration: 3858; Percent done: 96.5%; Mean loss: 2.8925\n",
            "Iteration: 3859; Percent done: 96.5%; Mean loss: 2.8124\n",
            "Iteration: 3860; Percent done: 96.5%; Mean loss: 2.5996\n",
            "Iteration: 3861; Percent done: 96.5%; Mean loss: 2.6223\n",
            "Iteration: 3862; Percent done: 96.5%; Mean loss: 2.5643\n",
            "Iteration: 3863; Percent done: 96.6%; Mean loss: 2.6669\n",
            "Iteration: 3864; Percent done: 96.6%; Mean loss: 2.6269\n",
            "Iteration: 3865; Percent done: 96.6%; Mean loss: 2.5809\n",
            "Iteration: 3866; Percent done: 96.7%; Mean loss: 2.8660\n",
            "Iteration: 3867; Percent done: 96.7%; Mean loss: 2.5488\n",
            "Iteration: 3868; Percent done: 96.7%; Mean loss: 2.6769\n",
            "Iteration: 3869; Percent done: 96.7%; Mean loss: 2.6678\n",
            "Iteration: 3870; Percent done: 96.8%; Mean loss: 2.7677\n",
            "Iteration: 3871; Percent done: 96.8%; Mean loss: 2.5945\n",
            "Iteration: 3872; Percent done: 96.8%; Mean loss: 2.7318\n",
            "Iteration: 3873; Percent done: 96.8%; Mean loss: 2.7581\n",
            "Iteration: 3874; Percent done: 96.9%; Mean loss: 2.6332\n",
            "Iteration: 3875; Percent done: 96.9%; Mean loss: 2.9031\n",
            "Iteration: 3876; Percent done: 96.9%; Mean loss: 2.5022\n",
            "Iteration: 3877; Percent done: 96.9%; Mean loss: 2.6918\n",
            "Iteration: 3878; Percent done: 97.0%; Mean loss: 3.0643\n",
            "Iteration: 3879; Percent done: 97.0%; Mean loss: 2.6216\n",
            "Iteration: 3880; Percent done: 97.0%; Mean loss: 2.7411\n",
            "Iteration: 3881; Percent done: 97.0%; Mean loss: 2.7796\n",
            "Iteration: 3882; Percent done: 97.0%; Mean loss: 2.7111\n",
            "Iteration: 3883; Percent done: 97.1%; Mean loss: 2.7582\n",
            "Iteration: 3884; Percent done: 97.1%; Mean loss: 2.6942\n",
            "Iteration: 3885; Percent done: 97.1%; Mean loss: 2.6544\n",
            "Iteration: 3886; Percent done: 97.2%; Mean loss: 2.6879\n",
            "Iteration: 3887; Percent done: 97.2%; Mean loss: 2.9362\n",
            "Iteration: 3888; Percent done: 97.2%; Mean loss: 2.9362\n",
            "Iteration: 3889; Percent done: 97.2%; Mean loss: 3.0460\n",
            "Iteration: 3890; Percent done: 97.2%; Mean loss: 2.6166\n",
            "Iteration: 3891; Percent done: 97.3%; Mean loss: 2.6531\n",
            "Iteration: 3892; Percent done: 97.3%; Mean loss: 2.5731\n",
            "Iteration: 3893; Percent done: 97.3%; Mean loss: 2.7875\n",
            "Iteration: 3894; Percent done: 97.4%; Mean loss: 2.7305\n",
            "Iteration: 3895; Percent done: 97.4%; Mean loss: 2.9154\n",
            "Iteration: 3896; Percent done: 97.4%; Mean loss: 2.5626\n",
            "Iteration: 3897; Percent done: 97.4%; Mean loss: 2.4793\n",
            "Iteration: 3898; Percent done: 97.5%; Mean loss: 2.6420\n",
            "Iteration: 3899; Percent done: 97.5%; Mean loss: 2.7571\n",
            "Iteration: 3900; Percent done: 97.5%; Mean loss: 2.5879\n",
            "Iteration: 3901; Percent done: 97.5%; Mean loss: 2.7062\n",
            "Iteration: 3902; Percent done: 97.5%; Mean loss: 2.7505\n",
            "Iteration: 3903; Percent done: 97.6%; Mean loss: 2.7129\n",
            "Iteration: 3904; Percent done: 97.6%; Mean loss: 2.5938\n",
            "Iteration: 3905; Percent done: 97.6%; Mean loss: 2.7369\n",
            "Iteration: 3906; Percent done: 97.7%; Mean loss: 2.7611\n",
            "Iteration: 3907; Percent done: 97.7%; Mean loss: 2.5765\n",
            "Iteration: 3908; Percent done: 97.7%; Mean loss: 2.5761\n",
            "Iteration: 3909; Percent done: 97.7%; Mean loss: 2.5601\n",
            "Iteration: 3910; Percent done: 97.8%; Mean loss: 2.5858\n",
            "Iteration: 3911; Percent done: 97.8%; Mean loss: 2.6247\n",
            "Iteration: 3912; Percent done: 97.8%; Mean loss: 2.7339\n",
            "Iteration: 3913; Percent done: 97.8%; Mean loss: 2.6444\n",
            "Iteration: 3914; Percent done: 97.9%; Mean loss: 2.5224\n",
            "Iteration: 3915; Percent done: 97.9%; Mean loss: 2.4149\n",
            "Iteration: 3916; Percent done: 97.9%; Mean loss: 2.8440\n",
            "Iteration: 3917; Percent done: 97.9%; Mean loss: 2.8073\n",
            "Iteration: 3918; Percent done: 98.0%; Mean loss: 2.8941\n",
            "Iteration: 3919; Percent done: 98.0%; Mean loss: 2.5086\n",
            "Iteration: 3920; Percent done: 98.0%; Mean loss: 2.7045\n",
            "Iteration: 3921; Percent done: 98.0%; Mean loss: 2.5803\n",
            "Iteration: 3922; Percent done: 98.0%; Mean loss: 2.8803\n",
            "Iteration: 3923; Percent done: 98.1%; Mean loss: 2.5444\n",
            "Iteration: 3924; Percent done: 98.1%; Mean loss: 2.7914\n",
            "Iteration: 3925; Percent done: 98.1%; Mean loss: 2.7713\n",
            "Iteration: 3926; Percent done: 98.2%; Mean loss: 2.7549\n",
            "Iteration: 3927; Percent done: 98.2%; Mean loss: 2.5479\n",
            "Iteration: 3928; Percent done: 98.2%; Mean loss: 2.9589\n",
            "Iteration: 3929; Percent done: 98.2%; Mean loss: 2.4246\n",
            "Iteration: 3930; Percent done: 98.2%; Mean loss: 2.7924\n",
            "Iteration: 3931; Percent done: 98.3%; Mean loss: 2.5832\n",
            "Iteration: 3932; Percent done: 98.3%; Mean loss: 2.9392\n",
            "Iteration: 3933; Percent done: 98.3%; Mean loss: 2.6119\n",
            "Iteration: 3934; Percent done: 98.4%; Mean loss: 2.6980\n",
            "Iteration: 3935; Percent done: 98.4%; Mean loss: 2.6987\n",
            "Iteration: 3936; Percent done: 98.4%; Mean loss: 2.3196\n",
            "Iteration: 3937; Percent done: 98.4%; Mean loss: 2.6218\n",
            "Iteration: 3938; Percent done: 98.5%; Mean loss: 2.6312\n",
            "Iteration: 3939; Percent done: 98.5%; Mean loss: 2.8851\n",
            "Iteration: 3940; Percent done: 98.5%; Mean loss: 2.8530\n",
            "Iteration: 3941; Percent done: 98.5%; Mean loss: 2.6247\n",
            "Iteration: 3942; Percent done: 98.6%; Mean loss: 2.5121\n",
            "Iteration: 3943; Percent done: 98.6%; Mean loss: 2.4215\n",
            "Iteration: 3944; Percent done: 98.6%; Mean loss: 2.5578\n",
            "Iteration: 3945; Percent done: 98.6%; Mean loss: 2.7281\n",
            "Iteration: 3946; Percent done: 98.7%; Mean loss: 2.4354\n",
            "Iteration: 3947; Percent done: 98.7%; Mean loss: 2.5147\n",
            "Iteration: 3948; Percent done: 98.7%; Mean loss: 2.6458\n",
            "Iteration: 3949; Percent done: 98.7%; Mean loss: 2.6545\n",
            "Iteration: 3950; Percent done: 98.8%; Mean loss: 2.4188\n",
            "Iteration: 3951; Percent done: 98.8%; Mean loss: 2.6402\n",
            "Iteration: 3952; Percent done: 98.8%; Mean loss: 2.7398\n",
            "Iteration: 3953; Percent done: 98.8%; Mean loss: 2.9650\n",
            "Iteration: 3954; Percent done: 98.9%; Mean loss: 2.7046\n",
            "Iteration: 3955; Percent done: 98.9%; Mean loss: 2.6274\n",
            "Iteration: 3956; Percent done: 98.9%; Mean loss: 2.7039\n",
            "Iteration: 3957; Percent done: 98.9%; Mean loss: 2.5848\n",
            "Iteration: 3958; Percent done: 99.0%; Mean loss: 2.7990\n",
            "Iteration: 3959; Percent done: 99.0%; Mean loss: 2.4189\n",
            "Iteration: 3960; Percent done: 99.0%; Mean loss: 2.5212\n",
            "Iteration: 3961; Percent done: 99.0%; Mean loss: 2.4977\n",
            "Iteration: 3962; Percent done: 99.1%; Mean loss: 2.4577\n",
            "Iteration: 3963; Percent done: 99.1%; Mean loss: 2.7912\n",
            "Iteration: 3964; Percent done: 99.1%; Mean loss: 2.6016\n",
            "Iteration: 3965; Percent done: 99.1%; Mean loss: 2.7411\n",
            "Iteration: 3966; Percent done: 99.2%; Mean loss: 2.7590\n",
            "Iteration: 3967; Percent done: 99.2%; Mean loss: 2.8265\n",
            "Iteration: 3968; Percent done: 99.2%; Mean loss: 2.6146\n",
            "Iteration: 3969; Percent done: 99.2%; Mean loss: 2.6856\n",
            "Iteration: 3970; Percent done: 99.2%; Mean loss: 2.6415\n",
            "Iteration: 3971; Percent done: 99.3%; Mean loss: 2.7349\n",
            "Iteration: 3972; Percent done: 99.3%; Mean loss: 2.5112\n",
            "Iteration: 3973; Percent done: 99.3%; Mean loss: 2.5897\n",
            "Iteration: 3974; Percent done: 99.4%; Mean loss: 2.9198\n",
            "Iteration: 3975; Percent done: 99.4%; Mean loss: 2.7867\n",
            "Iteration: 3976; Percent done: 99.4%; Mean loss: 2.7544\n",
            "Iteration: 3977; Percent done: 99.4%; Mean loss: 2.8434\n",
            "Iteration: 3978; Percent done: 99.5%; Mean loss: 2.6959\n",
            "Iteration: 3979; Percent done: 99.5%; Mean loss: 2.4810\n",
            "Iteration: 3980; Percent done: 99.5%; Mean loss: 2.4296\n",
            "Iteration: 3981; Percent done: 99.5%; Mean loss: 2.8037\n",
            "Iteration: 3982; Percent done: 99.6%; Mean loss: 2.6600\n",
            "Iteration: 3983; Percent done: 99.6%; Mean loss: 2.7171\n",
            "Iteration: 3984; Percent done: 99.6%; Mean loss: 2.8866\n",
            "Iteration: 3985; Percent done: 99.6%; Mean loss: 2.6356\n",
            "Iteration: 3986; Percent done: 99.7%; Mean loss: 2.7803\n",
            "Iteration: 3987; Percent done: 99.7%; Mean loss: 2.9039\n",
            "Iteration: 3988; Percent done: 99.7%; Mean loss: 2.5709\n",
            "Iteration: 3989; Percent done: 99.7%; Mean loss: 2.9062\n",
            "Iteration: 3990; Percent done: 99.8%; Mean loss: 2.5801\n",
            "Iteration: 3991; Percent done: 99.8%; Mean loss: 2.8929\n",
            "Iteration: 3992; Percent done: 99.8%; Mean loss: 2.4703\n",
            "Iteration: 3993; Percent done: 99.8%; Mean loss: 2.5728\n",
            "Iteration: 3994; Percent done: 99.9%; Mean loss: 2.7027\n",
            "Iteration: 3995; Percent done: 99.9%; Mean loss: 2.6371\n",
            "Iteration: 3996; Percent done: 99.9%; Mean loss: 2.4889\n",
            "Iteration: 3997; Percent done: 99.9%; Mean loss: 2.5623\n",
            "Iteration: 3998; Percent done: 100.0%; Mean loss: 2.5688\n",
            "Iteration: 3999; Percent done: 100.0%; Mean loss: 2.6582\n",
            "Iteration: 4000; Percent done: 100.0%; Mean loss: 2.4223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44bf7A6ie67z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# switch our model into evaluation mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "# initialize an instance of GreedySearchDecoder in order to be able to perform the evaluation and return the predicted output as text\n",
        "searcher = GreedySearchDecoder(encoder, decoder)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiYqhSw-fksB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7795ac0c-9fe6-45bb-f971-5aeb31a7c0c1"
      },
      "source": [
        "runChatBot(encoder, decoder, searcher, voc)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> hello\n",
            "Response: hello . . . . .\n",
            "> what is your name?\n",
            "Response: my name is travis . . .\n",
            "> How are you travis?\n",
            "Response: i m fine . . . .\n",
            "> Are you sure?\n",
            "Response: i m sure . . . .\n",
            "> Do you have any friends?\n",
            "Response: no . . . . .\n",
            "> that sucks\n",
            "Response: yes . . . . .\n",
            "> what is your job?\n",
            "Response: i don t know . . .\n",
            "> DO you have a job?\n",
            "Response: yes . . . . .\n",
            "> What is your job?\n",
            "Response: i don t know . . .\n",
            "> Do you feel bad?\n",
            "Response: i don t know . . .\n",
            "> Im sorry to hear that\n",
            "Response: what ? ? ? . .\n",
            "> Sorry\n",
            "Response: it s okay . . . .\n",
            "> It's a nice day today\n",
            "Response: i m a cop . . .\n",
            "> i dont like that\n",
            "Error: Unknown Word\n",
            "> I'm sorry you are a cop\n",
            "Response: i am . . .\n",
            "> that is unfortunate\n",
            "Response: you re a loser . . .\n",
            "> me? YOU\n",
            "Response: yes . . . . .\n",
            "> I hate you\n",
            "Response: i m fine . . . .\n",
            "> Bye\n",
            "Response: bye . . . . .\n",
            "> /end\n",
            "Response: i m sorry . . . .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ae03efd8be41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunChatBot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-71276c56d167>\u001b[0m in \u001b[0;36mrunChatBot\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}